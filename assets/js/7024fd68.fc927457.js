"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[368],{1804(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>i,default:()=>c,frontMatter:()=>o,metadata:()=>r,toc:()=>m});const r=JSON.parse('{"id":"physical-ai/the-ai-robot-brain-nvidia-isaac/isaac-ros-vslam","title":"Isaac ROS for VSLAM","description":"Overview","source":"@site/docs/physical-ai/the-ai-robot-brain-nvidia-isaac/isaac-ros-vslam.md","sourceDirName":"physical-ai/the-ai-robot-brain-nvidia-isaac","slug":"/physical-ai/the-ai-robot-brain-nvidia-isaac/isaac-ros-vslam","permalink":"/physical-ai-book/docs/physical-ai/the-ai-robot-brain-nvidia-isaac/isaac-ros-vslam","draft":false,"unlisted":false,"editUrl":"https://github.com/arijh/physical-ai-book/tree/main/packages/create-docusaurus/templates/shared/docs/physical-ai/the-ai-robot-brain-nvidia-isaac/isaac-ros-vslam.md","tags":[],"version":"current","frontMatter":{"sidebar_label":"Isaac ROS for VSLAM","title":"Isaac ROS for VSLAM"},"sidebar":"docs","previous":{"title":"Synthetic Data Generation","permalink":"/physical-ai-book/docs/physical-ai/the-ai-robot-brain-nvidia-isaac/synthetic-data-generation"},"next":{"title":"Nav2 Path Planning for Humanoid Robots","permalink":"/physical-ai-book/docs/physical-ai/the-ai-robot-brain-nvidia-isaac/nav2-path-planning-humanoid"}}');var s=t(4848),a=t(8453);const o={sidebar_label:"Isaac ROS for VSLAM",title:"Isaac ROS for VSLAM"},i="Isaac ROS for VSLAM",l={},m=[{value:"Overview",id:"overview",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Hands-on Steps",id:"hands-on-steps",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Code Examples",id:"code-examples",level:2},{value:"Small Simulation",id:"small-simulation",level:2},{value:"Quick Recap",id:"quick-recap",level:2},{value:"Summary of Chapter 4 So Far",id:"summary-of-chapter-4-so-far",level:2}];function p(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"isaac-ros-for-vslam",children:"Isaac ROS for VSLAM"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"Visual Simultaneous Localization and Mapping (VSLAM) is a critical capability for autonomous robots, enabling them to understand and navigate in unknown environments. NVIDIA Isaac ROS provides specialized packages and tools that accelerate VSLAM development by leveraging GPU acceleration and optimized algorithms. This lesson explores how to implement and optimize VSLAM systems using Isaac ROS within the Isaac Sim environment."}),"\n",(0,s.jsx)(n.p,{children:"Isaac ROS bridges the gap between high-performance GPU-accelerated computer vision algorithms and the ROS 2 ecosystem, making it possible to deploy sophisticated VSLAM systems that can run in real-time on robotic platforms."}),"\n",(0,s.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understand the architecture of Isaac ROS VSLAM components"}),"\n",(0,s.jsx)(n.li,{children:"Implement stereo visual odometry using Isaac ROS packages"}),"\n",(0,s.jsx)(n.li,{children:"Configure and optimize VSLAM pipelines for different environments"}),"\n",(0,s.jsx)(n.li,{children:"Integrate VSLAM with other ROS 2 navigation components"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate VSLAM performance and accuracy in simulation"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"hands-on-steps",children:"Hands-on Steps"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS Setup"}),": Install and configure Isaac ROS packages"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Stereo Camera Configuration"}),": Set up stereo cameras for visual odometry"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"VSLAM Pipeline Implementation"}),": Create a complete VSLAM system"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance Optimization"}),": Tune parameters for optimal performance"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Integration Testing"}),": Connect VSLAM to navigation stack"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Understanding of ROS 2 concepts and message types"}),"\n",(0,s.jsx)(n.li,{children:"Knowledge of computer vision fundamentals"}),"\n",(0,s.jsx)(n.li,{children:"Experience with Isaac Sim (from previous lessons)"}),"\n",(0,s.jsx)(n.li,{children:"Basic understanding of SLAM concepts"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,s.jsx)(n.p,{children:"Let's start by creating a node that interfaces with Isaac ROS stereo visual odometry:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# isaac_ros_vslam_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom nav_msgs.msg import Odometry\nfrom geometry_msgs.msg import PoseStamped, PointStamped\nfrom std_msgs.msg import String, Header\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport math\nfrom tf2_ros import TransformBroadcaster\nfrom geometry_msgs.msg import TransformStamped\n\nclass IsaacROSVisualOdometryNode(Node):\n    """\n    Node that simulates integration with Isaac ROS Visual Odometry\n    In a real implementation, this would interface with Isaac ROS packages\n    """\n    def __init__(self):\n        super().__init__(\'isaac_ros_vslam_node\')\n\n        # Publishers\n        self.odom_pub = self.create_publisher(Odometry, \'/visual_odometry/odom\', 10)\n        self.pose_pub = self.create_publisher(PoseStamped, \'/visual_odometry/pose\', 10)\n        self.status_pub = self.create_publisher(String, \'/visual_odometry/status\', 10)\n\n        # Subscribers\n        self.left_image_sub = self.create_subscription(\n            Image, \'/isaac_sim/camera/left/image_raw\', self.left_image_callback, 10)\n        self.right_image_sub = self.create_subscription(\n            Image, \'/isaac_sim/camera/right/image_raw\', self.right_image_callback, 10)\n        self.left_info_sub = self.create_subscription(\n            CameraInfo, \'/isaac_sim/camera/left/camera_info\', self.left_info_callback, 10)\n        self.right_info_sub = self.create_subscription(\n            CameraInfo, \'/isaac_sim/camera/right/camera_info\', self.right_info_callback, 10)\n\n        # TF broadcaster\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n        # Timers\n        self.processing_timer = self.create_timer(0.1, self.process_visual_odometry)  # 10 Hz\n\n        # Internal components\n        self.cv_bridge = CvBridge()\n        self.left_image = None\n        self.right_image = None\n        self.left_camera_info = None\n        self.right_camera_info = None\n        self.camera_baseline = 0.1  # meters (typical stereo baseline)\n\n        # VSLAM state\n        self.current_position = np.array([0.0, 0.0, 0.0])\n        self.current_orientation = np.array([0.0, 0.0, 0.0, 1.0])  # x, y, z, w quaternion\n        self.previous_features = None\n        self.tracking_features = {}\n        self.frame_count = 0\n\n        # Performance metrics\n        self.processing_times = []\n        self.position_uncertainty = 0.1  # Initial uncertainty\n\n        # Feature tracking parameters\n        self.feature_detector_params = {\n            \'max_features\': 1000,\n            \'quality_level\': 0.01,\n            \'min_distance\': 10,\n            \'block_size\': 3\n        }\n\n        self.get_logger().info("Isaac ROS Visual Odometry Node initialized")\n\n    def left_image_callback(self, msg):\n        """Process left camera image"""\n        try:\n            self.left_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")\n        except Exception as e:\n            self.get_logger().error(f"Error converting left image: {e}")\n\n    def right_image_callback(self, msg):\n        """Process right camera image"""\n        try:\n            self.right_image = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")\n        except Exception as e:\n            self.get_logger().error(f"Error converting right image: {e}")\n\n    def left_info_callback(self, msg):\n        """Process left camera info"""\n        self.left_camera_info = msg\n        # Extract baseline from projection matrix if available\n        if len(msg.p) >= 3:\n            self.camera_baseline = abs(msg.p[3]) / msg.p[0]  # P(0,3) / P(0,0)\n\n    def right_info_callback(self, msg):\n        """Process right camera info"""\n        self.right_camera_info = msg\n\n    def detect_features(self, image):\n        """Detect features in image using Shi-Tomasi corner detection"""\n        if image is None:\n            return np.array([])\n\n        # Convert to grayscale\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n        # Detect good features to track\n        features = cv2.goodFeaturesToTrack(\n            gray,\n            maxCorners=self.feature_detector_params[\'max_features\'],\n            qualityLevel=self.feature_detector_params[\'quality_level\'],\n            minDistance=self.feature_detector_params[\'min_distance\'],\n            blockSize=self.feature_detector_params[\'block_size\']\n        )\n\n        return features if features is not None else np.array([])\n\n    def track_features(self, prev_img, curr_img, prev_features):\n        """Track features between frames using Lucas-Kanade optical flow"""\n        if prev_img is None or curr_img is None or len(prev_features) == 0:\n            return np.array([]), np.array([]), np.array([])\n\n        # Convert to grayscale\n        prev_gray = cv2.cvtColor(prev_img, cv2.COLOR_BGR2GRAY)\n        curr_gray = cv2.cvtColor(curr_img, cv2.COLOR_BGR2GRAY)\n\n        # Calculate optical flow\n        next_features, status, error = cv2.calcOpticalFlowPyrLK(\n            prev_gray, curr_gray, prev_features, None,\n            winSize=(15, 15),\n            maxLevel=2,\n            criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03)\n        )\n\n        # Filter out bad matches\n        good_old = prev_features[status == 1]\n        good_new = next_features[status == 1]\n\n        return good_old, good_new, status\n\n    def estimate_essential_matrix(self, points1, points2):\n        """Estimate essential matrix from corresponding points"""\n        if len(points1) < 5:\n            return None\n\n        # Estimate essential matrix\n        essential_matrix, mask = cv2.findEssentialMat(\n            points2, points1,\n            focal=self.left_camera_info.k[0] if self.left_camera_info else 1.0,\n            pp=(self.left_camera_info.k[2], self.left_camera_info.k[5]) if self.left_camera_info else (0.0, 0.0),\n            method=cv2.RANSAC,\n            threshold=1.0,\n            prob=0.999\n        )\n\n        return essential_matrix, mask\n\n    def decompose_essential_matrix(self, essential_matrix):\n        """Decompose essential matrix to get rotation and translation"""\n        if essential_matrix is None:\n            return None, None\n\n        # Decompose essential matrix\n        _, rotation, translation, _ = cv2.recoverPose(essential_matrix)\n\n        return rotation, translation\n\n    def triangulate_points(self, points1, points2):\n        """Triangulate 3D points from stereo correspondences"""\n        if self.left_camera_info is None or self.right_camera_info is None:\n            return np.array([])\n\n        # Create projection matrices\n        # Left camera: identity rotation, zero translation\n        proj_matrix1 = np.array([\n            [self.left_camera_info.k[0], 0, self.left_camera_info.k[2], 0],\n            [0, self.left_camera_info.k[4], self.left_camera_info.k[5], 0],\n            [0, 0, 1, 0]\n        ])\n\n        # Right camera: identity rotation, baseline translation\n        proj_matrix2 = np.array([\n            [self.right_camera_info.k[0], 0, self.right_camera_info.k[2], -self.right_camera_info.k[0] * self.camera_baseline],\n            [0, self.right_camera_info.k[4], self.right_camera_info.k[5], 0],\n            [0, 0, 1, 0]\n        ])\n\n        # Triangulate points\n        points4d = cv2.triangulatePoints(\n            proj_matrix1, proj_matrix2,\n            points1.T, points2.T\n        )\n\n        # Convert from homogeneous coordinates\n        points3d = (points4d[:3] / points4d[3]).T\n\n        return points3d\n\n    def process_visual_odometry(self):\n        """Main visual odometry processing function"""\n        start_time = self.get_clock().now()\n\n        # Check if we have both stereo images\n        if self.left_image is None or self.right_image is None:\n            return\n\n        # Detect features in current left image\n        current_features = self.detect_features(self.left_image)\n\n        if self.previous_features is not None and len(self.previous_features) > 0 and len(current_features) > 0:\n            # Track features between frames\n            prev_matched, curr_matched, status = self.track_features(\n                self.previous_left_image, self.left_image, self.previous_features\n            )\n\n            if len(prev_matched) > 5:  # Need minimum points for estimation\n                # Estimate motion using essential matrix\n                essential_matrix, mask = self.estimate_essential_matrix(prev_matched, curr_matched)\n\n                if essential_matrix is not None:\n                    rotation, translation = self.decompose_essential_matrix(essential_matrix)\n\n                    if rotation is not None and translation is not None:\n                        # Convert rotation matrix to quaternion\n                        quat = self.rotation_matrix_to_quaternion(rotation)\n\n                        # Update position and orientation\n                        # Apply translation scaled by some factor (since we don\'t have true scale)\n                        scale_factor = 0.1  # This would be determined by actual depth in real implementation\n                        delta_pos = np.array([translation[0, 0] * scale_factor,\n                                            translation[1, 0] * scale_factor,\n                                            translation[2, 0] * scale_factor])\n\n                        # Update position (in robot frame, transform to world frame)\n                        # For simplicity, we\'ll just add the delta\n                        self.current_position += delta_pos\n\n                        # Update orientation\n                        # This is a simplified update - in practice you\'d compose rotations\n                        self.current_orientation = self.multiply_quaternions(\n                            self.current_orientation,\n                            [quat[0], quat[1], quat[2], quat[3]]\n                        )\n\n                        # Update uncertainty based on number of tracked features\n                        self.position_uncertainty = max(0.05, 10.0 / len(prev_matched))\n\n        # Store current data for next iteration\n        self.previous_features = current_features\n        self.previous_left_image = self.left_image.copy()\n\n        # Publish results\n        self.publish_odometry()\n\n        # Calculate processing time\n        end_time = self.get_clock().now()\n        processing_time = (end_time - start_time).nanoseconds / 1e6  # ms\n        self.processing_times.append(processing_time)\n        if len(self.processing_times) > 100:\n            self.processing_times.pop(0)\n\n        # Publish status\n        avg_processing_time = np.mean(self.processing_times) if self.processing_times else 0\n        status_msg = String()\n        status_msg.data = f"Frame: {self.frame_count}, Features: {len(current_features) if current_features is not None else 0}, " \\\n                         f"Pos: ({self.current_position[0]:.2f}, {self.current_position[1]:.2f}, {self.current_position[2]:.2f}), " \\\n                         f"ProcTime: {avg_processing_time:.1f}ms, Uncertainty: {self.position_uncertainty:.3f}"\n        self.status_pub.publish(status_msg)\n\n        self.frame_count += 1\n\n    def rotation_matrix_to_quaternion(self, rotation_matrix):\n        """Convert rotation matrix to quaternion"""\n        # Method from https://www.euclideanspace.com/maths/geometry/rotations/conversions/matrixToQuaternion/\n        trace = np.trace(rotation_matrix)\n\n        if trace > 0:\n            s = math.sqrt(trace + 1.0) * 2  # s=4*qw\n            qw = 0.25 * s\n            qx = (rotation_matrix[2, 1] - rotation_matrix[1, 2]) / s\n            qy = (rotation_matrix[0, 2] - rotation_matrix[2, 0]) / s\n            qz = (rotation_matrix[1, 0] - rotation_matrix[0, 1]) / s\n        else:\n            if rotation_matrix[0, 0] > rotation_matrix[1, 1] and rotation_matrix[0, 0] > rotation_matrix[2, 2]:\n                s = math.sqrt(1.0 + rotation_matrix[0, 0] - rotation_matrix[1, 1] - rotation_matrix[2, 2]) * 2\n                qw = (rotation_matrix[2, 1] - rotation_matrix[1, 2]) / s\n                qx = 0.25 * s\n                qy = (rotation_matrix[0, 1] + rotation_matrix[1, 0]) / s\n                qz = (rotation_matrix[0, 2] + rotation_matrix[2, 0]) / s\n            elif rotation_matrix[1, 1] > rotation_matrix[2, 2]:\n                s = math.sqrt(1.0 + rotation_matrix[1, 1] - rotation_matrix[0, 0] - rotation_matrix[2, 2]) * 2\n                qw = (rotation_matrix[0, 2] - rotation_matrix[2, 0]) / s\n                qx = (rotation_matrix[0, 1] + rotation_matrix[1, 0]) / s\n                qy = 0.25 * s\n                qz = (rotation_matrix[1, 2] + rotation_matrix[2, 1]) / s\n            else:\n                s = math.sqrt(1.0 + rotation_matrix[2, 2] - rotation_matrix[0, 0] - rotation_matrix[1, 1]) * 2\n                qw = (rotation_matrix[1, 0] - rotation_matrix[0, 1]) / s\n                qx = (rotation_matrix[0, 2] + rotation_matrix[2, 0]) / s\n                qy = (rotation_matrix[1, 2] + rotation_matrix[2, 1]) / s\n                qz = 0.25 * s\n\n        return np.array([qx, qy, qz, qw])\n\n    def multiply_quaternions(self, q1, q2):\n        """Multiply two quaternions"""\n        x1, y1, z1, w1 = q1\n        x2, y2, z2, w2 = q2\n\n        x = w1 * x2 + x1 * w2 + y1 * z2 - z1 * y2\n        y = w1 * y2 - x1 * z2 + y1 * w2 + z1 * x2\n        z = w1 * z2 + x1 * y2 - y1 * x2 + z1 * w2\n        w = w1 * w2 - x1 * x2 - y1 * y2 - z1 * z2\n\n        # Normalize\n        norm = math.sqrt(x*x + y*y + z*z + w*w)\n        if norm > 0:\n            return [x/norm, y/norm, z/norm, w/norm]\n        else:\n            return [0, 0, 0, 1]\n\n    def publish_odometry(self):\n        """Publish odometry information"""\n        current_time = self.get_clock().now()\n\n        # Create odometry message\n        odom_msg = Odometry()\n        odom_msg.header.stamp = current_time.to_msg()\n        odom_msg.header.frame_id = \'odom\'\n        odom_msg.child_frame_id = \'base_link\'\n\n        # Set position\n        odom_msg.pose.pose.position.x = float(self.current_position[0])\n        odom_msg.pose.pose.position.y = float(self.current_position[1])\n        odom_msg.pose.pose.position.z = float(self.current_position[2])\n\n        # Set orientation\n        odom_msg.pose.pose.orientation.x = float(self.current_orientation[0])\n        odom_msg.pose.pose.orientation.y = float(self.current_orientation[1])\n        odom_msg.pose.pose.orientation.z = float(self.current_orientation[2])\n        odom_msg.pose.pose.orientation.w = float(self.current_orientation[3])\n\n        # Set covariance based on uncertainty\n        uncertainty = self.position_uncertainty\n        odom_msg.pose.covariance = [\n            uncertainty, 0.0, 0.0, 0.0, 0.0, 0.0,\n            0.0, uncertainty, 0.0, 0.0, 0.0, 0.0,\n            0.0, 0.0, uncertainty, 0.0, 0.0, 0.0,\n            0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n            0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\n            0.0, 0.0, 0.0, 0.0, 0.0, uncertainty\n        ]\n\n        # Publish odometry\n        self.odom_pub.publish(odom_msg)\n\n        # Create and publish pose stamped message\n        pose_msg = PoseStamped()\n        pose_msg.header.stamp = current_time.to_msg()\n        pose_msg.header.frame_id = \'odom\'\n        pose_msg.pose = odom_msg.pose.pose\n        self.pose_pub.publish(pose_msg)\n\n        # Broadcast transform\n        t = TransformStamped()\n        t.header.stamp = current_time.to_msg()\n        t.header.frame_id = \'odom\'\n        t.child_frame_id = \'base_link\'\n        t.transform.translation.x = float(self.current_position[0])\n        t.transform.translation.y = float(self.current_position[1])\n        t.transform.translation.z = float(self.current_position[2])\n        t.transform.rotation.x = float(self.current_orientation[0])\n        t.transform.rotation.y = float(self.current_orientation[1])\n        t.transform.rotation.z = float(self.current_orientation[2])\n        t.transform.rotation.w = float(self.current_orientation[3])\n        self.tf_broadcaster.sendTransform(t)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    vo_node = IsaacROSVisualOdometryNode()\n\n    try:\n        rclpy.spin(vo_node)\n    except KeyboardInterrupt:\n        vo_node.get_logger().info("Isaac ROS Visual Odometry Node stopped by user")\n    finally:\n        vo_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    import cv2  # Import here to avoid issues if not available\n    main()\n'})}),"\n",(0,s.jsx)(n.p,{children:"Now let's create a more advanced VSLAM node that integrates with Isaac ROS packages and includes mapping capabilities:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# isaac_ros_slam_mapper.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo, PointCloud2\nfrom nav_msgs.msg import Odometry, OccupancyGrid\nfrom geometry_msgs.msg import PoseStamped, Point\nfrom std_msgs.msg import String, Header\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport math\nimport open3d as o3d\nfrom scipy.spatial import cKDTree\nfrom tf2_ros import TransformListener, Buffer\nimport tf2_geometry_msgs\nimport threading\nimport time\n\nclass IsaacROSVSLAMMapper(Node):\n    """\n    Advanced VSLAM mapper node that integrates with Isaac ROS packages\n    This node performs mapping in addition to localization\n    """\n    def __init__(self):\n        super().__init__(\'isaac_ros_slam_mapper\')\n\n        # Publishers\n        self.map_pub = self.create_publisher(OccupancyGrid, \'/slam_map\', 10)\n        self.pointcloud_pub = self.create_publisher(PointCloud2, \'/slam_pointcloud\', 10)\n        self.odom_pub = self.create_publisher(Odometry, \'/slam/odom\', 10)\n        self.status_pub = self.create_publisher(String, \'/slam/status\', 10)\n\n        # Subscribers\n        self.stereo_sub = self.create_subscription(\n            String, \'/isaac_ros/stereo_processed\', self.stereo_callback, 10)\n        self.odom_sub = self.create_subscription(\n            Odometry, \'/visual_odometry/odom\', self.odom_callback, 10)\n\n        # Timers\n        self.mapping_timer = self.create_timer(0.5, self.update_map)  # 2 Hz\n        self.publish_timer = self.create_timer(0.1, self.publish_results)  # 10 Hz\n\n        # Internal components\n        self.cv_bridge = CvBridge()\n        self.tf_buffer = Buffer()\n        self.tf_listener = TransformListener(self.tf_buffer, self)\n\n        # SLAM state\n        self.current_pose = np.array([0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0])  # x, y, z, qx, qy, qz, qw\n        self.map_points = []  # List of 3D points in global frame\n        self.local_map = {}  # Dictionary of points for efficient lookup\n        self.keyframes = []  # Store key poses for loop closure\n        self.graph_optimizer = None  # Placeholder for graph optimization\n\n        # Mapping parameters\n        self.map_resolution = 0.1  # meters per cell\n        self.map_size = 50  # meters (50x50 map)\n        self.map_center = np.array([0.0, 0.0])  # Center of map in world coordinates\n        self.occupancy_grid = np.zeros((int(self.map_size / self.map_resolution),\n                                       int(self.map_size / self.map_resolution)), dtype=np.int8)\n\n        # Performance tracking\n        self.frame_count = 0\n        self.processing_times = []\n        self.map_size_history = []\n\n        # Feature management\n        self.feature_lifetime = 10  # How many frames to keep features\n        self.max_map_points = 10000  # Limit for memory management\n\n        self.get_logger().info("Isaac ROS VSLAM Mapper initialized")\n\n    def stereo_callback(self, msg):\n        """Process stereo vision results from Isaac ROS"""\n        # In a real implementation, this would receive processed stereo data\n        # For this example, we\'ll simulate receiving 3D points from stereo processing\n        try:\n            # Parse the message (in real implementation, this would be actual stereo data)\n            # For simulation, we\'ll generate points based on the environment\n            points_3d = self.generate_simulated_points()\n\n            # Transform points to global frame using current pose\n            global_points = self.transform_points_to_global(points_3d)\n\n            # Add points to map\n            self.add_points_to_map(global_points)\n\n            # Update keyframe if significant movement occurred\n            if self.should_add_keyframe():\n                self.add_keyframe()\n\n        except Exception as e:\n            self.get_logger().error(f"Error processing stereo data: {e}")\n\n    def odom_callback(self, msg):\n        """Update current pose from visual odometry"""\n        self.current_pose[0] = msg.pose.pose.position.x\n        self.current_pose[1] = msg.pose.pose.position.y\n        self.current_pose[2] = msg.pose.pose.position.z\n        self.current_pose[3] = msg.pose.pose.orientation.x\n        self.current_pose[4] = msg.pose.pose.orientation.y\n        self.current_pose[5] = msg.pose.pose.orientation.z\n        self.current_pose[6] = msg.pose.pose.orientation.w\n\n    def generate_simulated_points(self):\n        """Generate simulated 3D points from stereo processing"""\n        # This simulates what would come from Isaac ROS stereo packages\n        # In reality, this would be actual points from stereo reconstruction\n        points = []\n\n        # Generate points based on current position to simulate environment\n        for i in range(50):  # Generate 50 points per frame\n            # Simulate points around the robot\'s current position\n            angle = np.random.uniform(0, 2 * math.pi)\n            distance = np.random.uniform(1, 10)  # 1-10 meters away\n\n            x = distance * math.cos(angle)\n            y = distance * math.sin(angle)\n            z = np.random.uniform(0, 2)  # Ground to 2m height\n\n            points.append([x, y, z])\n\n        return np.array(points)\n\n    def transform_points_to_global(self, points_local):\n        """Transform points from camera frame to global map frame"""\n        # Extract current pose\n        pos = self.current_pose[:3]\n        quat = self.current_pose[3:]\n\n        # Convert quaternion to rotation matrix\n        rotation = self.quaternion_to_rotation_matrix(quat)\n\n        # Transform points\n        points_global = []\n        for point in points_local:\n            # Rotate and translate\n            rotated_point = rotation @ point\n            global_point = rotated_point + pos\n            points_global.append(global_point)\n\n        return np.array(points_global)\n\n    def quaternion_to_rotation_matrix(self, quat):\n        """Convert quaternion to rotation matrix"""\n        x, y, z, w = quat\n\n        r11 = 1 - 2*(y*y + z*z)\n        r12 = 2*(x*y - w*z)\n        r13 = 2*(x*z + w*y)\n\n        r21 = 2*(x*y + w*z)\n        r22 = 1 - 2*(x*x + z*z)\n        r23 = 2*(y*z - w*x)\n\n        r31 = 2*(x*z - w*y)\n        r32 = 2*(y*z + w*x)\n        r33 = 1 - 2*(x*x + y*y)\n\n        return np.array([\n            [r11, r12, r13],\n            [r21, r22, r23],\n            [r31, r32, r33]\n        ])\n\n    def add_points_to_map(self, points):\n        """Add 3D points to the map with proper filtering"""\n        for point in points:\n            x, y, z = point\n\n            # Filter points that are too far from robot\n            robot_x, robot_y = self.current_pose[0], self.current_pose[1]\n            dist_to_robot = math.sqrt((x - robot_x)**2 + (y - robot_y)**2)\n\n            if dist_to_robot > 15:  # Too far away\n                continue\n\n            # Add to local map with position as key\n            pos_key = (round(x, 2), round(y, 2), round(z, 2))\n            self.local_map[pos_key] = {\n                \'position\': point,\n                \'observations\': 1,\n                \'first_observed\': self.frame_count,\n                \'last_observed\': self.frame_count\n            }\n\n            # Add to global point list\n            self.map_points.append(point)\n\n        # Limit map size for performance\n        if len(self.map_points) > self.max_map_points:\n            # Keep only recent points\n            self.map_points = self.map_points[-self.max_map_points:]\n\n    def should_add_keyframe(self):\n        """Determine if we should add a new keyframe"""\n        if not self.keyframes:\n            return True\n\n        # Add keyframe if enough movement occurred\n        last_keyframe_pose = self.keyframes[-1]\n        current_pos = self.current_pose[:3]\n\n        distance = np.linalg.norm(current_pos - last_keyframe_pose[:3])\n\n        return distance > 1.0  # Add keyframe every meter\n\n    def add_keyframe(self):\n        """Add current pose as a keyframe"""\n        self.keyframes.append(self.current_pose.copy())\n\n    def update_map(self):\n        """Update the occupancy grid map"""\n        start_time = self.get_clock().now()\n\n        # Clear the occupancy grid\n        self.occupancy_grid.fill(-1)  # Unknown\n\n        # Convert map points to occupancy grid\n        map_shape = self.occupancy_grid.shape\n        center_x, center_y = self.map_center\n\n        for point in self.map_points[-1000:]:  # Only recent points for performance\n            x, y, z = point\n\n            # Convert world coordinates to grid indices\n            grid_x = int((x - (center_x - self.map_size/2)) / self.map_resolution)\n            grid_y = int((y - (center_y - self.map_size/2)) / self.map_resolution)\n\n            # Check bounds\n            if 0 <= grid_x < map_shape[1] and 0 <= grid_y < map_shape[0]:\n                # Mark as occupied (100) if it\'s a ground point or obstacle\n                # For simplicity, mark all points as occupied\n                self.occupancy_grid[grid_y, grid_x] = 100\n\n        # Calculate processing time\n        end_time = self.get_clock().now()\n        processing_time = (end_time - start_time).nanoseconds / 1e6  # ms\n        self.processing_times.append(processing_time)\n        if len(self.processing_times) > 100:\n            self.processing_times.pop(0)\n\n        self.frame_count += 1\n\n    def publish_results(self):\n        """Publish SLAM results"""\n        current_time = self.get_clock().now()\n\n        # Publish occupancy grid map\n        map_msg = OccupancyGrid()\n        map_msg.header.stamp = current_time.to_msg()\n        map_msg.header.frame_id = \'map\'\n        map_msg.info.resolution = float(self.map_resolution)\n        map_msg.info.width = self.occupancy_grid.shape[1]\n        map_msg.info.height = self.occupancy_grid.shape[0]\n        map_msg.info.origin.position.x = float(self.map_center[0] - self.map_size/2)\n        map_msg.info.origin.position.y = float(self.map_center[1] - self.map_size/2)\n        map_msg.data = self.occupancy_grid.flatten().tolist()\n        self.map_pub.publish(map_msg)\n\n        # Publish odometry (same as input but with updated frame_id)\n        odom_msg = Odometry()\n        odom_msg.header.stamp = current_time.to_msg()\n        odom_msg.header.frame_id = \'map\'\n        odom_msg.child_frame_id = \'base_link\'\n        odom_msg.pose.pose.position.x = float(self.current_pose[0])\n        odom_msg.pose.pose.position.y = float(self.current_pose[1])\n        odom_msg.pose.pose.position.z = float(self.current_pose[2])\n        odom_msg.pose.pose.orientation.x = float(self.current_pose[3])\n        odom_msg.pose.pose.orientation.y = float(self.current_pose[4])\n        odom_msg.pose.pose.orientation.z = float(self.current_pose[5])\n        odom_msg.pose.pose.orientation.w = float(self.current_pose[6])\n        self.odom_pub.publish(odom_msg)\n\n        # Publish status\n        avg_processing_time = np.mean(self.processing_times) if self.processing_times else 0\n        status_msg = String()\n        status_msg.data = f"Frame: {self.frame_count}, MapPoints: {len(self.map_points)}, " \\\n                         f"Keyframes: {len(self.keyframes)}, MapSize: {len(self.map_points)}, " \\\n                         f"ProcTime: {avg_processing_time:.1f}ms"\n        self.status_pub.publish(status_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    slam_node = IsaacROSVSLAMMapper()\n\n    try:\n        rclpy.spin(slam_node)\n    except KeyboardInterrupt:\n        slam_node.get_logger().info("Isaac ROS VSLAM Mapper stopped by user")\n    finally:\n        slam_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"small-simulation",children:"Small Simulation"}),"\n",(0,s.jsx)(n.p,{children:"Let's create a performance evaluation node for the VSLAM system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# vslam_performance_evaluator.py\nimport rclpy\nfrom rclpy.node import Node\nfrom nav_msgs.msg import Odometry\nfrom geometry_msgs.msg import PoseStamped, PointStamped\nfrom sensor_msgs.msg import PointCloud2\nfrom std_msgs.msg import String, Float32\nfrom visualization_msgs.msg import Marker, MarkerArray\nimport numpy as np\nimport math\nfrom scipy.spatial.distance import cdist\nfrom sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\n\nclass VSLAMEvaluator(Node):\n    \"\"\"\n    Evaluate VSLAM performance by comparing against ground truth\n    \"\"\"\n    def __init__(self):\n        super().__init__('vslam_evaluator')\n\n        # Publishers\n        self.rmse_pub = self.create_publisher(Float32, '/vslam_evaluation/rmse', 10)\n        self.ate_pub = self.create_publisher(Float32, '/vslam_evaluation/ate', 10)\n        self.rpe_pub = self.create_publisher(Float32, '/vslam_evaluation/rpe', 10)\n        self.status_pub = self.create_publisher(String, '/vslam_evaluation/status', 10)\n        self.error_markers_pub = self.create_publisher(MarkerArray, '/vslam_evaluation/error_markers', 10)\n\n        # Subscribers\n        self.vslam_odom_sub = self.create_subscription(\n            Odometry, '/slam/odom', self.vslam_odom_callback, 10)\n        self.ground_truth_sub = self.create_subscription(\n            Odometry, '/ground_truth/odom', self.ground_truth_callback, 10)\n\n        # Timers\n        self.evaluation_timer = self.create_timer(1.0, self.evaluate_performance)\n\n        # Data storage\n        self.vslam_trajectory = []\n        self.ground_truth_trajectory = []\n        self.evaluation_results = {\n            'rmse': float('inf'),\n            'ate': float('inf'),\n            'rpe': float('inf'),\n            'trajectory_length': 0,\n            'processing_times': []\n        }\n\n        # Evaluation parameters\n        self.max_trajectory_length = 1000\n        self.evaluation_window = 50  # For RPE calculation\n\n        self.get_logger().info(\"VSLAM Performance Evaluator initialized\")\n\n    def vslam_odom_callback(self, msg):\n        \"\"\"Store VSLAM estimated poses\"\"\"\n        pose = np.array([\n            msg.pose.pose.position.x,\n            msg.pose.pose.position.y,\n            msg.pose.pose.position.z,\n            msg.pose.pose.orientation.x,\n            msg.pose.pose.orientation.y,\n            msg.pose.pose.orientation.z,\n            msg.pose.pose.orientation.w\n        ])\n        self.vslam_trajectory.append({\n            'timestamp': msg.header.stamp,\n            'pose': pose,\n            'position': pose[:3]\n        })\n\n        # Limit trajectory size\n        if len(self.vslam_trajectory) > self.max_trajectory_length:\n            self.vslam_trajectory.pop(0)\n\n    def ground_truth_callback(self, msg):\n        \"\"\"Store ground truth poses\"\"\"\n        pose = np.array([\n            msg.pose.pose.position.x,\n            msg.pose.pose.position.y,\n            msg.pose.pose.position.z,\n            msg.pose.pose.orientation.x,\n            msg.pose.pose.orientation.y,\n            msg.pose.pose.orientation.z,\n            msg.pose.pose.orientation.w\n        ])\n        self.ground_truth_trajectory.append({\n            'timestamp': msg.header.stamp,\n            'pose': pose,\n            'position': pose[:3]\n        })\n\n        # Limit trajectory size\n        if len(self.ground_truth_trajectory) > self.max_trajectory_length:\n            self.ground_truth_trajectory.pop(0)\n\n    def calculate_rmse(self):\n        \"\"\"Calculate Root Mean Square Error between trajectories\"\"\"\n        if len(self.vslam_trajectory) == 0 or len(self.ground_truth_trajectory) == 0:\n            return float('inf')\n\n        # Align trajectories by time (simplified approach)\n        min_len = min(len(self.vslam_trajectory), len(self.ground_truth_trajectory))\n\n        if min_len < 2:\n            return float('inf')\n\n        # Calculate position errors\n        errors = []\n        for i in range(min_len):\n            vslam_pos = self.vslam_trajectory[i]['position']\n            gt_pos = self.ground_truth_trajectory[i]['position']\n\n            error = np.linalg.norm(vslam_pos - gt_pos)\n            errors.append(error)\n\n        if errors:\n            rmse = np.sqrt(np.mean(np.array(errors) ** 2))\n            return float(rmse)\n        else:\n            return float('inf')\n\n    def calculate_ate(self):\n        \"\"\"Calculate Absolute Trajectory Error\"\"\"\n        if len(self.vslam_trajectory) < 2 or len(self.ground_truth_trajectory) < 2:\n            return float('inf')\n\n        # Find optimal alignment between trajectories\n        min_len = min(len(self.vslam_trajectory), len(self.ground_truth_trajectory))\n\n        if min_len < 2:\n            return float('inf')\n\n        vslam_positions = np.array([t['position'] for t in self.vslam_trajectory[:min_len]])\n        gt_positions = np.array([t['position'] for t in self.ground_truth_trajectory[:min_len]])\n\n        # Calculate ATE (after alignment, simplified as direct comparison)\n        ate = np.mean(np.linalg.norm(vslam_positions - gt_positions, axis=1))\n        return float(ate)\n\n    def calculate_rpe(self):\n        \"\"\"Calculate Relative Pose Error\"\"\"\n        if len(self.vslam_trajectory) < 3 or len(self.ground_truth_trajectory) < 3:\n            return float('inf')\n\n        min_len = min(len(self.vslam_trajectory), len(self.ground_truth_trajectory))\n\n        if min_len < 3:\n            return float('inf')\n\n        rpe_values = []\n\n        # Calculate RPE for consecutive pairs\n        for i in range(min_len - 1):\n            # VSLAM relative motion\n            vslam_pos1 = self.vslam_trajectory[i]['position']\n            vslam_pos2 = self.vslam_trajectory[i + 1]['position']\n            vslam_rel = vslam_pos2 - vslam_pos1\n\n            # Ground truth relative motion\n            gt_pos1 = self.ground_truth_trajectory[i]['position']\n            gt_pos2 = self.ground_truth_trajectory[i + 1]['position']\n            gt_rel = gt_pos2 - gt_pos1\n\n            # Calculate relative error\n            rel_error = np.linalg.norm(vslam_rel - gt_rel)\n            rpe_values.append(rel_error)\n\n        if rpe_values:\n            rpe = np.mean(rpe_values)\n            return float(rpe)\n        else:\n            return float('inf')\n\n    def create_error_markers(self):\n        \"\"\"Create visualization markers for errors\"\"\"\n        marker_array = MarkerArray()\n\n        # Position error markers\n        for i in range(min(len(self.vslam_trajectory), len(self.ground_truth_trajectory))):\n            if i % 10 == 0:  # Only show every 10th error for performance\n                vslam_pos = self.vslam_trajectory[i]['position']\n                gt_pos = self.ground_truth_trajectory[i]['position']\n\n                # Line marker showing error\n                error_marker = Marker()\n                error_marker.header.frame_id = \"map\"\n                error_marker.header.stamp = self.get_clock().now().to_msg()\n                error_marker.ns = \"vslam_errors\"\n                error_marker.id = i\n                error_marker.type = Marker.LINE_STRIP\n                error_marker.action = Marker.ADD\n                error_marker.scale.x = 0.02\n                error_marker.color.r = 1.0\n                error_marker.color.a = 0.8\n\n                # Add start (GT) and end (VSLAM) points\n                start_point = Point()\n                start_point.x = float(gt_pos[0])\n                start_point.y = float(gt_pos[1])\n                start_point.z = float(gt_pos[2])\n                error_marker.points.append(start_point)\n\n                end_point = Point()\n                end_point.x = float(vslam_pos[0])\n                end_point.y = float(vslam_pos[1])\n                end_point.z = float(vslam_pos[2])\n                error_marker.points.append(end_point)\n\n                marker_array.markers.append(error_marker)\n\n        return marker_array\n\n    def evaluate_performance(self):\n        \"\"\"Run comprehensive performance evaluation\"\"\"\n        if len(self.vslam_trajectory) == 0 or len(self.ground_truth_trajectory) == 0:\n            return\n\n        # Calculate metrics\n        rmse = self.calculate_rmse()\n        ate = self.calculate_ate()\n        rpe = self.calculate_rpe()\n\n        # Store results\n        self.evaluation_results['rmse'] = rmse\n        self.evaluation_results['ate'] = ate\n        self.evaluation_results['rpe'] = rpe\n        self.evaluation_results['trajectory_length'] = min(len(self.vslam_trajectory),\n                                                          len(self.ground_truth_trajectory))\n\n        # Publish metrics\n        rmse_msg = Float32()\n        rmse_msg.data = rmse\n        self.rmse_pub.publish(rmse_msg)\n\n        ate_msg = Float32()\n        ate_msg.data = ate\n        self.ate_pub.publish(ate_msg)\n\n        rpe_msg = Float32()\n        rpe_msg.data = rpe\n        self.rpe_pub.publish(rpe_msg)\n\n        # Publish error visualization\n        error_markers = self.create_error_markers()\n        self.error_markers_pub.publish(error_markers)\n\n        # Publish status\n        status_msg = String()\n        status_msg.data = f\"RMSE: {rmse:.3f}m, ATE: {ate:.3f}m, RPE: {rpe:.3f}m, \" \\\n                         f\"TrajLen: {self.evaluation_results['trajectory_length']}\"\n        self.status_pub.publish(status_msg)\n\n        # Log evaluation results\n        self.get_logger().info(f\"VSLAM Evaluation - RMSE: {rmse:.3f}m, ATE: {ate:.3f}m, RPE: {rpe:.3f}m\")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    evaluator = VSLAMEvaluator()\n\n    try:\n        rclpy.spin(evaluator)\n    except KeyboardInterrupt:\n        evaluator.get_logger().info(\"VSLAM Performance Evaluator stopped by user\")\n\n        # Print final evaluation summary\n        results = evaluator.evaluation_results\n        print(\"\\n\" + \"=\"*50)\n        print(\"VSLAM Performance Evaluation Summary\")\n        print(\"=\"*50)\n        print(f\"Root Mean Square Error (RMSE): {results['rmse']:.3f} m\")\n        print(f\"Absolute Trajectory Error (ATE): {results['ate']:.3f} m\")\n        print(f\"Relative Pose Error (RPE): {results['rpe']:.3f} m\")\n        print(f\"Trajectory Length: {results['trajectory_length']} poses\")\n        print(\"=\"*50)\n\n    finally:\n        evaluator.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"quick-recap",children:"Quick Recap"}),"\n",(0,s.jsx)(n.p,{children:"In this lesson, we've covered:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS Architecture"}),": Understanding the components of Isaac ROS VSLAM packages"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Visual Odometry"}),": Implementing stereo visual odometry with feature tracking and motion estimation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Mapping Integration"}),": Combining localization with map building capabilities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Performance Evaluation"}),": Techniques for assessing VSLAM accuracy and robustness"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"GPU Acceleration"}),": Leveraging Isaac ROS's optimized algorithms for real-time performance"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Isaac ROS provides a powerful framework for implementing sophisticated VSLAM systems that can run efficiently on robotic platforms. The integration of GPU-accelerated computer vision algorithms with the ROS 2 ecosystem enables the deployment of advanced perception systems that were previously computationally prohibitive on mobile robots."}),"\n",(0,s.jsx)(n.p,{children:"In the next lesson, we'll explore Nav2-based path planning for humanoid robots, focusing on how to adapt traditional navigation approaches for robots with complex kinematics and dynamics."}),"\n",(0,s.jsx)(n.h2,{id:"summary-of-chapter-4-so-far",children:"Summary of Chapter 4 So Far"}),"\n",(0,s.jsx)(n.p,{children:'In Chapter 4: "The AI-Robot Brain (NVIDIA Isaac)", we\'ve covered:'}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Introduction to NVIDIA Isaac Sim"}),": Core concepts and capabilities of the Isaac Sim platform"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Synthetic Data Generation"}),": Creating diverse, labeled datasets for AI training with domain randomization"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Isaac ROS for VSLAM"}),": Implementing visual SLAM systems with Isaac ROS packages"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"These lessons provide the foundation for understanding how NVIDIA's AI tools can accelerate robotics development, from simulation to perception and mapping."})]})}function c(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(p,{...e})}):p(e)}},8453(e,n,t){t.d(n,{R:()=>o,x:()=>i});var r=t(6540);const s={},a=r.createContext(s);function o(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function i(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),r.createElement(a.Provider,{value:n},e.children)}}}]);