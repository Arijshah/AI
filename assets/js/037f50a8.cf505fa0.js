"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[555],{5543(n,e,t){t.r(e),t.d(e,{assets:()=>c,contentTitle:()=>r,default:()=>d,frontMatter:()=>s,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"physical-ai/vision-language-action-vla/introduction-to-vla-integration","title":"Introduction to Vision-Language-Action Integration","description":"Overview","source":"@site/docs/physical-ai/vision-language-action-vla/introduction-to-vla-integration.md","sourceDirName":"physical-ai/vision-language-action-vla","slug":"/physical-ai/vision-language-action-vla/introduction-to-vla-integration","permalink":"/physical-ai-book/docs/physical-ai/vision-language-action-vla/introduction-to-vla-integration","draft":false,"unlisted":false,"editUrl":"https://github.com/arijh/physical-ai-book/tree/main/packages/create-docusaurus/templates/shared/docs/physical-ai/vision-language-action-vla/introduction-to-vla-integration.md","tags":[],"version":"current","frontMatter":{"sidebar_label":"Introduction to Vision-Language-Action Integration","title":"Introduction to Vision-Language-Action Integration"},"sidebar":"docs","previous":{"title":"Nav2 Path Planning for Humanoid Robots","permalink":"/physical-ai-book/docs/physical-ai/the-ai-robot-brain-nvidia-isaac/nav2-path-planning-humanoid"},"next":{"title":"Voice-to-Action using Whisper","permalink":"/physical-ai-book/docs/physical-ai/vision-language-action-vla/voice-to-action-using-whisper"}}');var a=t(4848),i=t(8453);const s={sidebar_label:"Introduction to Vision-Language-Action Integration",title:"Introduction to Vision-Language-Action Integration"},r="Introduction to Vision-Language-Action Integration",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Hands-on Steps",id:"hands-on-steps",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Code Examples",id:"code-examples",level:2},{value:"Small Simulation",id:"small-simulation",level:2},{value:"Quick Recap",id:"quick-recap",level:2}];function m(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,i.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"introduction-to-vision-language-action-integration",children:"Introduction to Vision-Language-Action Integration"})}),"\n",(0,a.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(e.p,{children:"Vision-Language-Action (VLA) represents the cutting edge of AI robotics, where visual perception, natural language understanding, and physical action are seamlessly integrated. This paradigm enables robots to understand and execute complex commands expressed in natural language, perceive their environment visually, and perform appropriate physical actions. VLA systems are crucial for creating intuitive human-robot interaction and enabling robots to operate effectively in human-centered environments."}),"\n",(0,a.jsx)(e.p,{children:"The VLA framework combines large language models (LLMs) for understanding commands, computer vision for environmental perception, and robotics control for executing actions. This integration allows robots to interpret high-level, ambiguous human instructions and translate them into specific, executable robot behaviors."}),"\n",(0,a.jsx)(e.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,a.jsx)(e.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Understand the architecture and components of VLA systems"}),"\n",(0,a.jsx)(e.li,{children:"Explain the integration points between vision, language, and action systems"}),"\n",(0,a.jsx)(e.li,{children:"Design cognitive planning pipelines that map language to robot actions"}),"\n",(0,a.jsx)(e.li,{children:"Implement basic VLA system components using modern AI tools"}),"\n",(0,a.jsx)(e.li,{children:"Evaluate the effectiveness of VLA systems for robot control"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"hands-on-steps",children:"Hands-on Steps"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"VLA Architecture Setup"}),": Design the system architecture connecting vision, language, and action components"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Perception Pipeline"}),": Create visual perception for environmental understanding"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Language Processing"}),": Implement natural language understanding for commands"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Action Mapping"}),": Connect language understanding to robot action execution"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Integration Testing"}),": Test the complete VLA pipeline with robot commands"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Understanding of ROS 2 concepts and robotics frameworks"}),"\n",(0,a.jsx)(e.li,{children:"Knowledge of large language models and their applications"}),"\n",(0,a.jsx)(e.li,{children:"Basic understanding of computer vision concepts"}),"\n",(0,a.jsx)(e.li,{children:"Experience with Python and AI/ML frameworks"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,a.jsx)(e.p,{children:"Let's start by creating the core architecture for a VLA system:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# vla_architecture.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import Twist, Pose, Point\nfrom std_msgs.msg import String\nfrom std_msgs.msg import Header\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport json\nimport asyncio\nimport openai\nfrom typing import Dict, List, Tuple, Optional\nfrom dataclasses import dataclass\n\n@dataclass\nclass VLACommand:\n    \"\"\"Data class for VLA commands\"\"\"\n    text: str\n    visual_context: Optional[np.ndarray] = None\n    robot_state: Dict = None\n    action_sequence: List[str] = None\n\n@dataclass\nclass VLAActionResult:\n    \"\"\"Data class for VLA action results\"\"\"\n    success: bool\n    executed_actions: List[str]\n    reasoning_trace: List[str]\n    confidence: float\n\nclass VLAArchitecture(Node):\n    \"\"\"\n    Vision-Language-Action architecture node\n    Integrates perception, language understanding, and action execution\n    \"\"\"\n    def __init__(self):\n        super().__init__('vla_architecture')\n\n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, '/robot/cmd_vel', 10)\n        self.status_pub = self.create_publisher(String, '/vla/status', 10)\n        self.result_pub = self.create_publisher(String, '/vla/result', 10)\n\n        # Subscribers\n        self.voice_cmd_sub = self.create_subscription(String, '/voice/command', self.voice_command_callback, 10)\n        self.camera_sub = self.create_subscription(Image, '/camera/rgb/image_raw', self.camera_callback, 10)\n        self.robot_state_sub = self.create_subscription(String, '/robot/state', self.robot_state_callback, 10)\n\n        # Internal components\n        self.cv_bridge = CvBridge()\n        self.current_image = None\n        self.current_robot_state = {}\n        self.command_history = []\n\n        # VLA components\n        self.perception_module = VisionPerceptionModule(self)\n        self.language_module = LanguageUnderstandingModule(self)\n        self.action_module = ActionExecutionModule(self)\n\n        # VLA processing timer\n        self.vla_timer = self.create_timer(0.1, self.process_vla_cycle)\n\n        self.get_logger().info(\"VLA Architecture initialized\")\n\n    def voice_command_callback(self, msg):\n        \"\"\"Process voice commands\"\"\"\n        self.get_logger().info(f\"Received voice command: {msg.data}\")\n\n        # Create VLA command with current visual context\n        vla_cmd = VLACommand(\n            text=msg.data,\n            visual_context=self.current_image,\n            robot_state=self.current_robot_state.copy()\n        )\n\n        # Add to processing queue\n        self.command_history.append(vla_cmd)\n\n    def camera_callback(self, msg):\n        \"\"\"Process camera images for visual context\"\"\"\n        try:\n            self.current_image = self.cv_bridge.imgmsg_to_cv2(msg, \"bgr8\")\n        except Exception as e:\n            self.get_logger().error(f\"Error converting image: {e}\")\n\n    def robot_state_callback(self, msg):\n        \"\"\"Update robot state\"\"\"\n        try:\n            self.current_robot_state = json.loads(msg.data)\n        except json.JSONDecodeError:\n            self.get_logger().error(\"Error parsing robot state\")\n\n    def process_vla_cycle(self):\n        \"\"\"Main VLA processing cycle\"\"\"\n        if not self.command_history:\n            return\n\n        # Process the oldest command\n        cmd = self.command_history[0]\n\n        # Step 1: Visual perception\n        visual_info = self.perception_module.analyze_scene(cmd.visual_context)\n\n        # Step 2: Language understanding with visual context\n        action_sequence = self.language_module.parse_command_with_context(\n            cmd.text,\n            visual_info,\n            cmd.robot_state\n        )\n\n        # Step 3: Action execution\n        result = self.action_module.execute_action_sequence(action_sequence)\n\n        # Publish results\n        result_msg = String()\n        result_msg.data = json.dumps({\n            'command': cmd.text,\n            'actions': action_sequence,\n            'success': result.success,\n            'confidence': result.confidence,\n            'reasoning': result.reasoning_trace\n        })\n        self.result_pub.publish(result_msg)\n\n        status_msg = String()\n        status_msg.data = f\"Processed: '{cmd.text}', Actions: {len(action_sequence)}, Success: {result.success}\"\n        self.status_pub.publish(status_msg)\n\n        self.get_logger().info(f\"VLA Result: {status_msg.data}\")\n\n        # Remove processed command\n        self.command_history.pop(0)\n\nclass VisionPerceptionModule:\n    \"\"\"\n    Vision module for scene understanding\n    \"\"\"\n    def __init__(self, parent_node):\n        self.node = parent_node\n        # In a real implementation, this would connect to a vision model (e.g., CLIP, DETR, etc.)\n\n    def analyze_scene(self, image: np.ndarray) -> Dict:\n        \"\"\"\n        Analyze the current scene to extract relevant information\n        In a real implementation, this would use computer vision models\n        \"\"\"\n        if image is None:\n            return {'objects': [], 'spatial_relations': [], 'environment': 'unknown'}\n\n        # Simulated scene analysis\n        scene_info = {\n            'objects': [\n                {'name': 'table', 'position': [1.0, 0.0, 0.0], 'size': 'medium'},\n                {'name': 'chair', 'position': [1.5, -0.5, 0.0], 'size': 'medium'},\n                {'name': 'cup', 'position': [1.2, 0.2, 0.8], 'size': 'small'}\n            ],\n            'spatial_relations': [\n                'cup is on table',\n                'chair is near table'\n            ],\n            'environment': 'indoor',\n            'navigation_clear': True  # Whether path is clear\n        }\n\n        self.node.get_logger().info(f\"Scene analysis: {len(scene_info['objects'])} objects detected\")\n        return scene_info\n\nclass LanguageUnderstandingModule:\n    \"\"\"\n    Language understanding module for command parsing\n    \"\"\"\n    def __init__(self, parent_node):\n        self.node = parent_node\n        # In a real implementation, this would connect to an LLM (e.g., GPT, Claude, etc.)\n        self.action_mapping = {\n            'move to': ['navigate_to_location'],\n            'go to': ['navigate_to_location'],\n            'pick up': ['approach_object', 'grasp_object'],\n            'grasp': ['approach_object', 'grasp_object'],\n            'take': ['approach_object', 'grasp_object'],\n            'place': ['navigate_to_location', 'release_object'],\n            'put': ['navigate_to_location', 'release_object'],\n            'find': ['scan_environment', 'identify_object'],\n            'look for': ['scan_environment', 'identify_object'],\n            'bring': ['approach_object', 'grasp_object', 'navigate_to_location', 'release_object']\n        }\n\n    def parse_command_with_context(self, command: str, visual_info: Dict, robot_state: Dict) -> List[str]:\n        \"\"\"\n        Parse natural language command with visual and robot state context\n        \"\"\"\n        command_lower = command.lower()\n        actions = []\n\n        # Map command to actions based on keywords\n        for keyword, mapped_actions in self.action_mapping.items():\n            if keyword in command_lower:\n                actions.extend(mapped_actions)\n\n        # Add context-aware refinements\n        if 'navigate_to_location' in actions:\n            # Extract location from command\n            target_location = self.extract_location_from_command(command, visual_info)\n            if target_location:\n                actions.append(f'navigate_to_{target_location}')\n\n        if 'approach_object' in actions:\n            # Extract object from command\n            target_object = self.extract_object_from_command(command, visual_info)\n            if target_object:\n                actions.append(f'approach_{target_object}')\n\n        # Remove duplicates while preserving order\n        unique_actions = []\n        for action in actions:\n            if action not in unique_actions:\n                unique_actions.append(action)\n\n        self.node.get_logger().info(f\"Parsed command '{command}' to actions: {unique_actions}\")\n        return unique_actions\n\n    def extract_location_from_command(self, command: str, visual_info: Dict) -> Optional[str]:\n        \"\"\"Extract target location from command using visual context\"\"\"\n        command_lower = command.lower()\n\n        # Look for location indicators in visual info\n        for obj in visual_info.get('objects', []):\n            if obj['name'] in command_lower:\n                return obj['name']\n\n        # Look for spatial terms\n        if 'table' in command_lower:\n            return 'table'\n        elif 'kitchen' in command_lower or 'counter' in command_lower:\n            return 'kitchen'\n        elif 'bedroom' in command_lower:\n            return 'bedroom'\n\n        return 'default_location'\n\n    def extract_object_from_command(self, command: str, visual_info: Dict) -> Optional[str]:\n        \"\"\"Extract target object from command using visual context\"\"\"\n        command_lower = command.lower()\n\n        # Look for objects in visual info\n        for obj in visual_info.get('objects', []):\n            if obj['name'] in command_lower:\n                return obj['name']\n\n        # Look for object descriptors\n        if 'cup' in command_lower or 'mug' in command_lower:\n            return 'cup'\n        elif 'book' in command_lower:\n            return 'book'\n        elif 'ball' in command_lower:\n            return 'ball'\n\n        return 'unknown_object'\n\nclass ActionExecutionModule:\n    \"\"\"\n    Action execution module for robot control\n    \"\"\"\n    def __init__(self, parent_node):\n        self.node = parent_node\n        self.action_functions = {\n            'navigate_to_location': self.execute_navigate_to_location,\n            'approach_object': self.execute_approach_object,\n            'grasp_object': self.execute_grasp_object,\n            'release_object': self.execute_release_object,\n            'scan_environment': self.execute_scan_environment,\n            'identify_object': self.execute_identify_object\n        }\n\n    def execute_action_sequence(self, action_sequence: List[str]) -> VLAActionResult:\n        \"\"\"\n        Execute a sequence of actions\n        \"\"\"\n        executed_actions = []\n        reasoning_trace = []\n        success = True\n        confidence = 0.9  # Initial high confidence\n\n        for action in action_sequence:\n            self.node.get_logger().info(f\"Executing action: {action}\")\n\n            # Extract specific action and parameters\n            if '_' in action:\n                base_action = action.split('_', 1)[0]\n                param = action.split('_', 1)[1]\n            else:\n                base_action = action\n                param = None\n\n            # Execute action if supported\n            if base_action in self.action_functions:\n                try:\n                    action_result = self.action_functions[base_action](param)\n                    executed_actions.append(action)\n                    reasoning_trace.append(f\"Executed {action}: {action_result}\")\n\n                    # Update confidence based on action success\n                    if not action_result.get('success', True):\n                        confidence *= 0.8  # Reduce confidence on failure\n                        success = False\n                except Exception as e:\n                    self.node.get_logger().error(f\"Error executing action {action}: {e}\")\n                    reasoning_trace.append(f\"Failed to execute {action}: {str(e)}\")\n                    success = False\n                    confidence *= 0.5  # Significantly reduce confidence on error\n            else:\n                self.node.get_logger().warn(f\"Unknown action: {action}\")\n                reasoning_trace.append(f\"Unknown action: {action}\")\n                success = False\n\n        return VLAActionResult(\n            success=success,\n            executed_actions=executed_actions,\n            reasoning_trace=reasoning_trace,\n            confidence=confidence\n        )\n\n    def execute_navigate_to_location(self, location: str) -> Dict:\n        \"\"\"Execute navigation to a specific location\"\"\"\n        self.node.get_logger().info(f\"Navigating to {location}\")\n\n        # In a real implementation, this would send navigation commands\n        # For simulation, just return success\n        return {'success': True, 'location': location, 'time_taken': 0.5}\n\n    def execute_approach_object(self, obj_name: str) -> Dict:\n        \"\"\"Execute approach to a specific object\"\"\"\n        self.node.get_logger().info(f\"Approaching {obj_name}\")\n        return {'success': True, 'object': obj_name, 'distance': 0.1}\n\n    def execute_grasp_object(self, obj_name: str = None) -> Dict:\n        \"\"\"Execute grasping action\"\"\"\n        self.node.get_logger().info(f\"Grasping {'object' if obj_name is None else obj_name}\")\n        return {'success': True, 'object': obj_name, 'grasped': True}\n\n    def execute_release_object(self, obj_name: str = None) -> Dict:\n        \"\"\"Execute releasing action\"\"\"\n        self.node.get_logger().info(f\"Releasing {'object' if obj_name is None else obj_name}\")\n        return {'success': True, 'object': obj_name, 'released': True}\n\n    def execute_scan_environment(self, param: str = None) -> Dict:\n        \"\"\"Execute environment scanning\"\"\"\n        self.node.get_logger().info(\"Scanning environment\")\n        return {'success': True, 'objects_found': 5, 'features': ['table', 'chair', 'cup']}\n\n    def execute_identify_object(self, obj_name: str = None) -> Dict:\n        \"\"\"Execute object identification\"\"\"\n        self.node.get_logger().info(f\"Identifying {obj_name}\")\n        return {'success': True, 'object': obj_name, 'confidence': 0.95}\n\ndef main(args=None):\n    rclpy.init(args=args)\n    vla_node = VLAArchitecture()\n\n    try:\n        rclpy.spin(vla_node)\n    except KeyboardInterrupt:\n        vla_node.get_logger().info(\"VLA Architecture stopped by user\")\n    finally:\n        vla_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.p,{children:"Now let's create a more advanced cognitive planning module that maps LLMs to ROS 2 actions:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# cognitive_planning_module.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Pose, Point\nfrom sensor_msgs.msg import Image\nfrom nav_msgs.msg import Path\nfrom builtin_interfaces.msg import Time\nimport json\nimport asyncio\nimport time\nfrom typing import Dict, List, Tuple, Optional\nfrom dataclasses import dataclass\nimport requests\nimport re\n\n@dataclass\nclass CognitivePlan:\n    """Data class for cognitive plans"""\n    plan_id: str\n    original_command: str\n    plan_steps: List[Dict]\n    execution_context: Dict\n    created_at: float\n\nclass CognitivePlanningNode(Node):\n    """\n    Cognitive planning node that maps LLMs to ROS 2 actions\n    """\n    def __init__(self):\n        super().__init__(\'cognitive_planning_node\')\n\n        # Publishers\n        self.plan_pub = self.create_publisher(String, \'/cognitive_plan\', 10)\n        self.action_pub = self.create_publisher(String, \'/robot/action\', 10)\n        self.status_pub = self.create_publisher(String, \'/cognitive_planning/status\', 10)\n\n        # Subscribers\n        self.command_sub = self.create_subscription(String, \'/natural_language/command\', self.command_callback, 10)\n        self.vision_sub = self.create_subscription(Image, \'/camera/rgb/image_raw\', self.vision_callback, 10)\n        self.world_state_sub = self.create_subscription(String, \'/world_state\', self.world_state_callback, 10)\n\n        # Timers\n        self.planning_timer = self.create_timer(0.5, self.planning_cycle)\n\n        # Internal state\n        self.pending_commands = []\n        self.world_state = {}\n        self.vision_context = None\n        self.plan_history = []\n        self.llm_client = SimpleLLMClient()  # Simulated LLM client\n\n        self.get_logger().info("Cognitive Planning Node initialized")\n\n    def command_callback(self, msg):\n        """Process natural language commands"""\n        self.get_logger().info(f"Received command: {msg.data}")\n        self.pending_commands.append({\n            \'command\': msg.data,\n            \'timestamp\': time.time(),\n            \'context\': self.world_state.copy()\n        })\n\n    def vision_callback(self, msg):\n        """Process vision context"""\n        # In a real implementation, this would process the image\n        # For this example, we\'ll just note that vision data is available\n        self.vision_context = {\'available\': True, \'timestamp\': time.time()}\n\n    def world_state_callback(self, msg):\n        """Update world state"""\n        try:\n            self.world_state = json.loads(msg.data)\n        except json.JSONDecodeError:\n            self.get_logger().error("Error parsing world state")\n\n    def planning_cycle(self):\n        """Main planning cycle"""\n        if not self.pending_commands:\n            return\n\n        # Process the oldest command\n        cmd_data = self.pending_commands[0]\n\n        # Create cognitive plan\n        plan = self.create_cognitive_plan(cmd_data[\'command\'], cmd_data[\'context\'])\n\n        if plan:\n            # Publish plan\n            plan_msg = String()\n            plan_msg.data = json.dumps({\n                \'plan_id\': plan.plan_id,\n                \'original_command\': plan.original_command,\n                \'steps\': plan.plan_steps,\n                \'context\': plan.execution_context\n            })\n            self.plan_pub.publish(plan_msg)\n\n            # Execute plan\n            self.execute_plan(plan)\n\n            # Update status\n            status_msg = String()\n            status_msg.data = f"Executed plan for: \'{cmd_data[\'command\']}\', Steps: {len(plan.plan_steps)}"\n            self.status_pub.publish(status_msg)\n\n            self.get_logger().info(status_msg.data)\n\n        # Remove processed command\n        self.pending_commands.pop(0)\n\n    def create_cognitive_plan(self, command: str, context: Dict) -> Optional[CognitivePlan]:\n        """Create a cognitive plan from natural language command"""\n        try:\n            # Use LLM to generate plan\n            plan_prompt = self.create_planning_prompt(command, context)\n            plan_response = self.llm_client.generate(plan_prompt)\n\n            # Parse LLM response into executable plan\n            plan_steps = self.parse_plan_response(plan_response, command, context)\n\n            if plan_steps:\n                plan = CognitivePlan(\n                    plan_id=f"plan_{int(time.time())}",\n                    original_command=command,\n                    plan_steps=plan_steps,\n                    execution_context=context,\n                    created_at=time.time()\n                )\n\n                # Add to history\n                self.plan_history.append(plan)\n\n                return plan\n\n        except Exception as e:\n            self.get_logger().error(f"Error creating cognitive plan: {e}")\n\n        return None\n\n    def create_planning_prompt(self, command: str, context: Dict) -> str:\n        """Create prompt for LLM-based planning"""\n        prompt = f"""\n        You are a cognitive planning assistant for a robot. Given the user command and current world state,\n        break down the command into executable steps for a robot.\n\n        Command: {command}\n\n        World State: {json.dumps(context, indent=2)}\n\n        Provide a sequence of robot actions as a JSON list. Each action should be:\n        - Navigate to a location\n        - Detect an object\n        - Grasp an object\n        - Place an object\n        - Manipulate an object\n        - Wait for something\n        - Communicate with user\n\n        Example output format:\n        [\n          {{"action": "navigate", "target": "kitchen_table", "description": "Move to kitchen table"}},\n          {{"action": "detect", "object": "red_mug", "description": "Look for red mug"}},\n          {{"action": "grasp", "object": "red_mug", "description": "Pick up red mug"}},\n          {{"action": "navigate", "target": "counter", "description": "Move to counter"}},\n          {{"action": "place", "object": "red_mug", "target": "counter", "description": "Place mug on counter"}}\n        ]\n\n        Respond with ONLY the JSON list, nothing else:\n        """\n\n        return prompt\n\n    def parse_plan_response(self, response: str, original_command: str, context: Dict) -> List[Dict]:\n        """Parse LLM response into structured plan steps"""\n        # Try to extract JSON from response\n        json_match = re.search(r\'\\[.*\\]\', response, re.DOTALL)\n        if json_match:\n            try:\n                plan_steps = json.loads(json_match.group(0))\n                return plan_steps\n            except json.JSONDecodeError:\n                pass\n\n        # If no JSON found, try to create simple plan based on keywords\n        command_lower = original_command.lower()\n\n        simple_plan = []\n        if \'bring\' in command_lower or \'get\' in command_lower:\n            # Find object to bring\n            obj_match = re.search(r\'bring (.+?) to|get (.+?) to|bring (.+)\', command_lower)\n            obj = obj_match.group(1) if obj_match else \'object\'\n\n            simple_plan = [\n                {"action": "detect", "object": obj, "description": f"Detect {obj}"},\n                {"action": "navigate", "target": obj, "description": f"Navigate to {obj}"},\n                {"action": "grasp", "object": obj, "description": f"Grasp {obj}"},\n                {"action": "navigate", "target": "delivery_location", "description": "Navigate to delivery location"},\n                {"action": "place", "object": obj, "target": "delivery_location", "description": f"Place {obj}"}\n            ]\n        elif \'move\' in command_lower or \'go\' in command_lower:\n            # Extract destination\n            dest_match = re.search(r\'to (.+)|at (.+)\', command_lower)\n            dest = dest_match.group(1) if dest_match else \'destination\'\n\n            simple_plan = [\n                {"action": "navigate", "target": dest, "description": f"Navigate to {dest}"}\n            ]\n        elif \'pick up\' in command_lower or \'take\' in command_lower:\n            # Extract object\n            obj_match = re.search(r\'pick up (.+)|take (.+)\', command_lower)\n            obj = obj_match.group(1) if obj_match else \'object\'\n\n            simple_plan = [\n                {"action": "detect", "object": obj, "description": f"Detect {obj}"},\n                {"action": "navigate", "target": obj, "description": f"Navigate to {obj}"},\n                {"action": "grasp", "object": obj, "description": f"Grasp {obj}"}\n            ]\n\n        return simple_plan\n\n    def execute_plan(self, plan: CognitivePlan):\n        """Execute the cognitive plan"""\n        for step_idx, step in enumerate(plan.plan_steps):\n            self.get_logger().info(f"Executing step {step_idx + 1}/{len(plan.plan_steps)}: {step}")\n\n            # Map action to ROS 2 message\n            action_msg = String()\n            action_msg.data = json.dumps({\n                \'plan_id\': plan.plan_id,\n                \'step_number\': step_idx + 1,\n                \'action\': step,\n                \'original_command\': plan.original_command\n            })\n\n            self.action_pub.publish(action_msg)\n\n            # Simple delay to allow action execution\n            time.sleep(0.5)  # In real implementation, wait for action completion\n\nclass SimpleLLMClient:\n    """\n    Simulated LLM client for demonstration purposes\n    In a real implementation, this would connect to an actual LLM service\n    """\n    def __init__(self):\n        self.model = "gpt-4"  # Simulated model\n        self.response_cache = {}\n\n    def generate(self, prompt: str) -> str:\n        """Generate response from LLM"""\n        # In a real implementation, this would call an LLM API\n        # For this example, we\'ll return simulated responses based on common commands\n\n        if "bring" in prompt.lower() or "get" in prompt.lower():\n            return """[\n  {"action": "detect", "object": "coffee mug", "description": "Detect coffee mug on table"},\n  {"action": "navigate", "target": "table", "description": "Navigate to table"},\n  {"action": "grasp", "object": "coffee mug", "description": "Grasp coffee mug"},\n  {"action": "navigate", "target": "counter", "description": "Navigate to counter"},\n  {"action": "place", "object": "coffee mug", "target": "counter", "description": "Place coffee mug on counter"}\n]"""\n        elif "move" in prompt.lower() or "go" in prompt.lower():\n            return """[\n  {"action": "navigate", "target": "kitchen", "description": "Navigate to kitchen"},\n  {"action": "wait", "duration": 1.0, "description": "Wait for confirmation"}\n]"""\n        else:\n            # Default response\n            return """[\n  {"action": "detect", "object": "unknown", "description": "Scan environment for relevant objects"},\n  {"action": "communicate", "message": "I need more specific instructions", "description": "Ask for clarification"}\n]"""\n\ndef main(args=None):\n    rclpy.init(args=args)\n    planner = CognitivePlanningNode()\n\n    try:\n        rclpy.spin(planner)\n    except KeyboardInterrupt:\n        planner.get_logger().info("Cognitive Planning Node stopped by user")\n    finally:\n        planner.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(e.h2,{id:"small-simulation",children:"Small Simulation"}),"\n",(0,a.jsx)(e.p,{children:"Let's create a simulation environment that demonstrates the VLA integration:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# vla_integration_simulator.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, CameraInfo\nfrom geometry_msgs.msg import Twist, Pose, Point\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport cv2\nimport json\nimport time\nimport random\n\nclass VLAIntegrationSimulator(Node):\n    \"\"\"\n    Simulation environment for VLA integration\n    Demonstrates the complete Vision-Language-Action pipeline\n    \"\"\"\n    def __init__(self):\n        super().__init__('vla_integration_simulator')\n\n        # Publishers\n        self.camera_pub = self.create_publisher(Image, '/camera/rgb/image_raw', 10)\n        self.world_state_pub = self.create_publisher(String, '/world_state', 10)\n        self.voice_cmd_pub = self.create_publisher(String, '/voice/command', 10)\n        self.status_pub = self.create_publisher(String, '/vla_simulation/status', 10)\n\n        # Subscribers\n        self.vla_result_sub = self.create_subscription(String, '/vla/result', self.vla_result_callback, 10)\n        self.robot_action_sub = self.create_subscription(String, '/robot/action', self.robot_action_callback, 10)\n\n        # Timers\n        self.camera_timer = self.create_timer(0.1, self.publish_camera_feed)  # 10 Hz\n        self.world_state_timer = self.create_timer(1.0, self.publish_world_state)  # 1 Hz\n        self.command_timer = self.create_timer(5.0, self.publish_random_command)  # Every 5 seconds\n        self.simulation_timer = self.create_timer(0.05, self.simulation_step)  # 20 Hz\n\n        # Internal components\n        self.cv_bridge = CvBridge()\n        self.sim_time = 0.0\n        self.robot_position = np.array([0.0, 0.0])\n        self.objects = [\n            {'name': 'table', 'position': np.array([2.0, 1.0]), 'type': 'furniture'},\n            {'name': 'chair', 'position': np.array([2.5, 0.5]), 'type': 'furniture'},\n            {'name': 'cup', 'position': np.array([2.2, 1.2]), 'type': 'object'},\n            {'name': 'book', 'position': np.array([1.8, 0.8]), 'type': 'object'},\n        ]\n        self.commands = [\n            \"Bring me the cup from the table\",\n            \"Go to the kitchen\",\n            \"Pick up the red book\",\n            \"Move to the chair\",\n            \"Find the cup and bring it to me\"\n        ]\n\n        self.get_logger().info(\"VLA Integration Simulator initialized\")\n\n    def publish_camera_feed(self):\n        \"\"\"Generate and publish simulated camera images\"\"\"\n        # Create a simple simulated environment image\n        image = np.zeros((480, 640, 3), dtype=np.uint8)\n\n        # Draw floor grid\n        for y in range(0, 480, 50):\n            cv2.line(image, (0, y), (640, y), (50, 50, 50), 1)\n        for x in range(0, 640, 50):\n            cv2.line(image, (x, 0), (x, 480), (50, 50, 50), 1)\n\n        # Draw robot position (as a circle)\n        robot_img_x = int(320 + self.robot_position[0] * 50)  # Scale position for image\n        robot_img_y = int(240 - self.robot_position[1] * 50)  # Invert Y for image coordinates\n        cv2.circle(image, (robot_img_x, robot_img_y), 15, (0, 255, 0), -1)\n\n        # Draw objects\n        for obj in self.objects:\n            obj_img_x = int(320 + obj['position'][0] * 50)\n            obj_img_y = int(240 - obj['position'][1] * 50)\n\n            if obj['type'] == 'furniture':\n                cv2.rectangle(image,\n                            (obj_img_x - 20, obj_img_y - 20),\n                            (obj_img_x + 20, obj_img_y + 20),\n                            (100, 100, 200), -1)\n            else:  # object\n                cv2.circle(image, (obj_img_x, obj_img_y), 10, (200, 100, 100), -1)\n\n            # Label the object\n            cv2.putText(image, obj['name'], (obj_img_x - 15, obj_img_y - 25),\n                       cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n\n        # Add timestamp text\n        cv2.putText(image, f\"Time: {self.sim_time:.1f}s\", (10, 30),\n                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n\n        # Publish image\n        img_msg = self.cv_bridge.cv2_to_imgmsg(image, encoding=\"bgr8\")\n        img_msg.header.stamp = self.get_clock().now().to_msg()\n        img_msg.header.frame_id = 'camera_rgb_optical_frame'\n        self.camera_pub.publish(img_msg)\n\n    def publish_world_state(self):\n        \"\"\"Publish current world state\"\"\"\n        world_state = {\n            'timestamp': self.sim_time,\n            'robot_position': self.robot_position.tolist(),\n            'robot_orientation': 0.0,  # Facing angle\n            'objects': [\n                {\n                    'name': obj['name'],\n                    'position': obj['position'].tolist(),\n                    'type': obj['type'],\n                    'visible': True  # All objects are visible in simulation\n                }\n                for obj in self.objects\n            ],\n            'navigation_map': 'simulated_map',\n            'battery_level': 0.85\n        }\n\n        state_msg = String()\n        state_msg.data = json.dumps(world_state)\n        self.world_state_pub.publish(state_msg)\n\n    def publish_random_command(self):\n        \"\"\"Publish a random voice command\"\"\"\n        if self.commands:\n            command = random.choice(self.commands)\n            cmd_msg = String()\n            cmd_msg.data = command\n            self.voice_cmd_pub.publish(cmd_msg)\n\n            self.get_logger().info(f\"Published command: '{command}'\")\n\n    def vla_result_callback(self, msg):\n        \"\"\"Process VLA results\"\"\"\n        try:\n            result = json.loads(msg.data)\n            self.get_logger().info(f\"VLA Result: Command '{result['command']}' -> Actions: {result['actions']}\")\n        except json.JSONDecodeError:\n            self.get_logger().error(\"Error parsing VLA result\")\n\n    def robot_action_callback(self, msg):\n        \"\"\"Process robot actions\"\"\"\n        try:\n            action_data = json.loads(msg.data)\n            action = action_data['action']\n\n            self.get_logger().info(f\"Robot executing: {action['action']} - {action['description']}\")\n\n            # Update robot state based on action\n            if action['action'] == 'navigate' and 'target' in action:\n                # Move robot towards target\n                target_name = action['target']\n\n                # Find target position\n                target_pos = None\n                for obj in self.objects:\n                    if obj['name'] == target_name or target_name in obj['name']:\n                        target_pos = obj['position']\n                        break\n\n                if target_pos is not None:\n                    # Simple navigation: move towards target\n                    direction = target_pos - self.robot_position\n                    distance = np.linalg.norm(direction)\n                    if distance > 0.1:  # If not already at target\n                        # Move 0.1 units towards target\n                        move_vector = direction / distance * min(0.1, distance)\n                        self.robot_position += move_vector\n\n        except json.JSONDecodeError:\n            self.get_logger().error(\"Error parsing robot action\")\n\n    def simulation_step(self):\n        \"\"\"Main simulation step\"\"\"\n        self.sim_time += 0.05  # 20 Hz\n\n        # Publish status\n        status_msg = String()\n        status_msg.data = f\"Time: {self.sim_time:.2f}s, Robot: ({self.robot_position[0]:.2f}, {self.robot_position[1]:.2f}), \" \\\n                         f\"Objects: {len(self.objects)}, Commands: {len(self.commands)}\"\n        self.status_pub.publish(status_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    simulator = VLAIntegrationSimulator()\n\n    try:\n        rclpy.spin(simulator)\n    except KeyboardInterrupt:\n        simulator.get_logger().info(\"VLA Integration Simulator stopped by user\")\n    finally:\n        simulator.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"quick-recap",children:"Quick Recap"}),"\n",(0,a.jsx)(e.p,{children:"In this lesson, we've covered:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"VLA Architecture"}),": Understanding the components that connect vision, language, and action systems"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Perception Pipeline"}),": Creating visual perception for environmental understanding"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Language Processing"}),": Implementing natural language understanding for commands"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Action Mapping"}),": Connecting language understanding to robot action execution"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Integration Testing"}),": Complete VLA pipeline with simulation environment"]}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:"The Vision-Language-Action (VLA) framework represents a significant advancement in robotics, enabling more intuitive human-robot interaction. By combining visual perception, natural language understanding, and action execution, VLA systems allow robots to interpret high-level, ambiguous human instructions and translate them into specific, executable behaviors."}),"\n",(0,a.jsx)(e.p,{children:"In the next lesson, we'll explore voice-to-action systems using Whisper for speech recognition and how to map voice commands to specific robot actions."})]})}function d(n={}){const{wrapper:e}={...(0,i.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(m,{...n})}):m(n)}},8453(n,e,t){t.d(e,{R:()=>s,x:()=>r});var o=t(6540);const a={},i=o.createContext(a);function s(n){const e=o.useContext(i);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:s(n.components),o.createElement(i.Provider,{value:e},n.children)}}}]);