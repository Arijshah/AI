"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[879],{5723(e,n,t){t.r(n),t.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>i,metadata:()=>o,toc:()=>c});const o=JSON.parse('{"id":"physical-ai/vision-language-action-vla/capstone-project-autonomous-humanoid-robot","title":"Capstone Project: Autonomous Humanoid Robot in Simulation","description":"Overview","source":"@site/docs/physical-ai/vision-language-action-vla/capstone-project-autonomous-humanoid-robot.md","sourceDirName":"physical-ai/vision-language-action-vla","slug":"/physical-ai/vision-language-action-vla/capstone-project-autonomous-humanoid-robot","permalink":"/AI/docs/physical-ai/vision-language-action-vla/capstone-project-autonomous-humanoid-robot","draft":false,"unlisted":false,"editUrl":"https://github.com/arijh/physical-ai-book/tree/main/packages/create-docusaurus/templates/shared/docs/physical-ai/vision-language-action-vla/capstone-project-autonomous-humanoid-robot.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4},"sidebar":"docs","previous":{"title":"Cognitive Planning with LLMs Mapped to ROS 2 Actions","permalink":"/AI/docs/physical-ai/vision-language-action-vla/cognitive-planning-with-llms"}}');var s=t(4848),a=t(8453);const i={sidebar_position:4},r="Capstone Project: Autonomous Humanoid Robot in Simulation",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Hands-on Steps",id:"hands-on-steps",level:2},{value:"Step 1: Set up the Capstone Project Structure",id:"step-1-set-up-the-capstone-project-structure",level:3},{value:"Step 2: Create the Main Autonomous Humanoid Node",id:"step-2-create-the-main-autonomous-humanoid-node",level:3},{value:"Step 3: Create the Vision System Module",id:"step-3-create-the-vision-system-module",level:3},{value:"Step 4: Create the Cognitive Planner Module",id:"step-4-create-the-cognitive-planner-module",level:3},{value:"Step 5: Create the Motion Controller",id:"step-5-create-the-motion-controller",level:3},{value:"Step 6: Create the Navigation System",id:"step-6-create-the-navigation-system",level:3},{value:"Step 7: Create the Main Launch File",id:"step-7-create-the-main-launch-file",level:3},{value:"Step 8: Create a Simulation Test Script",id:"step-8-create-a-simulation-test-script",level:3},{value:"Code Examples",id:"code-examples",level:2},{value:"Small Simulation",id:"small-simulation",level:2},{value:"Quick Recap",id:"quick-recap",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"capstone-project-autonomous-humanoid-robot-in-simulation",children:"Capstone Project: Autonomous Humanoid Robot in Simulation"})}),"\n",(0,s.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,s.jsx)(n.p,{children:"This capstone lesson integrates all the concepts from the Vision-Language-Action (VLA) chapter into a complete autonomous humanoid robot system. We'll create a robot that can understand natural language commands, plan complex tasks using LLMs, perceive its environment, and execute sophisticated behaviors in simulation. This project demonstrates the full VLA pipeline in action."}),"\n",(0,s.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,s.jsx)(n.p,{children:"By the end of this lesson, you will:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Integrate VLA components into a complete autonomous system"}),"\n",(0,s.jsx)(n.li,{children:"Implement a multi-modal perception system combining vision, language, and action"}),"\n",(0,s.jsx)(n.li,{children:"Create a humanoid robot controller that responds to voice commands"}),"\n",(0,s.jsx)(n.li,{children:"Design a cognitive architecture for autonomous decision-making"}),"\n",(0,s.jsx)(n.li,{children:"Validate the system in simulation with complex scenarios"}),"\n",(0,s.jsx)(n.li,{children:"Evaluate the performance of the integrated VLA system"}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"hands-on-steps",children:"Hands-on Steps"}),"\n",(0,s.jsx)(n.h3,{id:"step-1-set-up-the-capstone-project-structure",children:"Step 1: Set up the Capstone Project Structure"}),"\n",(0,s.jsx)(n.p,{children:"First, let's create the main package for our capstone project:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"# Create the capstone project package\ncd ~/ros2_ws/src\nros2 pkg create --dependencies rclpy std_msgs sensor_msgs geometry_msgs vision_msgs nav_msgs moveit_msgs --node-name autonomous_humanoid humanoid_capstone\n\ncd humanoid_capstone\nmkdir -p humanoid_capstone/{perception,planning,control,navigation,utils}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-2-create-the-main-autonomous-humanoid-node",children:"Step 2: Create the Main Autonomous Humanoid Node"}),"\n",(0,s.jsx)(n.p,{children:"Create the central node that orchestrates all VLA components:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# humanoid_capstone/humanoid_capstone/autonomous_humanoid_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nfrom sensor_msgs.msg import Image, LaserScan\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom humanoid_capstone.perception.vision_system import VisionSystem\nfrom humanoid_capstone.planning.cognitive_planner import CognitivePlanner\nfrom humanoid_capstone.control.motion_controller import MotionController\nfrom humanoid_capstone.navigation.path_planner import PathPlanner\nimport asyncio\nfrom concurrent.futures import ThreadPoolExecutor\nimport json\n\nclass AutonomousHumanoidNode(Node):\n    def __init__(self):\n        super().__init__('autonomous_humanoid')\n\n        # Initialize all subsystems\n        self.vision_system = VisionSystem(self)\n        self.cognitive_planner = CognitivePlanner(self)\n        self.motion_controller = MotionController(self)\n        self.path_planner = PathPlanner(self)\n\n        # Publishers\n        self.status_pub = self.create_publisher(String, 'system_status', 10)\n        self.behavior_pub = self.create_publisher(String, 'behavior_command', 10)\n        self.navigation_goal_pub = self.create_publisher(PoseStamped, 'navigation_goal', 10)\n\n        # Subscribers\n        self.voice_command_sub = self.create_subscription(\n            String,\n            'voice_command',\n            self.voice_command_callback,\n            10\n        )\n\n        self.vision_sub = self.create_subscription(\n            Image,\n            'camera/image_raw',\n            self.vision_callback,\n            10\n        )\n\n        self.laser_sub = self.create_subscription(\n            LaserScan,\n            'scan',\n            self.laser_callback,\n            10\n        )\n\n        # Internal state\n        self.current_task = None\n        self.robot_state = {\n            'position': {'x': 0.0, 'y': 0.0, 'theta': 0.0},\n            'battery': 1.0,\n            'detected_objects': [],\n            'navigation_status': 'idle',\n            'current_behavior': 'idle'\n        }\n\n        # Executor for async operations\n        self.executor_pool = ThreadPoolExecutor(max_workers=8)\n        self.asyncio_loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(self.asyncio_loop)\n\n        # Timer for system status updates\n        self.status_timer = self.create_timer(1.0, self.publish_system_status)\n\n        self.get_logger().info('Autonomous Humanoid Node initialized')\n\n    def voice_command_callback(self, msg):\n        \"\"\"Handle voice commands from the voice-to-action system\"\"\"\n        command_text = msg.data\n        self.get_logger().info(f'Received voice command: {command_text}')\n\n        # Process command asynchronously\n        future = asyncio.run_coroutine_threadsafe(\n            self.process_voice_command(command_text),\n            self.asyncio_loop\n        )\n\n    async def process_voice_command(self, command_text: str):\n        \"\"\"Process a voice command through the full VLA pipeline\"\"\"\n        try:\n            self.get_logger().info(f'Processing voice command: {command_text}')\n\n            # Update system status\n            self.update_system_status('processing_command', command_text)\n\n            # Use cognitive planner to generate a plan\n            plan = await self.cognitive_planner.plan_task(command_text, self.robot_state)\n\n            if plan:\n                self.get_logger().info(f'Generated plan with {len(plan)} steps')\n                self.current_task = plan\n\n                # Execute the plan\n                success = await self.execute_plan(plan)\n\n                if success:\n                    self.get_logger().info('Task completed successfully')\n                    self.update_system_status('task_completed', command_text)\n                else:\n                    self.get_logger().error('Task execution failed')\n                    self.update_system_status('task_failed', command_text)\n            else:\n                self.get_logger().error('Failed to generate plan')\n                self.update_system_status('planning_failed', command_text)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing voice command: {str(e)}')\n            self.update_system_status('error', str(e))\n\n    async def execute_plan(self, plan: list) -> bool:\n        \"\"\"Execute a plan step by step\"\"\"\n        self.update_system_status('executing_plan', f'Plan with {len(plan)} steps')\n\n        for i, step in enumerate(plan):\n            self.get_logger().info(f'Executing step {i+1}/{len(plan)}: {step[\"action\"]}')\n\n            try:\n                success = await self.execute_plan_step(step)\n\n                if success:\n                    self.get_logger().info(f'Step {i+1} completed successfully')\n                else:\n                    self.get_logger().error(f'Step {i+1} failed')\n                    return False\n\n            except Exception as e:\n                self.get_logger().error(f'Step {i+1} execution error: {str(e)}')\n                return False\n\n        return True\n\n    async def execute_plan_step(self, step: dict) -> bool:\n        \"\"\"Execute a single step in the plan\"\"\"\n        action_type = step.get('action', '')\n        parameters = step.get('parameters', {})\n\n        if action_type == 'navigate_to':\n            return await self.execute_navigation_step(parameters)\n        elif action_type == 'detect_object':\n            return await self.execute_detection_step(parameters)\n        elif action_type == 'manipulate_object':\n            return await self.execute_manipulation_step(parameters)\n        elif action_type == 'speak':\n            return await self.execute_speech_step(parameters)\n        elif action_type == 'wait':\n            return await self.execute_wait_step(parameters)\n        else:\n            self.get_logger().error(f'Unknown action type: {action_type}')\n            return False\n\n    async def execute_navigation_step(self, params: dict) -> bool:\n        \"\"\"Execute navigation step\"\"\"\n        target_x = params.get('x', 0.0)\n        target_y = params.get('y', 0.0)\n\n        # Plan path using navigation system\n        path = await self.path_planner.plan_path(\n            self.robot_state['position']['x'],\n            self.robot_state['position']['y'],\n            target_x, target_y\n        )\n\n        if path:\n            # Execute navigation\n            success = await self.motion_controller.navigate_to_pose(target_x, target_y)\n            if success:\n                # Update robot state\n                self.robot_state['position']['x'] = target_x\n                self.robot_state['position']['y'] = target_y\n                return True\n\n        return False\n\n    async def execute_detection_step(self, params: dict) -> bool:\n        \"\"\"Execute object detection step\"\"\"\n        target_object = params.get('target_object', 'any')\n\n        # Use vision system to detect objects\n        detections = await self.vision_system.detect_objects(target_object)\n\n        if detections:\n            self.robot_state['detected_objects'] = detections\n            self.get_logger().info(f'Detected {len(detections)} objects')\n            return True\n\n        return False\n\n    async def execute_manipulation_step(self, params: dict) -> bool:\n        \"\"\"Execute manipulation step\"\"\"\n        object_name = params.get('object', '')\n        action = params.get('manipulation_action', 'pick_up')\n\n        # For now, just log the action\n        # In a real implementation, this would control the robot's arms\n        self.get_logger().info(f'Manipulation: {action} {object_name}')\n        return True\n\n    async def execute_speech_step(self, params: dict) -> bool:\n        \"\"\"Execute speech step\"\"\"\n        text = params.get('text', '')\n        self.get_logger().info(f'Speaking: {text}')\n        # In a real implementation, this would use TTS\n        return True\n\n    async def execute_wait_step(self, params: dict) -> bool:\n        \"\"\"Execute wait step\"\"\"\n        duration = params.get('duration', 1.0)\n        import time\n        time.sleep(duration)  # In async context, use asyncio.sleep\n        return True\n\n    def vision_callback(self, msg):\n        \"\"\"Handle incoming vision data\"\"\"\n        # Process image through vision system\n        future = asyncio.run_coroutine_threadsafe(\n            self.vision_system.process_image(msg),\n            self.asyncio_loop\n        )\n\n    def laser_callback(self, msg):\n        \"\"\"Handle incoming laser scan data\"\"\"\n        # Update navigation safety based on laser data\n        self.path_planner.update_obstacles(msg)\n\n    def update_system_status(self, status: str, details: str = \"\"):\n        \"\"\"Update and publish system status\"\"\"\n        status_msg = String()\n        status_msg.data = json.dumps({\n            'status': status,\n            'details': details,\n            'timestamp': self.get_clock().now().nanoseconds,\n            'robot_state': self.robot_state\n        })\n        self.status_pub.publish(status_msg)\n\n        # Update internal state\n        self.robot_state['current_behavior'] = status\n\n    def publish_system_status(self):\n        \"\"\"Publish periodic system status updates\"\"\"\n        self.update_system_status('operational')\n\n    def destroy_node(self):\n        \"\"\"Clean up resources\"\"\"\n        self.executor_pool.shutdown(wait=True)\n        super().destroy_node()\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = AutonomousHumanoidNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info('Shutting down Autonomous Humanoid Node')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-3-create-the-vision-system-module",children:"Step 3: Create the Vision System Module"}),"\n",(0,s.jsx)(n.p,{children:"Now let's create the vision system that will handle perception tasks:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# humanoid_capstone/humanoid_capstone/perception/vision_system.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray, ObjectHypothesisWithPose\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport asyncio\nimport logging\n\nclass VisionSystem:\n    def __init__(self, node):\n        self.node = node\n        self.logger = node.get_logger()\n        self.cv_bridge = CvBridge()\n\n        # Initialize vision processing components\n        self.object_detector = self._init_object_detector()\n        self.pose_estimator = self._init_pose_estimator()\n\n        # Store recent detections\n        self.recent_detections = []\n\n    def _init_object_detector(self):\n        \"\"\"Initialize object detection model\"\"\"\n        # For simulation, we'll use a simple color-based detector\n        # In real implementation, this would be YOLO, Detectron2, etc.\n        return ColorBasedDetector()\n\n    def _init_pose_estimator(self):\n        \"\"\"Initialize pose estimation\"\"\"\n        # For simulation, we'll use simple geometric pose estimation\n        return GeometricPoseEstimator()\n\n    async def process_image(self, image_msg: Image):\n        \"\"\"Process incoming image message\"\"\"\n        try:\n            # Convert ROS image to OpenCV format\n            cv_image = self.cv_bridge.imgmsg_to_cv2(image_msg, desired_encoding='bgr8')\n\n            # Perform object detection\n            detections = self.object_detector.detect(cv_image)\n\n            # Estimate poses\n            for detection in detections:\n                pose = self.pose_estimator.estimate_pose(cv_image, detection)\n                detection['pose'] = pose\n\n            # Update recent detections\n            self.recent_detections = detections\n\n            return detections\n\n        except Exception as e:\n            self.logger.error(f'Error processing image: {str(e)}')\n            return []\n\n    async def detect_objects(self, target_object: str = \"any\"):\n        \"\"\"Detect specific objects in the current view\"\"\"\n        # In simulation, return predefined objects\n        if target_object.lower() == \"any\":\n            return self.recent_detections\n        else:\n            # Filter detections by target object\n            filtered = [obj for obj in self.recent_detections\n                       if target_object.lower() in obj.get('class', '').lower()]\n            return filtered\n\n    def get_object_location(self, object_name: str):\n        \"\"\"Get the location of a specific object\"\"\"\n        for detection in self.recent_detections:\n            if detection.get('class', '').lower() == object_name.lower():\n                return detection.get('pose', {}).get('position', {})\n        return None\n\n    def has_clear_path_to_object(self, object_name: str):\n        \"\"\"Check if there's a clear path to the object\"\"\"\n        # This would check navigation map for obstacles\n        # For simulation, assume path is clear if object is detected\n        obj_location = self.get_object_location(object_name)\n        return obj_location is not None\n\n\nclass ColorBasedDetector:\n    \"\"\"Simple color-based object detector for simulation\"\"\"\n    def __init__(self):\n        # Define color ranges for different objects\n        self.color_ranges = {\n            'red_cup': ([0, 50, 50], [10, 255, 255]),\n            'blue_box': ([100, 50, 50], [130, 255, 255]),\n            'green_ball': ([40, 50, 50], [80, 255, 255]),\n            'yellow_book': ([20, 50, 50], [30, 255, 255])\n        }\n\n    def detect(self, image):\n        \"\"\"Detect objects based on color ranges\"\"\"\n        detections = []\n\n        for obj_name, (lower, upper) in self.color_ranges.items():\n            # Create mask for color range\n            hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n            mask = cv2.inRange(hsv, np.array(lower), np.array(upper))\n\n            # Find contours\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n\n            for contour in contours:\n                if cv2.contourArea(contour) > 100:  # Filter small contours\n                    # Get bounding box\n                    x, y, w, h = cv2.boundingRect(contour)\n\n                    detection = {\n                        'class': obj_name,\n                        'confidence': 0.8,  # Simulation confidence\n                        'bbox': {'x': x, 'y': y, 'width': w, 'height': h},\n                        'center': {'x': x + w/2, 'y': y + h/2}\n                    }\n                    detections.append(detection)\n\n        return detections\n\n\nclass GeometricPoseEstimator:\n    \"\"\"Simple geometric pose estimator for simulation\"\"\"\n    def __init__(self):\n        self.focal_length = 500  # Simulation focal length\n        self.known_object_sizes = {\n            'red_cup': 0.1,  # 10cm diameter\n            'blue_box': 0.15,  # 15cm side\n            'green_ball': 0.08,  # 8cm diameter\n            'yellow_book': 0.2  # 20cm length\n        }\n\n    def estimate_pose(self, image, detection):\n        \"\"\"Estimate 3D pose from 2D detection\"\"\"\n        obj_class = detection.get('class', '')\n        bbox = detection.get('bbox', {})\n        center = detection.get('center', {})\n\n        if obj_class in self.known_object_sizes:\n            # Estimate distance based on object size in image\n            pixel_size = max(bbox.get('width', 1), bbox.get('height', 1))\n            known_size = self.known_object_sizes[obj_class]\n\n            # Simple distance estimation (in simulation units)\n            distance = (known_size * self.focal_length) / pixel_size\n\n            # Convert image coordinates to world coordinates\n            # This is a simplified simulation model\n            world_x = (center['x'] - image.shape[1]/2) * distance / self.focal_length\n            world_y = (center['y'] - image.shape[0]/2) * distance / self.focal_length\n\n            pose = {\n                'position': {'x': world_x, 'y': world_y, 'z': distance},\n                'orientation': {'w': 1.0, 'x': 0.0, 'y': 0.0, 'z': 0.0}\n            }\n\n            return pose\n\n        return {'position': {'x': 0, 'y': 0, 'z': 1}, 'orientation': {'w': 1.0, 'x': 0.0, 'y': 0.0, 'z': 0.0}}\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-4-create-the-cognitive-planner-module",children:"Step 4: Create the Cognitive Planner Module"}),"\n",(0,s.jsx)(n.p,{children:"Now let's create the cognitive planner that will use LLMs for high-level reasoning:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# humanoid_capstone/humanoid_capstone/planning/cognitive_planner.py\nimport openai\nimport json\nfrom typing import Dict, List, Any, Optional\nimport asyncio\nimport logging\n\nclass CognitivePlanner:\n    def __init__(self, node):\n        self.node = node\n        self.logger = node.get_logger()\n\n        # Initialize LLM client\n        self.client = openai.OpenAI()  # Configure with your API key\n\n        self.system_prompt = """\n        You are a cognitive planning assistant for an autonomous humanoid robot. Your task is to decompose high-level human commands into detailed, executable plans that the robot can follow.\n\n        The robot has the following capabilities:\n        - Navigation: Move to specific locations (x, y coordinates)\n        - Object Detection: Identify and locate objects in the environment\n        - Manipulation: Pick up and place objects (simulation only)\n        - Speech: Communicate with humans\n        - Perception: Understand the environment through vision and sensors\n\n        Available actions:\n        1. navigate_to: Move robot to a specific location\n           Parameters: x (float), y (float)\n        2. detect_object: Look for a specific object\n           Parameters: target_object (string)\n        3. manipulate_object: Pick up or place an object\n           Parameters: object (string), manipulation_action (string: "pick_up", "place")\n        4. speak: Say something to the human\n           Parameters: text (string)\n        5. wait: Wait for a certain duration\n           Parameters: duration (float)\n\n        When creating plans:\n        1. Break down complex tasks into simple, sequential steps\n        2. Consider the robot\'s current state and environment\n        3. Include necessary perception steps before manipulation\n        4. Add verification steps after critical actions\n        5. Consider safety and feasibility\n\n        Respond in JSON format with the following structure:\n        {\n            "plan": [\n                {\n                    "id": 1,\n                    "action": "action_name",\n                    "description": "What the robot should do",\n                    "parameters": {"param1": "value1"},\n                    "required_conditions": ["condition1"],\n                    "expected_outcomes": ["outcome1"]\n                }\n            ]\n        }\n        """\n\n    async def plan_task(self, goal: str, robot_state: Dict[str, Any]) -> Optional[List[Dict[str, Any]]]:\n        """Generate a plan for a given goal using LLM"""\n        try:\n            # Create detailed prompt with context\n            prompt = f"""\n            Robot State: {json.dumps(robot_state, indent=2)}\n\n            Human Goal: {goal}\n\n            Please create a detailed plan for the robot to achieve this goal. Consider the current state and environment.\n            """\n\n            response = await self.client.chat.completions.create(\n                model="gpt-4",  # Use appropriate model\n                messages=[\n                    {"role": "system", "content": self.system_prompt},\n                    {"role": "user", "content": prompt}\n                ],\n                temperature=0.1,\n                max_tokens=2048,\n                response_format={"type": "json_object"}\n            )\n\n            plan_json = response.choices[0].message.content\n            plan_data = json.loads(plan_json)\n\n            return plan_data.get(\'plan\', [])\n\n        except Exception as e:\n            self.logger.error(f"Error generating plan: {str(e)}")\n            return None\n\n    async def refine_plan(self, plan: List[Dict[str, Any]], feedback: str) -> Optional[List[Dict[str, Any]]]:\n        """Refine an existing plan based on feedback"""\n        try:\n            prompt = f"""\n            Current Plan: {json.dumps(plan, indent=2)}\n\n            Feedback: {feedback}\n\n            Please refine the plan to address the feedback while maintaining the original goal.\n            """\n\n            response = await self.client.chat.completions.create(\n                model="gpt-4",\n                messages=[\n                    {"role": "system", "content": self.system_prompt},\n                    {"role": "user", "content": prompt}\n                ],\n                temperature=0.1,\n                max_tokens=2048,\n                response_format={"type": "json_object"}\n            )\n\n            plan_json = response.choices[0].message.content\n            refined_plan = json.loads(plan_json)\n\n            return refined_plan.get(\'plan\', [])\n\n        except Exception as e:\n            self.logger.error(f"Error refining plan: {str(e)}")\n            return None\n\n    async def validate_plan(self, plan: List[Dict[str, Any]], environment_state: Dict[str, Any]) -> Dict[str, Any]:\n        """Validate if a plan is feasible in the current environment"""\n        try:\n            prompt = f"""\n            Plan: {json.dumps(plan, indent=2)}\n\n            Environment State: {json.dumps(environment_state, indent=2)}\n\n            Please evaluate if this plan is feasible and safe. Return a JSON object with:\n            {{\n                "feasible": true/false,\n                "issues": ["issue1", "issue2"],\n                "confidence": 0.0-1.0,\n                "suggestions": ["suggestion1", "suggestion2"]\n            }}\n            """\n\n            response = await self.client.chat.completions.create(\n                model="gpt-4",\n                messages=[\n                    {"role": "system", "content": self.system_prompt},\n                    {"role": "user", "content": prompt}\n                ],\n                temperature=0.1,\n                max_tokens=1024,\n                response_format={"type": "json_object"}\n            )\n\n            validation = json.loads(response.choices[0].message.content)\n            return validation\n\n        except Exception as e:\n            self.logger.error(f"Error validating plan: {str(e)}")\n            return {"feasible": False, "issues": [str(e)], "confidence": 0.0, "suggestions": []}\n\n    def analyze_plan_complexity(self, plan: List[Dict[str, Any]]) -> Dict[str, Any]:\n        """Analyze the complexity and risk of a plan"""\n        complexity = {\n            "total_steps": len(plan),\n            "action_types": set(),\n            "estimated_duration": 0.0,\n            "risk_level": "low",\n            "dependencies": 0\n        }\n\n        for step in plan:\n            action_type = step.get("action", "unknown")\n            complexity["action_types"].add(action_type)\n\n            # Estimate time for each action type\n            if action_type == "navigate_to":\n                complexity["estimated_duration"] += 10.0  # 10 seconds per navigation\n            elif action_type == "detect_object":\n                complexity["estimated_duration"] += 5.0\n            elif action_type == "manipulate_object":\n                complexity["estimated_duration"] += 8.0\n            elif action_type == "speak":\n                complexity["estimated_duration"] += 2.0\n            elif action_type == "wait":\n                complexity["estimated_duration"] += step.get("parameters", {}).get("duration", 1.0)\n\n        complexity["action_types"] = list(complexity["action_types"])\n\n        # Determine risk based on plan length and action types\n        if len(plan) > 10:\n            complexity["risk_level"] = "high"\n        elif len(plan) > 5:\n            complexity["risk_level"] = "medium"\n\n        return complexity\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-5-create-the-motion-controller",children:"Step 5: Create the Motion Controller"}),"\n",(0,s.jsx)(n.p,{children:"Now let's create the motion controller for the humanoid robot:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# humanoid_capstone/humanoid_capstone/control/motion_controller.py\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom nav_msgs.msg import Path\nfrom humanoid_capstone.utils.humanoid_kinematics import HumanoidKinematics\nimport asyncio\nimport math\n\nclass MotionController:\n    def __init__(self, node):\n        self.node = node\n        self.logger = node.get_logger()\n\n        # Initialize publishers for different motion types\n        self.cmd_vel_pub = node.create_publisher(Twist, 'cmd_vel', 10)\n        self.body_pose_pub = node.create_publisher(PoseStamped, 'body_pose', 10)\n\n        # Initialize humanoid kinematics\n        self.kinematics = HumanoidKinematics()\n\n        # Robot state\n        self.current_pose = {'x': 0.0, 'y': 0.0, 'theta': 0.0}\n        self.is_moving = False\n\n    async def navigate_to_pose(self, target_x: float, target_y: float) -> bool:\n        \"\"\"Navigate to a target pose using simple proportional control\"\"\"\n        try:\n            self.logger.info(f'Navigating to ({target_x}, {target_y})')\n\n            # Simple navigation using proportional control\n            distance_threshold = 0.1  # 10cm threshold\n            angular_threshold = 0.1   # 0.1 rad threshold\n\n            while True:\n                # Calculate distance and angle to target\n                dx = target_x - self.current_pose['x']\n                dy = target_y - self.current_pose['y']\n                distance = math.sqrt(dx*dx + dy*dy)\n\n                target_angle = math.atan2(dy, dx)\n                angle_diff = target_angle - self.current_pose['theta']\n\n                # Normalize angle difference\n                while angle_diff > math.pi:\n                    angle_diff -= 2 * math.pi\n                while angle_diff < -math.pi:\n                    angle_diff += 2 * math.pi\n\n                # Check if we're close enough\n                if distance < distance_threshold and abs(angle_diff) < angular_threshold:\n                    # Stop the robot\n                    self._stop_robot()\n                    self.logger.info('Navigation completed')\n                    return True\n\n                # Create velocity command\n                cmd = Twist()\n\n                # Proportional control for linear velocity\n                if distance > distance_threshold:\n                    cmd.linear.x = min(0.5, distance * 0.5)  # Max 0.5 m/s\n                else:\n                    cmd.linear.x = 0.0\n\n                # Proportional control for angular velocity\n                if abs(angle_diff) > angular_threshold:\n                    cmd.angular.z = max(-0.5, min(0.5, angle_diff * 1.0))  # Max 0.5 rad/s\n                else:\n                    cmd.angular.z = 0.0\n\n                # Publish command\n                self.cmd_vel_pub.publish(cmd)\n\n                # Update current pose (in simulation, assume perfect odometry)\n                self.current_pose['x'] += cmd.linear.x * 0.1 * math.cos(self.current_pose['theta'])\n                self.current_pose['y'] += cmd.linear.x * 0.1 * math.sin(self.current_pose['theta'])\n                self.current_pose['theta'] += cmd.angular.z * 0.1\n\n                # Small delay\n                await asyncio.sleep(0.1)\n\n        except Exception as e:\n            self.logger.error(f'Navigation error: {str(e)}')\n            self._stop_robot()\n            return False\n\n    def _stop_robot(self):\n        \"\"\"Stop all robot motion\"\"\"\n        cmd = Twist()\n        self.cmd_vel_pub.publish(cmd)\n        self.is_moving = False\n\n    async def execute_gesture(self, gesture_name: str) -> bool:\n        \"\"\"Execute a predefined gesture\"\"\"\n        try:\n            self.logger.info(f'Executing gesture: {gesture_name}')\n\n            # Define gesture patterns\n            gestures = {\n                'wave': self._execute_wave_gesture,\n                'point': self._execute_point_gesture,\n                'greet': self._execute_greet_gesture,\n                'think': self._execute_think_gesture\n            }\n\n            if gesture_name in gestures:\n                return await gestures[gesture_name]()\n            else:\n                self.logger.error(f'Unknown gesture: {gesture_name}')\n                return False\n\n        except Exception as e:\n            self.logger.error(f'Gesture execution error: {str(e)}')\n            return False\n\n    async def _execute_wave_gesture(self) -> bool:\n        \"\"\"Execute waving gesture\"\"\"\n        # In simulation, just log the action\n        self.logger.info('Executing wave gesture')\n        await asyncio.sleep(2.0)  # Simulate gesture duration\n        return True\n\n    async def _execute_point_gesture(self) -> bool:\n        \"\"\"Execute pointing gesture\"\"\"\n        self.logger.info('Executing point gesture')\n        await asyncio.sleep(1.5)\n        return True\n\n    async def _execute_greet_gesture(self) -> bool:\n        \"\"\"Execute greeting gesture\"\"\"\n        self.logger.info('Executing greet gesture')\n        await asyncio.sleep(2.0)\n        return True\n\n    async def _execute_think_gesture(self) -> bool:\n        \"\"\"Execute thinking gesture (tilt head, etc.)\"\"\"\n        self.logger.info('Executing think gesture')\n        await asyncio.sleep(1.0)\n        return True\n\n    async def balance_control(self) -> bool:\n        \"\"\"Maintain balance for humanoid robot\"\"\"\n        # Implement balance control algorithms\n        # This would involve PID control of joint positions\n        # to maintain center of mass over support polygon\n        self.logger.info('Balance control active')\n        return True\n\n    def update_robot_pose(self, pose_msg: PoseStamped):\n        \"\"\"Update robot's current pose from localization system\"\"\"\n        self.current_pose['x'] = pose_msg.pose.position.x\n        self.current_pose['y'] = pose_msg.pose.position.y\n        # Convert quaternion to euler for theta\n        quat = pose_msg.pose.orientation\n        self.current_pose['theta'] = math.atan2(\n            2 * (quat.w * quat.z + quat.x * quat.y),\n            1 - 2 * (quat.y * quat.y + quat.z * quat.z)\n        )\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-6-create-the-navigation-system",children:"Step 6: Create the Navigation System"}),"\n",(0,s.jsx)(n.p,{children:"Now let's create the path planning and navigation system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# humanoid_capstone/humanoid_capstone/navigation/path_planner.py\nimport rclpy\nfrom rclpy.node import Node\nfrom nav_msgs.msg import OccupancyGrid, Path\nfrom sensor_msgs.msg import LaserScan\nfrom geometry_msgs.msg import PoseStamped\nimport numpy as np\nimport heapq\nfrom typing import List, Tuple, Optional\nimport asyncio\n\nclass PathPlanner:\n    def __init__(self, node):\n        self.node = node\n        self.logger = node.get_logger()\n\n        # Initialize navigation map\n        self.navigation_map = None\n        self.map_resolution = 0.05  # 5cm resolution\n        self.map_origin = {\'x\': 0.0, \'y\': 0.0}\n\n        # Store obstacles from laser scans\n        self.obstacles = []\n\n    async def plan_path(self, start_x: float, start_y: float, goal_x: float, goal_y: float) -> Optional[List[Tuple[float, float]]]:\n        """Plan a path from start to goal using A* algorithm"""\n        try:\n            self.logger.info(f\'Planning path from ({start_x}, {start_y}) to ({goal_x}, {goal_y})\')\n\n            # For simulation, create a simple grid-based map\n            # In real implementation, this would use the actual navigation map\n            grid_size = 100  # 100x100 grid\n            grid = self._create_simulation_grid(grid_size)\n\n            # Convert world coordinates to grid coordinates\n            start_grid = self._world_to_grid(start_x, start_y, grid_size)\n            goal_grid = self._world_to_grid(goal_x, goal_y, grid_size)\n\n            # Check if start and goal are valid\n            if not self._is_valid_cell(start_grid[0], start_grid[1], grid) or \\\n               not self._is_valid_cell(goal_grid[0], goal_grid[1], grid):\n                self.logger.error(\'Start or goal position is invalid\')\n                return None\n\n            # Run A* pathfinding\n            path = self._a_star(grid, start_grid, goal_grid)\n\n            if path:\n                # Convert grid path back to world coordinates\n                world_path = []\n                for grid_x, grid_y in path:\n                    world_x, world_y = self._grid_to_world(grid_x, grid_y, grid_size)\n                    world_path.append((world_x, world_y))\n\n                self.logger.info(f\'Found path with {len(world_path)} waypoints\')\n                return world_path\n\n        except Exception as e:\n            self.logger.error(f\'Path planning error: {str(e)}\')\n\n        return None\n\n    def _create_simulation_grid(self, size: int) -> np.ndarray:\n        """Create a simulation grid with some obstacles"""\n        grid = np.zeros((size, size), dtype=np.uint8)\n\n        # Add some static obstacles (for simulation)\n        # These represent walls or furniture in the environment\n        obstacles = [\n            (20, 20, 5, 5),    # x, y, width, height\n            (60, 30, 10, 3),\n            (40, 70, 3, 10),\n            (80, 80, 5, 5)\n        ]\n\n        for x, y, w, h in obstacles:\n            if x + w < size and y + h < size:\n                grid[y:y+h, x:x+w] = 1  # 1 = occupied\n\n        return grid\n\n    def _a_star(self, grid: np.ndarray, start: Tuple[int, int], goal: Tuple[int, int]) -> Optional[List[Tuple[int, int]]]:\n        """A* pathfinding algorithm"""\n        rows, cols = grid.shape\n\n        # Directions: up, down, left, right, and diagonals\n        directions = [\n            (-1, -1), (-1, 0), (-1, 1),\n            (0, -1),           (0, 1),\n            (1, -1),  (1, 0),  (1, 1)\n        ]\n\n        # Cost for diagonal movement\n        diagonal_cost = 1.414\n\n        # Initialize open and closed sets\n        open_set = [(0, start)]\n        came_from = {}\n        g_score = {start: 0}\n        f_score = {start: self._heuristic(start, goal)}\n\n        while open_set:\n            current = heapq.heappop(open_set)[1]\n\n            if current == goal:\n                # Reconstruct path\n                path = []\n                while current in came_from:\n                    path.append(current)\n                    current = came_from[current]\n                path.append(start)\n                path.reverse()\n                return path\n\n            for i, (dx, dy) in enumerate(directions):\n                neighbor = (current[0] + dx, current[1] + dy)\n\n                # Check bounds\n                if not (0 <= neighbor[0] < cols and 0 <= neighbor[1] < rows):\n                    continue\n\n                # Check if cell is walkable\n                if grid[neighbor[1], neighbor[0]] == 1:  # Occupied\n                    continue\n\n                # Calculate movement cost\n                move_cost = diagonal_cost if i >= 4 else 1.0  # Diagonal vs cardinal\n                tentative_g_score = g_score[current] + move_cost\n\n                if neighbor not in g_score or tentative_g_score < g_score[neighbor]:\n                    came_from[neighbor] = current\n                    g_score[neighbor] = tentative_g_score\n                    f_score[neighbor] = tentative_g_score + self._heuristic(neighbor, goal)\n                    heapq.heappush(open_set, (f_score[neighbor], neighbor))\n\n        return None  # No path found\n\n    def _heuristic(self, a: Tuple[int, int], b: Tuple[int, int]) -> float:\n        """Heuristic function for A* (Euclidean distance)"""\n        return math.sqrt((a[0] - b[0])**2 + (a[1] - b[1])**2)\n\n    def _is_valid_cell(self, x: int, y: int, grid: np.ndarray) -> bool:\n        """Check if a cell is valid (within bounds and not occupied)"""\n        rows, cols = grid.shape\n        return (0 <= x < cols and 0 <= y < rows and grid[y, x] == 0)\n\n    def _world_to_grid(self, x: float, y: float, grid_size: int) -> Tuple[int, int]:\n        """Convert world coordinates to grid coordinates"""\n        grid_x = int((x - self.map_origin[\'x\']) / self.map_resolution)\n        grid_y = int((y - self.map_origin[\'y\']) / self.map_resolution)\n        return (max(0, min(grid_size-1, grid_x)), max(0, min(grid_size-1, grid_y)))\n\n    def _grid_to_world(self, grid_x: int, grid_y: int, grid_size: int) -> Tuple[float, float]:\n        """Convert grid coordinates to world coordinates"""\n        world_x = grid_x * self.map_resolution + self.map_origin[\'x\']\n        world_y = grid_y * self.map_resolution + self.map_origin[\'y\']\n        return (world_x, world_y)\n\n    def update_obstacles(self, laser_scan: LaserScan):\n        """Update obstacle map based on laser scan data"""\n        try:\n            # Process laser scan to detect obstacles\n            angle_min = laser_scan.angle_min\n            angle_increment = laser_scan.angle_increment\n\n            new_obstacles = []\n            for i, range_val in enumerate(laser_scan.ranges):\n                if not (laser_scan.range_min <= range_val <= laser_scan.range_max):\n                    continue  # Invalid range\n\n                angle = angle_min + i * angle_increment\n                x = range_val * math.cos(angle)\n                y = range_val * math.sin(angle)\n\n                # Convert to grid coordinates and mark as occupied\n                new_obstacles.append((x, y))\n\n            self.obstacles = new_obstacles\n            self.logger.debug(f\'Updated obstacles from laser scan: {len(new_obstacles)} detected\')\n\n        except Exception as e:\n            self.logger.error(f\'Error processing laser scan: {str(e)}\')\n\n    def is_path_clear(self, path: List[Tuple[float, float]]) -> bool:\n        """Check if a path is clear of obstacles"""\n        # For simulation, assume path is clear if no dynamic obstacles detected\n        # In real implementation, this would check the path against the costmap\n        return len(self.obstacles) == 0\n'})}),"\n",(0,s.jsx)(n.h3,{id:"step-7-create-the-main-launch-file",children:"Step 7: Create the Main Launch File"}),"\n",(0,s.jsx)(n.p,{children:"Now let's create a launch file to bring up the entire system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# humanoid_capstone/launch/autonomous_humanoid.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import DeclareLaunchArgument, IncludeLaunchDescription\nfrom launch.substitutions import LaunchConfiguration\nfrom launch.launch_description_sources import PythonLaunchDescriptionSource\nfrom ament_index_python.packages import get_package_share_directory\nimport os\n\ndef generate_launch_description():\n    # Declare launch arguments\n    ld = LaunchDescription([\n        DeclareLaunchArgument(\n            'use_sim_time',\n            default_value='true',\n            description='Use simulation (Gazebo) clock if true'\n        ),\n\n        DeclareLaunchArgument(\n            'robot_model',\n            default_value='humanoid',\n            description='Robot model to use'\n        )\n    ])\n\n    # Autonomous Humanoid Node\n    autonomous_humanoid_node = Node(\n        package='humanoid_capstone',\n        executable='autonomous_humanoid_node',\n        name='autonomous_humanoid',\n        parameters=[\n            {'use_sim_time': LaunchConfiguration('use_sim_time')},\n            {'robot_model': LaunchConfiguration('robot_model')}\n        ],\n        output='screen'\n    )\n\n    # Voice Recognition Node (from previous lesson)\n    voice_recognition_node = Node(\n        package='voice_to_action',\n        executable='voice_recognition_node',\n        name='voice_recognition',\n        parameters=[\n            {'use_sim_time': LaunchConfiguration('use_sim_time')}\n        ],\n        output='screen'\n    )\n\n    # Vision Processing Node\n    vision_node = Node(\n        package='humanoid_capstone',\n        executable='vision_processor_node',\n        name='vision_processor',\n        parameters=[\n            {'use_sim_time': LaunchConfiguration('use_sim_time')}\n        ],\n        output='screen'\n    )\n\n    # Navigation Node\n    navigation_node = Node(\n        package='nav2_bringup',\n        executable='nav2_launch',\n        name='navigation_system',\n        parameters=[\n            {'use_sim_time': LaunchConfiguration('use_sim_time')}\n        ],\n        output='screen'\n    )\n\n    # Add all nodes to launch description\n    ld.add_action(autonomous_humanoid_node)\n    ld.add_action(voice_recognition_node)\n    ld.add_action(vision_node)\n    ld.add_action(navigation_node)\n\n    return ld\n"})}),"\n",(0,s.jsx)(n.h3,{id:"step-8-create-a-simulation-test-script",children:"Step 8: Create a Simulation Test Script"}),"\n",(0,s.jsx)(n.p,{children:"Finally, let's create a comprehensive test script that demonstrates the complete system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# humanoid_capstone/test/capstone_demo.py\n#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import PoseStamped\nimport time\nimport asyncio\n\nclass CapstoneDemo(Node):\n    def __init__(self):\n        super().__init__('capstone_demo')\n\n        # Publisher for voice commands\n        self.voice_cmd_pub = self.create_publisher(String, 'voice_command', 10)\n\n        # Publisher for system status requests\n        self.status_req_pub = self.create_publisher(String, 'status_request', 10)\n\n        self.demo_commands = [\n            \"Navigate to the kitchen and find the red cup\",\n            \"Pick up the blue box and place it on the table\",\n            \"Go to the living room and wait there\",\n            \"Find the green ball and bring it to me\"\n        ]\n\n        self.get_logger().info('Capstone Demo Node initialized')\n\n    def run_demo_scenario(self):\n        \"\"\"Run a complete demo scenario\"\"\"\n        self.get_logger().info('Starting Capstone Demo Scenario')\n\n        for i, command in enumerate(self.demo_commands):\n            self.get_logger().info(f'Executing demo command {i+1}: {command}')\n\n            # Publish voice command\n            cmd_msg = String()\n            cmd_msg.data = command\n            self.voice_cmd_pub.publish(cmd_msg)\n\n            # Wait for execution to complete\n            time.sleep(15)  # Wait 15 seconds per command (simulation time)\n\n            self.get_logger().info(f'Completed command {i+1}')\n\n        self.get_logger().info('Capstone Demo completed successfully!')\n\n    def demonstrate_vla_integration(self):\n        \"\"\"Demonstrate the full VLA pipeline\"\"\"\n        self.get_logger().info('Demonstrating VLA Integration...')\n\n        # 1. Vision: The robot perceives its environment\n        self.get_logger().info('1. Vision System Active - Detecting objects and environment')\n\n        # 2. Language: Processing natural language command\n        command = \"Please go to the kitchen, find the red cup, and bring it to the table\"\n        self.get_logger().info(f'2. Language Understanding - Processing: \"{command}\"')\n\n        # Publish the command\n        cmd_msg = String()\n        cmd_msg.data = command\n        self.voice_cmd_pub.publish(cmd_msg)\n\n        # 3. Action: Executing the plan\n        self.get_logger().info('3. Action Execution - Planning and executing task...')\n\n        # Wait for execution\n        time.sleep(20)\n\n        self.get_logger().info('VLA Integration demonstration complete!')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    demo = CapstoneDemo()\n\n    # Run the demo\n    demo.demonstrate_vla_integration()\n    demo.run_demo_scenario()\n\n    # Keep node alive briefly to see results\n    time.sleep(5)\n\n    demo.destroy_node()\n    rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,s.jsx)(n.p,{children:"Here's a complete example of how to integrate the VLA system with a humanoid robot in simulation:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:'# Example: Complete VLA system integration\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom humanoid_capstone.autonomous_humanoid_node import AutonomousHumanoidNode\n\nclass VLASystemDemo(Node):\n    def __init__(self):\n        super().__init__(\'vla_system_demo\')\n\n        # Initialize the complete VLA system\n        self.vla_system = AutonomousHumanoidNode()\n\n        # Publisher for high-level commands\n        self.command_pub = self.create_publisher(String, \'high_level_goal\', 10)\n\n        self.get_logger().info(\'VLA System Demo initialized\')\n\n    def demonstrate_complex_task(self):\n        """Demonstrate a complex multi-step task"""\n        complex_goal = "I need you to go to the kitchen, find the red cup on the counter, pick it up, then go to the living room and place it on the coffee table. After that, come back and tell me the task is complete."\n\n        self.get_logger().info(f\'Executing complex task: {complex_goal}\')\n\n        cmd_msg = String()\n        cmd_msg.data = complex_goal\n        self.command_pub.publish(cmd_msg)\n\n    def evaluate_system_performance(self):\n        """Evaluate the performance of the VLA system"""\n        # This would include metrics like:\n        # - Task completion rate\n        # - Planning time\n        # - Execution accuracy\n        # - Natural language understanding accuracy\n        # - System response time\n        pass\n\ndef run_vla_evaluation():\n    """Run comprehensive evaluation of the VLA system"""\n    rclpy.init()\n    demo = VLASystemDemo()\n\n    try:\n        demo.demonstrate_complex_task()\n        rclpy.spin_once(demo, timeout_sec=30.0)  # Let it run for 30 seconds\n    except KeyboardInterrupt:\n        pass\n    finally:\n        demo.destroy_node()\n        rclpy.shutdown()\n'})}),"\n",(0,s.jsx)(n.h2,{id:"small-simulation",children:"Small Simulation"}),"\n",(0,s.jsx)(n.p,{children:"Let's create a simple simulation to demonstrate the complete VLA system:"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# humanoid_capstone/simulations/vla_simulation.py\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nimport asyncio\nimport time\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any\n\n@dataclass\nclass RobotState:\n    x: float = 0.0\n    y: float = 0.0\n    theta: float = 0.0\n    battery: float = 1.0\n    holding_object: str = None\n\nclass VLASimulation:\n    def __init__(self):\n        self.robot = RobotState()\n        self.objects = {\n            'red_cup': {'x': 5.0, 'y': 3.0, 'picked_up': False},\n            'blue_box': {'x': -2.0, 'y': 4.0, 'picked_up': False},\n            'green_ball': {'x': 1.0, 'y': -1.0, 'picked_up': False}\n        }\n        self.path_history = [(0, 0)]\n        self.current_task = None\n        self.task_log = []\n\n        # Setup visualization\n        self.fig, self.ax = plt.subplots(figsize=(12, 8))\n        self.robot_plot = self.ax.plot([], [], 'ro', markersize=15, label='Robot')[0]\n        self.path_plot = self.ax.plot([], [], 'b-', alpha=0.7, label='Path')[0]\n        self.objects_plots = {}\n\n        # Set up the environment\n        self.ax.set_xlim(-10, 10)\n        self.ax.set_ylim(-10, 10)\n        self.ax.set_xlabel('X Position (m)')\n        self.ax.set_ylabel('Y Position (m)')\n        self.ax.set_title('VLA System Simulation: Vision-Language-Action Integration')\n        self.ax.grid(True)\n        self.ax.legend()\n\n        # Plot objects\n        for obj_name, obj_data in self.objects.items():\n            color = 'red' if 'red' in obj_name else 'blue' if 'blue' in obj_name else 'green'\n            plot = self.ax.plot([obj_data['x']], [obj_data['y']], f'{color}o', markersize=10, label=obj_name)[0]\n            self.objects_plots[obj_name] = plot\n\n    def process_command(self, command: str):\n        \"\"\"Process a natural language command\"\"\"\n        self.task_log.append(f\"Processing: {command}\")\n        print(f\"VLA System: Processing command - {command}\")\n\n        # Simple command parsing for simulation\n        if 'kitchen' in command.lower() and 'red cup' in command.lower():\n            self.current_task = {'action': 'fetch_object', 'target': 'red_cup', 'destination': (6, 3)}\n        elif 'living room' in command.lower() and 'place' in command.lower():\n            self.current_task = {'action': 'place_object', 'destination': (8, 1)}\n        elif 'find' in command.lower():\n            target = 'red_cup' if 'red cup' in command.lower() else 'blue_box' if 'blue box' in command.lower() else 'green_ball'\n            self.current_task = {'action': 'navigate_to', 'target': target}\n\n    def execute_task(self):\n        \"\"\"Execute the current task\"\"\"\n        if not self.current_task:\n            return\n\n        action = self.current_task['action']\n\n        if action == 'navigate_to':\n            target_obj = self.current_task['target']\n            target_x = self.objects[target_obj]['x']\n            target_y = self.objects[target_obj]['y']\n\n            # Move towards target\n            dx = target_x - self.robot.x\n            dy = target_y - self.robot.y\n            distance = np.sqrt(dx*dx + dy*dy)\n\n            if distance > 0.5:  # Not close enough\n                self.robot.x += 0.1 * dx / distance\n                self.robot.y += 0.1 * dy / distance\n                self.path_history.append((self.robot.x, self.robot.y))\n            else:\n                self.task_log.append(f\"Reached {target_obj}\")\n                print(f\"VLA System: Reached {target_obj}\")\n                self.current_task = None  # Task completed\n\n        elif action == 'fetch_object':\n            target_obj = self.current_task['target']\n            if self.is_close_to_object(target_obj):\n                self.objects[target_obj]['picked_up'] = True\n                self.robot.holding_object = target_obj\n                self.task_log.append(f\"Fetched {target_obj}\")\n                print(f\"VLA System: Fetched {target_obj}\")\n                # Move to next part of task - go to destination\n                dest = self.current_task['destination']\n                self.current_task = {'action': 'navigate_to', 'target': dest}\n\n    def is_close_to_object(self, obj_name: str) -> bool:\n        \"\"\"Check if robot is close to an object\"\"\"\n        obj_x = self.objects[obj_name]['x']\n        obj_y = self.objects[obj_name]['y']\n        distance = np.sqrt((obj_x - self.robot.x)**2 + (obj_y - self.robot.y)**2)\n        return distance < 1.0\n\n    def animate(self, frame):\n        \"\"\"Animation function\"\"\"\n        # Process commands periodically\n        if frame == 10:\n            self.process_command(\"Go find the red cup in the kitchen\")\n        elif frame == 50:\n            self.process_command(\"Now place it on the table in the living room\")\n\n        # Execute current task\n        self.execute_task()\n\n        # Update robot position\n        self.robot_plot.set_data([self.robot.x], [self.robot.y])\n        x_vals, y_vals = zip(*self.path_history)\n        self.path_plot.set_data(x_vals, y_vals)\n\n        # Update object positions (mark picked up objects)\n        for obj_name, plot in self.objects_plots.items():\n            if self.objects[obj_name]['picked_up']:\n                # Move picked up object with robot\n                plot.set_data([self.robot.x + 0.3], [self.robot.y + 0.3])\n            else:\n                plot.set_data([self.objects[obj_name]['x']], [self.objects[obj_name]['y']])\n\n        # Add task log as text on plot\n        self.ax.text(0.02, 0.98, '\\n'.join(self.task_log[-3:]),\n                    transform=self.ax.transAxes, verticalalignment='top',\n                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n\n        # Add robot orientation indicator\n        arrow_length = 0.5\n        arrow_dx = arrow_length * np.cos(self.robot.theta)\n        arrow_dy = arrow_length * np.sin(self.robot.theta)\n        self.ax.arrow(self.robot.x, self.robot.y, arrow_dx, arrow_dy,\n                     head_width=0.1, head_length=0.1, fc='red', ec='red')\n\n        return [self.robot_plot, self.path_plot] + list(self.objects_plots.values())\n\n    def run_simulation(self):\n        \"\"\"Run the VLA system simulation\"\"\"\n        print(\"Starting VLA System Simulation...\")\n        print(\"The robot will process natural language commands and execute tasks\")\n        print(\"Green = Objects, Red = Robot, Blue = Path\")\n\n        ani = FuncAnimation(self.fig, self.animate, frames=200, interval=100, blit=False)\n        plt.show()\n\nif __name__ == \"__main__\":\n    sim = VLASimulation()\n    sim.run_simulation()\n"})}),"\n",(0,s.jsx)(n.h2,{id:"quick-recap",children:"Quick Recap"}),"\n",(0,s.jsx)(n.p,{children:"In this capstone lesson, we've created a complete autonomous humanoid robot system that integrates all the Vision-Language-Action (VLA) components:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Vision System"}),": Object detection and pose estimation using computer vision"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Language Understanding"}),": LLM-based cognitive planning and task decomposition"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Action Execution"}),": Motion control, navigation, and manipulation capabilities"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Integration"}),": A central node that orchestrates all components in a unified system"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"The system can understand natural language commands, plan complex multi-step tasks using LLMs, perceive its environment through vision, and execute sophisticated behaviors in simulation. This demonstrates the full potential of VLA integration for autonomous humanoid robots."}),"\n",(0,s.jsx)(n.p,{children:"The capstone project showcases how all the individual components from this chapter work together to create an intelligent, autonomous robotic system capable of complex interactions with its environment based on natural language instructions."})]})}function m(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453(e,n,t){t.d(n,{R:()=>i,x:()=>r});var o=t(6540);const s={},a=o.createContext(s);function i(e){const n=o.useContext(a);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:i(e.components),o.createElement(a.Provider,{value:n},e.children)}}}]);