"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[249],{316(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>o,default:()=>m,frontMatter:()=>s,metadata:()=>i,toc:()=>l});const i=JSON.parse('{"id":"physical-ai/the-ai-robot-brain-nvidia-isaac/synthetic-data-generation","title":"Synthetic Data Generation","description":"Overview","source":"@site/docs/physical-ai/the-ai-robot-brain-nvidia-isaac/synthetic-data-generation.md","sourceDirName":"physical-ai/the-ai-robot-brain-nvidia-isaac","slug":"/physical-ai/the-ai-robot-brain-nvidia-isaac/synthetic-data-generation","permalink":"/AI/docs/physical-ai/the-ai-robot-brain-nvidia-isaac/synthetic-data-generation","draft":false,"unlisted":false,"editUrl":"https://github.com/arijh/physical-ai-book/tree/main/packages/create-docusaurus/templates/shared/docs/physical-ai/the-ai-robot-brain-nvidia-isaac/synthetic-data-generation.md","tags":[],"version":"current","frontMatter":{"sidebar_label":"Synthetic Data Generation","title":"Synthetic Data Generation"},"sidebar":"docs","previous":{"title":"Introduction to NVIDIA Isaac Sim","permalink":"/AI/docs/physical-ai/the-ai-robot-brain-nvidia-isaac/introduction-to-nvidia-isaac-sim"},"next":{"title":"Isaac ROS for VSLAM","permalink":"/AI/docs/physical-ai/the-ai-robot-brain-nvidia-isaac/isaac-ros-vslam"}}');var a=t(4848),r=t(8453);const s={sidebar_label:"Synthetic Data Generation",title:"Synthetic Data Generation"},o="Synthetic Data Generation",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Hands-on Steps",id:"hands-on-steps",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Code Examples",id:"code-examples",level:2},{value:"Small Simulation",id:"small-simulation",level:2},{value:"Quick Recap",id:"quick-recap",level:2}];function _(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"synthetic-data-generation",children:"Synthetic Data Generation"})}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(n.p,{children:"Synthetic data generation is a revolutionary approach in AI development that addresses the critical challenge of data scarcity in robotics. NVIDIA Isaac Sim excels at creating diverse, labeled, and photorealistic synthetic datasets that can be used to train perception models, navigation systems, and other AI components. This lesson explores the principles, techniques, and best practices for generating high-quality synthetic data that effectively transfers to real-world applications."}),"\n",(0,a.jsx)(n.p,{children:"The ability to generate unlimited, perfectly labeled training data with precise ground truth information makes synthetic data generation a cornerstone of modern AI-robotics development, significantly reducing the time and cost associated with real-world data collection."}),"\n",(0,a.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Understand the principles and benefits of synthetic data generation"}),"\n",(0,a.jsx)(n.li,{children:"Configure Isaac Sim for diverse data generation scenarios"}),"\n",(0,a.jsx)(n.li,{children:"Generate various types of synthetic sensor data (RGB, depth, segmentation, LIDAR)"}),"\n",(0,a.jsx)(n.li,{children:"Apply domain randomization techniques to improve model generalization"}),"\n",(0,a.jsx)(n.li,{children:"Validate synthetic-to-real transfer performance"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"hands-on-steps",children:"Hands-on Steps"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Environment Configuration"}),": Set up Isaac Sim for data generation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Scene Randomization"}),": Create diverse scene variations"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Sensor Data Generation"}),": Generate multiple sensor modalities"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Domain Randomization"}),": Apply randomization techniques"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Data Validation"}),": Test synthetic-to-real transfer"]}),"\n"]}),"\n",(0,a.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Understanding of Isaac Sim basics (from previous lesson)"}),"\n",(0,a.jsx)(n.li,{children:"Knowledge of computer vision and machine learning concepts"}),"\n",(0,a.jsx)(n.li,{children:"Experience with sensor data formats"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,a.jsx)(n.p,{children:"Let's start by creating a comprehensive synthetic data generation pipeline:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# synthetic_data_pipeline.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan, PointCloud2, CameraInfo\nfrom geometry_msgs.msg import Pose, Point\nfrom std_msgs.msg import String, Header, Bool\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport math\nimport json\nimport os\nimport cv2\nfrom datetime import datetime\nimport random\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Tuple\n\n@dataclass\nclass SceneConfiguration:\n    \"\"\"Data class for scene configuration parameters\"\"\"\n    lighting_condition: str\n    weather: str\n    object_count: int\n    texture_randomization: bool\n    camera_position: Tuple[float, float, float]\n    camera_orientation: Tuple[float, float, float]\n\n@dataclass\nclass ObjectConfiguration:\n    \"\"\"Data class for individual object parameters\"\"\"\n    object_id: int\n    object_type: str  # 'box', 'cylinder', 'sphere', 'capsule'\n    position: Tuple[float, float, float]\n    orientation: Tuple[float, float, float, float]  # quaternion\n    size: Tuple[float, float, float]  # x, y, z for box; radius, height for others\n    color: Tuple[float, float, float]  # RGB normalized\n    class_id: int  # for segmentation\n    material_properties: Dict[str, float]  # roughness, metallic, etc.\n\nclass SyntheticDataGenerator(Node):\n    \"\"\"\n    Advanced synthetic data generator with domain randomization\n    \"\"\"\n    def __init__(self):\n        super().__init__('synthetic_data_generator')\n\n        # Publishers\n        self.rgb_pub = self.create_publisher(Image, '/isaac_synthetic/rgb', 10)\n        self.depth_pub = self.create_publisher(Image, '/isaac_synthetic/depth', 10)\n        self.seg_pub = self.create_publisher(Image, '/isaac_synthetic/segmentation', 10)\n        self.normal_pub = self.create_publisher(Image, '/isaac_synthetic/normal', 10)\n        self.lidar_pub = self.create_publisher(LaserScan, '/isaac_synthetic/lidar', 10)\n        self.camera_info_pub = self.create_publisher(CameraInfo, '/isaac_synthetic/camera_info', 10)\n        self.metadata_pub = self.create_publisher(String, '/isaac_synthetic/metadata', 10)\n\n        # Control subscribers\n        self.enable_sub = self.create_subscription(Bool, '/isaac_synthetic/enable', self.enable_callback, 10)\n        self.config_sub = self.create_subscription(String, '/isaac_synthetic/config', self.config_callback, 10)\n\n        # Timers\n        self.data_gen_timer = self.create_timer(0.2, self.generate_data_cycle)  # 5Hz\n\n        # Internal components\n        self.cv_bridge = CvBridge()\n        self.data_counter = 0\n        self.collection_enabled = True\n        self.collection_dir = \"/tmp/isaac_synthetic_data\"\n        self.current_config = None\n\n        # Create collection directory\n        os.makedirs(self.collection_dir, exist_ok=True)\n\n        # Define object classes for segmentation\n        self.object_classes = {\n            0: 'background',\n            1: 'robot',\n            2: 'obstacle',\n            3: 'furniture',\n            4: 'wall',\n            5: 'floor',\n            6: 'ceiling',\n            7: 'decoration'\n        }\n\n        # Domain randomization parameters\n        self.lighting_conditions = ['bright', 'dim', 'variable', 'backlight']\n        self.weather_conditions = ['clear', 'overcast', 'foggy', 'rainy_simulation']\n        self.texture_sets = ['indoor', 'outdoor', 'industrial', 'residential']\n\n        # Camera parameters\n        self.camera_params = {\n            'width': 640,\n            'height': 480,\n            'fov': 60.0,  # degrees\n            'near_clip': 0.1,\n            'far_clip': 100.0\n        }\n\n        # Initialize first configuration\n        self.current_config = self.generate_random_scene_config()\n\n        self.get_logger().info(\"Advanced Synthetic Data Generator initialized\")\n\n    def enable_callback(self, msg):\n        \"\"\"Enable/disable data collection\"\"\"\n        self.collection_enabled = msg.data\n        self.get_logger().info(f\"Data collection {'enabled' if self.collection_enabled else 'disabled'}\")\n\n    def config_callback(self, msg):\n        \"\"\"Receive configuration parameters\"\"\"\n        try:\n            config_dict = json.loads(msg.data)\n            # Apply configuration changes\n            if 'lighting' in config_dict:\n                self.current_config.lighting_condition = config_dict['lighting']\n            if 'object_count' in config_dict:\n                self.current_config.object_count = config_dict['object_count']\n            if 'weather' in config_dict:\n                self.current_config.weather = config_dict['weather']\n\n            self.get_logger().info(f\"Configuration updated: {config_dict}\")\n        except json.JSONDecodeError:\n            self.get_logger().error(\"Invalid configuration JSON received\")\n\n    def generate_random_scene_config(self) -> SceneConfiguration:\n        \"\"\"Generate a random scene configuration with domain randomization\"\"\"\n        return SceneConfiguration(\n            lighting_condition=random.choice(self.lighting_conditions),\n            weather=random.choice(self.weather_conditions),\n            object_count=random.randint(3, 15),\n            texture_randomization=random.choice([True, False]),\n            camera_position=(\n                random.uniform(-2, 2),\n                random.uniform(-2, 2),\n                random.uniform(1, 3)\n            ),\n            camera_orientation=(\n                random.uniform(-0.3, 0.3),  # pitch\n                random.uniform(-0.3, 0.3),  # yaw\n                random.uniform(-0.1, 0.1)   # roll\n            )\n        )\n\n    def generate_random_object(self, object_id: int) -> ObjectConfiguration:\n        \"\"\"Generate a random object with domain randomization\"\"\"\n        obj_type = random.choice(['box', 'cylinder', 'sphere', 'capsule'])\n        class_id = random.choice(list(self.object_classes.keys())[2:])  # Exclude background and robot\n\n        # Random position within scene bounds\n        position = (\n            random.uniform(-8, 8),\n            random.uniform(-8, 8),\n            random.uniform(0.1, 3.0)\n        )\n\n        # Random orientation (quaternion)\n        roll = random.uniform(-0.5, 0.5)\n        pitch = random.uniform(-0.5, 0.5)\n        yaw = random.uniform(-math.pi, math.pi)\n        orientation = self.euler_to_quaternion(roll, pitch, yaw)\n\n        # Random size based on object type\n        if obj_type == 'box':\n            size = (\n                random.uniform(0.2, 2.0),\n                random.uniform(0.2, 2.0),\n                random.uniform(0.2, 2.0)\n            )\n        elif obj_type in ['cylinder', 'capsule']:\n            size = (\n                random.uniform(0.1, 1.0),  # radius\n                random.uniform(0.3, 2.0),  # height\n                0.0  # unused for cylinder/capsule\n            )\n        else:  # sphere\n            size = (\n                random.uniform(0.1, 1.0),  # radius\n                0.0,  # unused\n                0.0   # unused\n            )\n\n        # Random color\n        color = (\n            random.uniform(0.1, 1.0),\n            random.uniform(0.1, 1.0),\n            random.uniform(0.1, 1.0)\n        )\n\n        # Material properties for realistic rendering\n        material_properties = {\n            'roughness': random.uniform(0.1, 0.9),\n            'metallic': random.uniform(0.0, 0.5),\n            'specular': random.uniform(0.1, 0.8)\n        }\n\n        return ObjectConfiguration(\n            object_id=object_id,\n            object_type=obj_type,\n            position=position,\n            orientation=orientation,\n            size=size,\n            color=color,\n            class_id=class_id,\n            material_properties=material_properties\n        )\n\n    def euler_to_quaternion(self, roll: float, pitch: float, yaw: float) -> Tuple[float, float, float, float]:\n        \"\"\"Convert Euler angles to quaternion\"\"\"\n        cy = math.cos(yaw * 0.5)\n        sy = math.sin(yaw * 0.5)\n        cp = math.cos(pitch * 0.5)\n        sp = math.sin(pitch * 0.5)\n        cr = math.cos(roll * 0.5)\n        sr = math.sin(roll * 0.5)\n\n        w = cr * cp * cy + sr * sp * sy\n        x = sr * cp * cy - cr * sp * sy\n        y = cr * sp * cy + sr * cp * sy\n        z = cr * cp * sy - sr * sp * cy\n\n        return (x, y, z, w)\n\n    def generate_scene_data(self) -> Tuple[List[ObjectConfiguration], SceneConfiguration]:\n        \"\"\"Generate complete scene with objects and configuration\"\"\"\n        # Generate new scene configuration\n        scene_config = self.generate_random_scene_config()\n\n        # Generate objects\n        objects = []\n        for i in range(scene_config.object_count):\n            obj = self.generate_random_object(i)\n            objects.append(obj)\n\n        return objects, scene_config\n\n    def generate_synthetic_rgb(self, objects: List[ObjectConfiguration], scene_config: SceneConfiguration) -> np.ndarray:\n        \"\"\"Generate synthetic RGB image with domain randomization\"\"\"\n        height = self.camera_params['height']\n        width = self.camera_params['width']\n        image = np.zeros((height, width, 3), dtype=np.float32)\n\n        # Apply lighting condition\n        lighting_factor = {\n            'bright': 1.0,\n            'dim': 0.3,\n            'variable': random.uniform(0.4, 1.0),\n            'backlight': 0.7\n        }[scene_config.lighting_condition]\n\n        # Create background based on weather\n        if scene_config.weather == 'clear':\n            # Sky gradient\n            for y in range(height):\n                sky_color = [\n                    0.5 + 0.3 * y / height,  # Blue increases toward top\n                    0.7 + 0.2 * y / height,  # Green increases toward top\n                    1.0  # Sky blue\n                ]\n                image[y, :, :] = sky_color\n        elif scene_config.weather == 'overcast':\n            # Gray overcast\n            image[:, :, :] = [0.7, 0.7, 0.8]\n        elif scene_config.weather == 'foggy':\n            # Foggy with low contrast\n            base_color = [0.8, 0.8, 0.8]\n            for y in range(height):\n                fog_factor = 1.0 - 0.3 * y / height  # Less fog at top\n                image[y, :, :] = [c * fog_factor for c in base_color]\n\n        # Add ground plane\n        ground_level = int(0.6 * height)  # Ground starts at 60% height\n        for y in range(ground_level, height):\n            # Ground color with texture variation\n            ground_color = [\n                0.4 + random.uniform(-0.1, 0.1),  # Brownish\n                0.3 + random.uniform(-0.1, 0.1),\n                0.2 + random.uniform(-0.1, 0.1)\n            ]\n            image[y, :, :] = ground_color\n\n        # Add objects to scene\n        for obj in objects:\n            # Project 3D object to 2D image\n            # Simplified projection for demonstration\n            # In real Isaac Sim, this would use proper 3D rendering\n            obj_x, obj_y, obj_z = obj.position\n            screen_x = int(width / 2 + obj_x * 40)  # Scale factor for visibility\n            screen_y = int(height / 2 - obj_y * 40)  # Invert Y\n\n            if 0 <= screen_x < width and 0 <= screen_y < height:\n                # Determine object size in pixels based on distance\n                distance_factor = max(0.1, 5.0 / max(obj_z, 0.5))\n                size_pixels = max(1, int(obj.size[0] * 30 * distance_factor))\n\n                # Draw object based on type\n                color = [int(c * 255 * lighting_factor) for c in obj.color]\n                if obj.object_type == 'box':\n                    # Draw rectangle\n                    x1, x2 = max(0, screen_x - size_pixels), min(width, screen_x + size_pixels)\n                    y1, y2 = max(0, screen_y - size_pixels), min(height, screen_y + size_pixels)\n                    image[y1:y2, x1:x2] = [c / 255.0 for c in color]  # Normalize to 0-1\n                elif obj.object_type in ['cylinder', 'sphere', 'capsule']:\n                    # Draw circle\n                    cv2.circle(image, (screen_x, screen_y), size_pixels,\n                              [c / 255.0 for c in color], -1)\n\n        # Apply weather effects\n        if scene_config.weather == 'rainy_simulation':\n            # Add rain streaks (simulated)\n            for _ in range(50):\n                x = random.randint(0, width-1)\n                y = random.randint(0, height-10)\n                length = random.randint(5, 15)\n                cv2.line(image, (x, y), (x, min(y+length, height-1)), (0.7, 0.7, 1.0), 1)\n\n        # Add noise to make more realistic\n        noise = np.random.normal(0, 0.02, image.shape).astype(np.float32)\n        image = np.clip(image + noise, 0, 1)\n\n        return (image * 255).astype(np.uint8)\n\n    def generate_synthetic_depth(self, objects: List[ObjectConfiguration]) -> np.ndarray:\n        \"\"\"Generate synthetic depth image\"\"\"\n        height = self.camera_params['height']\n        width = self.camera_params['width']\n        depth = np.ones((height, width), dtype=np.float32) * self.camera_params['far_clip']\n\n        # Calculate depth for each object\n        for obj in objects:\n            obj_x, obj_y, obj_z = obj.position\n            screen_x = int(width / 2 + obj_x * 40)\n            screen_y = int(height / 2 - obj_y * 40)\n\n            if 0 <= screen_x < width and 0 <= screen_y < height:\n                # Object distance from camera\n                distance = math.sqrt(obj_x**2 + obj_y**2 + obj_z**2)\n\n                # Determine object size in pixels\n                distance_factor = max(0.1, 5.0 / max(obj_z, 0.5))\n                size_pixels = max(1, int(obj.size[0] * 30 * distance_factor))\n\n                # Fill object area with its distance\n                if obj.object_type == 'box':\n                    x1, x2 = max(0, screen_x - size_pixels), min(width, screen_x + size_pixels)\n                    y1, y2 = max(0, screen_y - size_pixels), min(height, screen_y + size_pixels)\n                    depth[y1:y2, x1:x2] = min(distance, self.camera_params['far_clip'])\n                elif obj.object_type in ['cylinder', 'sphere', 'capsule']:\n                    cv2.circle(depth, (screen_x, screen_y), size_pixels, distance, -1)\n\n        # Add realistic depth noise\n        noise = np.random.normal(0, 0.01, depth.shape).astype(np.float32)\n        depth = np.maximum(self.camera_params['near_clip'], depth + noise)\n\n        return depth\n\n    def generate_synthetic_segmentation(self, objects: List[ObjectConfiguration], width: int, height: int) -> np.ndarray:\n        \"\"\"Generate synthetic segmentation mask\"\"\"\n        segmentation = np.zeros((height, width), dtype=np.uint8)\n\n        # Set background class\n        segmentation[:, :] = 0  # background class\n\n        # Add objects with their class IDs\n        for obj in objects:\n            obj_x, obj_y, obj_z = obj.position\n            screen_x = int(width / 2 + obj_x * 40)\n            screen_y = int(height / 2 - obj_y * 40)\n\n            if 0 <= screen_x < width and 0 <= screen_y < height:\n                distance_factor = max(0.1, 5.0 / max(obj_z, 0.5))\n                size_pixels = max(1, int(obj.size[0] * 30 * distance_factor))\n\n                if obj.object_type == 'box':\n                    x1, x2 = max(0, screen_x - size_pixels), min(width, screen_x + size_pixels)\n                    y1, y2 = max(0, screen_y - size_pixels), min(height, screen_y + size_pixels)\n                    segmentation[y1:y2, x1:x2] = obj.class_id\n                elif obj.object_type in ['cylinder', 'sphere', 'capsule']:\n                    cv2.circle(segmentation, (screen_x, screen_y), size_pixels, obj.class_id, -1)\n\n        # Convert to 3-channel for ROS compatibility\n        seg_image = np.stack([segmentation, segmentation, segmentation], axis=2)\n        return seg_image.astype(np.uint8)\n\n    def generate_synthetic_normals(self, objects: List[ObjectConfiguration], width: int, height: int) -> np.ndarray:\n        \"\"\"Generate synthetic surface normal map\"\"\"\n        normals = np.zeros((height, width, 3), dtype=np.float32)\n\n        # For simplicity, assign face normals to objects\n        for obj in objects:\n            obj_x, obj_y, obj_z = obj.position\n            screen_x = int(width / 2 + obj_x * 40)\n            screen_y = int(height / 2 - obj_y * 40)\n\n            if 0 <= screen_x < width and 0 <= screen_y < height:\n                distance_factor = max(0.1, 5.0 / max(obj_z, 0.5))\n                size_pixels = max(1, int(obj.size[0] * 30 * distance_factor))\n\n                # Assign normal based on object type\n                if obj.object_type == 'box':\n                    normal = [0, 0, 1]  # Front face normal\n                    x1, x2 = max(0, screen_x - size_pixels), min(width, screen_x + size_pixels)\n                    y1, y2 = max(0, screen_y - size_pixels), min(height, screen_y + size_pixels)\n                    normals[y1:y2, x1:x2] = normal\n                elif obj.object_type in ['cylinder', 'sphere']:\n                    # Radial normals for curved surfaces\n                    for dy in range(-size_pixels, size_pixels):\n                        for dx in range(-size_pixels, size_pixels):\n                            if dx*dx + dy*dy <= size_pixels*size_pixels:\n                                py, px = screen_y + dy, screen_x + dx\n                                if 0 <= py < height and 0 <= px < width:\n                                    # Normal points outward from center\n                                    vec_x = px - screen_x\n                                    vec_y = py - screen_y\n                                    vec_z = 0\n                                    length = max(0.1, math.sqrt(vec_x*vec_x + vec_y*vec_y + vec_z*vec_z))\n                                    normals[py, px] = [vec_x/length, vec_y/length, vec_z/length]\n\n        # Normalize to 0-255 range\n        normals = ((normals + 1) * 127.5).astype(np.uint8)\n        return normals\n\n    def generate_synthetic_lidar(self, objects: List[ObjectConfiguration]) -> Tuple[List[float], float, float]:\n        \"\"\"Generate synthetic LIDAR scan\"\"\"\n        num_points = 1080\n        angle_min = -math.pi * 0.75  # -135 degrees\n        angle_max = math.pi * 0.75   # 135 degrees\n        angle_increment = (angle_max - angle_min) / num_points\n\n        ranges = []\n\n        for i in range(num_points):\n            angle = angle_min + i * angle_increment\n            ray_direction = (math.cos(angle), math.sin(angle))\n\n            min_distance = self.camera_params['far_clip']\n\n            # Check for intersections with all objects\n            for obj in objects:\n                if obj.object_type == 'box':\n                    distance = self.ray_box_intersection(\n                        (0, 0), ray_direction,  # Robot at origin\n                        obj.position[0], obj.position[1],\n                        obj.size[0]/2, obj.size[1]/2\n                    )\n                elif obj.object_type in ['cylinder', 'sphere']:\n                    distance = self.ray_cylinder_intersection(\n                        (0, 0), ray_direction,\n                        obj.position[0], obj.position[1],\n                        obj.size[0]  # radius\n                    )\n                else:\n                    continue\n\n                if distance and distance < min_distance:\n                    min_distance = distance\n\n            # Add realistic LIDAR noise\n            noise = np.random.normal(0, 0.01)\n            final_range = max(self.camera_params['near_clip'],\n                             min(self.camera_params['far_clip'], min_distance + noise))\n            ranges.append(final_range)\n\n        return ranges, angle_min, angle_increment\n\n    def ray_box_intersection(self, ray_origin, ray_dir, box_x, box_y, half_width, half_height):\n        \"\"\"Calculate intersection of ray with axis-aligned box\"\"\"\n        t1 = (box_x - half_width - ray_origin[0]) / ray_dir[0] if ray_dir[0] != 0 else float('inf')\n        t2 = (box_x + half_width - ray_origin[0]) / ray_dir[0] if ray_dir[0] != 0 else float('inf')\n        t3 = (box_y - half_height - ray_origin[1]) / ray_dir[1] if ray_dir[1] != 0 else float('inf')\n        t4 = (box_y + half_height - ray_origin[1]) / ray_dir[1] if ray_dir[1] != 0 else float('inf')\n\n        t_min = max(min(t1, t2), min(t3, t4))\n        t_max = min(max(t1, t2), max(t3, t4))\n\n        if t_max >= 0 and t_min <= t_max:\n            return t_min if t_min >= 0 else t_max\n\n        return None\n\n    def ray_cylinder_intersection(self, ray_origin, ray_dir, cyl_x, cyl_y, radius):\n        \"\"\"Calculate intersection of ray with cylinder\"\"\"\n        rel_x = ray_origin[0] - cyl_x\n        rel_y = ray_origin[1] - cyl_y\n\n        a = ray_dir[0]**2 + ray_dir[1]**2\n        b = 2 * (rel_x * ray_dir[0] + rel_y * ray_dir[1])\n        c = rel_x**2 + rel_y**2 - radius**2\n\n        discriminant = b**2 - 4*a*c\n\n        if discriminant < 0:\n            return None\n\n        sqrt_disc = math.sqrt(discriminant)\n        t1 = (-b - sqrt_disc) / (2*a)\n        t2 = (-b + sqrt_disc) / (2*a)\n\n        if t1 > 0:\n            return t1\n        elif t2 > 0:\n            return t2\n        else:\n            return None\n\n    def save_synthetic_data(self, rgb_image, depth_image, seg_image, normal_image, lidar_ranges, metadata):\n        \"\"\"Save synthetic data to disk with metadata\"\"\"\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n\n        # Create data directory for this batch\n        batch_dir = f\"{self.collection_dir}/batch_{self.data_counter:06d}\"\n        os.makedirs(batch_dir, exist_ok=True)\n\n        # Save images\n        cv2.imwrite(f\"{batch_dir}/rgb_{timestamp}.png\", cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR))\n        cv2.imwrite(f\"{batch_dir}/depth_{timestamp}.png\", (depth_image * 256).astype(np.uint16))\n        cv2.imwrite(f\"{batch_dir}/seg_{timestamp}.png\", seg_image)\n        cv2.imwrite(f\"{batch_dir}/normal_{timestamp}.png\", normal_image)\n\n        # Save LIDAR data\n        np.save(f\"{batch_dir}/lidar_{timestamp}.npy\", np.array(lidar_ranges))\n\n        # Save metadata\n        metadata['timestamp'] = timestamp\n        metadata['batch_id'] = self.data_counter\n        with open(f\"{batch_dir}/metadata.json\", 'w') as f:\n            json.dump(metadata, f, indent=2)\n\n        # Save individual object annotations\n        obj_annotations = []\n        for obj in metadata['objects']:\n            obj_annotations.append({\n                'id': obj.object_id,\n                'type': obj.object_type,\n                'class': self.object_classes.get(obj.class_id, 'unknown'),\n                'position': obj.position,\n                'size': obj.size,\n                'color': obj.color\n            })\n\n        with open(f\"{batch_dir}/object_annotations.json\", 'w') as f:\n            json.dump(obj_annotations, f, indent=2)\n\n    def generate_data_cycle(self):\n        \"\"\"Main data generation cycle\"\"\"\n        if not self.collection_enabled:\n            return\n\n        current_time = self.get_clock().now()\n\n        # Generate new scene\n        objects, scene_config = self.generate_scene_data()\n\n        # Generate all sensor data\n        rgb_image = self.generate_synthetic_rgb(objects, scene_config)\n        depth_image = self.generate_synthetic_depth(objects)\n        seg_image = self.generate_synthetic_segmentation(objects, self.camera_params['width'], self.camera_params['height'])\n        normal_image = self.generate_synthetic_normals(objects, self.camera_params['width'], self.camera_params['height'])\n        lidar_ranges, angle_min, angle_increment = self.generate_synthetic_lidar(objects)\n\n        # Create and publish ROS messages\n        rgb_msg = self.cv_bridge.cv2_to_imgmsg(rgb_image, encoding=\"rgb8\")\n        rgb_msg.header.stamp = current_time.to_msg()\n        rgb_msg.header.frame_id = 'camera_rgb_frame'\n        self.rgb_pub.publish(rgb_msg)\n\n        depth_msg = self.cv_bridge.cv2_to_imgmsg(depth_image, encoding=\"32FC1\")\n        depth_msg.header.stamp = current_time.to_msg()\n        depth_msg.header.frame_id = 'camera_depth_frame'\n        self.depth_pub.publish(depth_msg)\n\n        seg_msg = self.cv_bridge.cv2_to_imgmsg(seg_image, encoding=\"rgb8\")\n        seg_msg.header.stamp = current_time.to_msg()\n        seg_msg.header.frame_id = 'camera_seg_frame'\n        self.seg_pub.publish(seg_msg)\n\n        normal_msg = self.cv_bridge.cv2_to_imgmsg(normal_image, encoding=\"rgb8\")\n        normal_msg.header.stamp = current_time.to_msg()\n        normal_msg.header.frame_id = 'camera_normal_frame'\n        self.normal_pub.publish(normal_msg)\n\n        scan_msg = LaserScan()\n        scan_msg.header.stamp = current_time.to_msg()\n        scan_msg.header.frame_id = 'laser_frame'\n        scan_msg.angle_min = angle_min\n        scan_msg.angle_max = -angle_min\n        scan_msg.angle_increment = angle_increment\n        scan_msg.time_increment = 0.0\n        scan_msg.scan_time = 0.1\n        scan_msg.range_min = 0.1\n        scan_msg.range_max = 25.0\n        scan_msg.ranges = lidar_ranges\n        self.lidar_pub.publish(scan_msg)\n\n        # Create camera info message\n        camera_info_msg = CameraInfo()\n        camera_info_msg.header.stamp = current_time.to_msg()\n        camera_info_msg.header.frame_id = 'camera_rgb_frame'\n        camera_info_msg.width = self.camera_params['width']\n        camera_info_msg.height = self.camera_params['height']\n        camera_info_msg.distortion_model = 'plumb_bob'\n        # Simple pinhole camera model\n        fx = self.camera_params['width'] / (2 * math.tan(math.radians(self.camera_params['fov']/2)))\n        fy = fx\n        cx = self.camera_params['width'] / 2\n        cy = self.camera_params['height'] / 2\n        camera_info_msg.k = [fx, 0.0, cx, 0.0, fy, cy, 0.0, 0.0, 1.0]\n        self.camera_info_pub.publish(camera_info_msg)\n\n        # Create metadata and publish\n        metadata = {\n            'scene_config': {\n                'lighting_condition': scene_config.lighting_condition,\n                'weather': scene_config.weather,\n                'object_count': scene_config.object_count,\n                'texture_randomization': scene_config.texture_randomization\n            },\n            'objects': [obj for obj in objects],\n            'sensor_data_types': ['rgb', 'depth', 'segmentation', 'normals', 'lidar'],\n            'domain_randomization_applied': True\n        }\n\n        metadata_msg = String()\n        metadata_msg.data = json.dumps(metadata, default=lambda x: x.__dict__ if hasattr(x, '__dict__') else str(x))\n        self.metadata_pub.publish(metadata_msg)\n\n        # Save to disk\n        self.save_synthetic_data(rgb_image, depth_image, seg_image, normal_image, lidar_ranges, metadata)\n\n        # Publish status\n        status_msg = String()\n        status_msg.data = f\"Batch {self.data_counter}: {len(objects)} objects, \" \\\n                         f\"Lighting={scene_config.lighting_condition}, Weather={scene_config.weather}\"\n        self.get_logger().info(status_msg.data)\n\n        self.data_counter += 1\n\ndef main(args=None):\n    rclpy.init(args=args)\n    generator = SyntheticDataGenerator()\n\n    try:\n        rclpy.spin(generator)\n    except KeyboardInterrupt:\n        generator.get_logger().info(\"Synthetic Data Generator stopped by user\")\n    finally:\n        generator.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.p,{children:"Now let's create a specialized data validation tool for synthetic-to-real transfer:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# synthetic_data_validator.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan\nfrom std_msgs.msg import String, Float32\nimport numpy as np\nimport cv2\nfrom cv_bridge import CvBridge\nimport json\nfrom sklearn.metrics import mean_squared_error\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n\nclass SyntheticDataValidator(Node):\n    """\n    Validate synthetic data quality and synthetic-to-real transfer potential\n    """\n    def __init__(self):\n        super().__init__(\'synthetic_data_validator\')\n\n        # Publishers\n        self.quality_score_pub = self.create_publisher(Float32, \'/synthetic_validation/quality_score\', 10)\n        self.transfer_score_pub = self.create_publisher(Float32, \'/synthetic_validation/transfer_score\', 10)\n        self.status_pub = self.create_publisher(String, \'/synthetic_validation/status\', 10)\n\n        # Subscribers\n        self.rgb_sub = self.create_subscription(Image, \'/isaac_synthetic/rgb\', self.rgb_callback, 10)\n        self.real_rgb_sub = self.create_subscription(Image, \'/real_camera/rgb\', self.real_rgb_callback, 10)\n        self.depth_sub = self.create_subscription(Image, \'/isaac_synthetic/depth\', self.depth_callback, 10)\n\n        # Timers\n        self.validation_timer = self.create_timer(1.0, self.run_validation)\n\n        # Internal components\n        self.cv_bridge = CvBridge()\n        self.synthetic_rgb_buffer = []\n        self.real_rgb_buffer = []\n        self.synthetic_depth_buffer = []\n\n        # Validation parameters\n        self.buffer_size = 10\n        self.validation_metrics = {\n            \'sharpness\': 0.0,\n            \'color_distribution\': 0.0,\n            \'texture_complexity\': 0.0,\n            \'dynamic_range\': 0.0\n        }\n\n        self.get_logger().info("Synthetic Data Validator initialized")\n\n    def rgb_callback(self, msg):\n        """Process synthetic RGB images"""\n        try:\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, "rgb8")\n            self.synthetic_rgb_buffer.append(cv_image.copy())\n\n            if len(self.synthetic_rgb_buffer) > self.buffer_size:\n                self.synthetic_rgb_buffer.pop(0)\n        except Exception as e:\n            self.get_logger().error(f"Error processing synthetic RGB: {e}")\n\n    def real_rgb_callback(self, msg):\n        """Process real RGB images for comparison"""\n        try:\n            cv_image = self.cv_bridge.imgmsg_to_cv2(msg, "rgb8")\n            self.real_rgb_buffer.append(cv_image.copy())\n\n            if len(self.real_rgb_buffer) > self.buffer_size:\n                self.real_rgb_buffer.pop(0)\n        except Exception as e:\n            self.get_logger().error(f"Error processing real RGB: {e}")\n\n    def depth_callback(self, msg):\n        """Process synthetic depth images"""\n        try:\n            cv_depth = self.cv_bridge.imgmsg_to_cv2(msg, "32FC1")\n            self.synthetic_depth_buffer.append(cv_depth.copy())\n\n            if len(self.synthetic_depth_buffer) > self.buffer_size:\n                self.synthetic_depth_buffer.pop(0)\n        except Exception as e:\n            self.get_logger().error(f"Error processing synthetic depth: {e}")\n\n    def calculate_sharpness(self, image):\n        """Calculate image sharpness using Laplacian variance"""\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n        return laplacian_var\n\n    def calculate_color_distribution(self, image):\n        """Calculate color distribution statistics"""\n        if len(image.shape) == 3:\n            # Calculate histogram for each channel\n            hist_r = cv2.calcHist([image], [0], None, [256], [0, 256])\n            hist_g = cv2.calcHist([image], [1], None, [256], [0, 256])\n            hist_b = cv2.calcHist([image], [2], None, [256], [0, 256])\n\n            # Normalize histograms\n            hist_r = hist_r / hist_r.sum()\n            hist_g = hist_g / hist_g.sum()\n            hist_b = hist_b / hist_b.sum()\n\n            return {\n                \'mean\': [hist_r.mean(), hist_g.mean(), hist_b.mean()],\n                \'std\': [hist_r.std(), hist_g.std(), hist_b.std()],\n                \'entropy\': [\n                    -np.sum(hist_r * np.log2(hist_r + 1e-10)),\n                    -np.sum(hist_g * np.log2(hist_g + 1e-10)),\n                    -np.sum(hist_b * np.log2(hist_b + 1e-10))\n                ]\n            }\n        return {\'mean\': [0], \'std\': [0], \'entropy\': [0]}\n\n    def calculate_texture_complexity(self, image):\n        """Calculate texture complexity using Local Binary Patterns"""\n        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY) if len(image.shape) == 3 else image\n\n        # Simple texture measure using gradient magnitude\n        grad_x = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n        grad_y = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n        gradient_magnitude = np.sqrt(grad_x**2 + grad_y**2)\n\n        return gradient_magnitude.mean()\n\n    def calculate_dynamic_range(self, image):\n        """Calculate image dynamic range"""\n        if len(image.shape) == 3:\n            # Calculate for each channel\n            ranges = []\n            for i in range(3):\n                channel = image[:, :, i]\n                ranges.append(float(channel.max() - channel.min()))\n            return sum(ranges) / len(ranges)\n        else:\n            return float(image.max() - image.min())\n\n    def compare_synthetic_real(self):\n        """Compare synthetic and real image characteristics"""\n        if not self.synthetic_rgb_buffer or not self.real_rgb_buffer:\n            return 0.0\n\n        # Get latest images\n        synth_img = self.synthetic_rgb_buffer[-1]\n        real_img = self.real_rgb_buffer[-1]\n\n        # Calculate metrics for both\n        synth_metrics = {\n            \'sharpness\': self.calculate_sharpness(synth_img),\n            \'color_dist\': self.calculate_color_distribution(synth_img),\n            \'texture\': self.calculate_texture_complexity(synth_img),\n            \'dynamic_range\': self.calculate_dynamic_range(synth_img)\n        }\n\n        real_metrics = {\n            \'sharpness\': self.calculate_sharpness(real_img),\n            \'color_dist\': self.calculate_color_distribution(real_img),\n            \'texture\': self.calculate_texture_complexity(real_img),\n            \'dynamic_range\': self.calculate_dynamic_range(real_img)\n        }\n\n        # Calculate similarity scores (0-1, where 1 is most similar)\n        sharpness_sim = 1 / (1 + abs(synth_metrics[\'sharpness\'] - real_metrics[\'sharpness\']))\n        texture_sim = 1 / (1 + abs(synth_metrics[\'texture\'] - real_metrics[\'texture\']))\n\n        # Color distribution similarity (using Bhattacharyya distance for histograms)\n        color_sim = 0\n        for i in range(3):  # RGB channels\n            hist_synth = cv2.calcHist([synth_img], [i], None, [32], [0, 256])\n            hist_real = cv2.calcHist([real_img], [i], None, [32], [0, 256])\n\n            # Normalize\n            hist_synth = hist_synth / hist_synth.sum()\n            hist_real = hist_real / hist_real.sum()\n\n            # Bhattacharyya distance\n            bc = cv2.compareHist(hist_synth, hist_real, cv2.HISTCMP_BHATTACHARYYA)\n            color_sim += max(0, 1 - bc)  # Convert to similarity\n\n        color_sim = color_sim / 3  # Average across channels\n\n        # Combine metrics\n        combined_similarity = (sharpness_sim * 0.3 +\n                              texture_sim * 0.3 +\n                              color_sim * 0.4)\n\n        return min(1.0, max(0.0, combined_similarity))\n\n    def assess_synthetic_quality(self):\n        """Assess overall quality of synthetic data"""\n        if not self.synthetic_rgb_buffer:\n            return 0.0\n\n        latest_img = self.synthetic_rgb_buffer[-1]\n\n        # Calculate various quality metrics\n        sharpness = self.calculate_sharpness(latest_img)\n        texture_complexity = self.calculate_texture_complexity(latest_img)\n        color_stats = self.calculate_color_distribution(latest_img)\n\n        # Normalize metrics to 0-1 scale\n        sharpness_score = min(1.0, sharpness / 1000)  # Adjust normalization as needed\n        texture_score = min(1.0, texture_complexity / 50)  # Adjust normalization as needed\n        color_entropy_score = min(1.0, sum(color_stats[\'entropy\']) / 15)  # Adjust normalization as needed\n\n        # Combined quality score\n        quality_score = (sharpness_score * 0.4 +\n                        texture_score * 0.3 +\n                        color_entropy_score * 0.3)\n\n        return min(1.0, max(0.0, quality_score))\n\n    def run_validation(self):\n        """Run comprehensive validation"""\n        if not self.synthetic_rgb_buffer:\n            return\n\n        # Assess synthetic data quality\n        quality_score = self.assess_synthetic_quality()\n\n        # Assess synthetic-to-real transfer potential\n        transfer_score = self.compare_synthetic_real()\n\n        # Publish scores\n        quality_msg = Float32()\n        quality_msg.data = quality_score\n        self.quality_score_pub.publish(quality_msg)\n\n        transfer_msg = Float32()\n        transfer_msg.data = transfer_score\n        self.transfer_score_pub.publish(transfer_msg)\n\n        # Publish status\n        status_msg = String()\n        status_msg.data = f"Quality: {quality_score:.3f}, Transfer: {transfer_score:.3f}, " \\\n                         f"Buffer: {len(self.synthetic_rgb_buffer)}/{len(self.real_rgb_buffer)}"\n        self.status_pub.publish(status_msg)\n\n        # Log validation results\n        self.get_logger().info(f"Validation - Quality: {quality_score:.3f}, Transfer: {transfer_score:.3f}")\n\n        # Update metrics for reporting\n        latest_img = self.synthetic_rgb_buffer[-1]\n        self.validation_metrics[\'sharpness\'] = self.calculate_sharpness(latest_img)\n        self.validation_metrics[\'texture_complexity\'] = self.calculate_texture_complexity(latest_img)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    validator = SyntheticDataValidator()\n\n    try:\n        rclpy.spin(validator)\n    except KeyboardInterrupt:\n        validator.get_logger().info("Synthetic Data Validator stopped by user")\n    finally:\n        validator.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"small-simulation",children:"Small Simulation"}),"\n",(0,a.jsx)(n.p,{children:"Let's create a domain randomization experiment that demonstrates how varying environmental parameters affects synthetic data quality:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# domain_randomization_experiment.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan\nfrom std_msgs.msg import String, Float32MultiArray\nimport numpy as np\nimport cv2\nfrom cv_bridge import CvBridge\nimport json\nfrom dataclasses import dataclass\nfrom typing import List, Dict\nimport random\n\n@dataclass\nclass DomainConfig:\n    \"\"\"Configuration for domain randomization experiment\"\"\"\n    lighting_intensity: float\n    fog_density: float\n    texture_complexity: float\n    object_count: int\n    camera_noise: float\n    weather_type: str\n\nclass DomainRandomizationExperiment(Node):\n    \"\"\"\n    Experiment with different domain randomization parameters\n    to optimize synthetic-to-real transfer\n    \"\"\"\n    def __init__(self):\n        super().__init__('domain_randomization_experiment')\n\n        # Publishers\n        self.config_pub = self.create_publisher(String, '/domain_randomization/config', 10)\n        self.metrics_pub = self.create_publisher(Float32MultiArray, '/domain_randomization/metrics', 10)\n        self.status_pub = self.create_publisher(String, '/domain_randomization/status', 10)\n\n        # Timers\n        self.experiment_timer = self.create_timer(2.0, self.run_experiment_cycle)\n\n        # Internal components\n        self.cv_bridge = CvBridge()\n        self.experiment_counter = 0\n        self.experiment_configs = []\n        self.results_history = []\n\n        # Define experiment parameters\n        self.lighting_range = (0.3, 1.5)  # Factor applied to base lighting\n        self.fog_range = (0.0, 0.3)       # Fog density\n        self.texture_range = (0.1, 1.0)   # Texture complexity factor\n        self.object_count_range = (3, 12) # Number of objects in scene\n        self.camera_noise_range = (0.0, 0.05)  # Noise level\n        self.weather_types = ['clear', 'foggy', 'rainy', 'snowy', 'overcast']\n\n        # Initialize with random configurations\n        self.generate_initial_configs(20)\n\n        self.get_logger().info(\"Domain Randomization Experiment initialized\")\n\n    def generate_initial_configs(self, num_configs: int):\n        \"\"\"Generate initial random configurations for experiment\"\"\"\n        for _ in range(num_configs):\n            config = DomainConfig(\n                lighting_intensity=random.uniform(*self.lighting_range),\n                fog_density=random.uniform(*self.fog_range),\n                texture_complexity=random.uniform(*self.texture_range),\n                object_count=random.randint(*self.object_count_range),\n                camera_noise=random.uniform(*self.camera_noise_range),\n                weather_type=random.choice(self.weather_types)\n            )\n            self.experiment_configs.append(config)\n\n    def generate_synthetic_scene(self, config: DomainConfig) -> Dict:\n        \"\"\"Generate synthetic scene based on configuration\"\"\"\n        # This would interface with Isaac Sim in a real implementation\n        # For this example, we'll simulate the effects of different parameters\n\n        # Calculate how each parameter affects the resulting image quality\n        # These are simplified models of how domain parameters affect data quality\n\n        # Lighting affects visibility and contrast\n        lighting_quality = min(1.0, config.lighting_intensity * 0.8)\n\n        # Fog reduces visibility and contrast\n        fog_quality = max(0.1, 1.0 - config.fog_density * 2)\n\n        # Texture complexity affects feature richness\n        texture_quality = config.texture_complexity\n\n        # Object count affects training diversity\n        object_diversity = min(1.0, config.object_count / 15)\n\n        # Camera noise affects data precision\n        noise_impact = max(0.5, 1.0 - config.camera_noise * 20)\n\n        # Weather affects overall scene quality\n        weather_impact = {\n            'clear': 1.0,\n            'foggy': 0.7,\n            'rainy': 0.6,\n            'snowy': 0.6,\n            'overcast': 0.8\n        }[config.weather_type]\n\n        # Calculate combined quality score\n        combined_quality = (lighting_quality * 0.2 +\n                           fog_quality * 0.2 +\n                           texture_quality * 0.2 +\n                           object_diversity * 0.15 +\n                           noise_impact * 0.15 +\n                           weather_impact * 0.1)\n\n        # Calculate transferability score (how well this config might transfer to real)\n        # Based on how close parameters are to real-world conditions\n        real_world_similarity = (\n            (1 - abs(config.lighting_intensity - 1.0) * 0.3) * 0.25 +  # Close to 1.0 is more realistic\n            (1 - config.fog_density * 0.5) * 0.25 +  # Less fog is more common\n            min(1.0, config.texture_complexity * 0.8) * 0.2 +  # Rich textures are good\n            min(1.0, config.object_count / 10) * 0.15 +  # Moderate object count\n            max(0.5, 1 - config.camera_noise * 10) * 0.15 +  # Less noise is better\n            0.05  # Base score\n        )\n\n        # Calculate diversity score (how different this config is from others)\n        diversity_score = self.calculate_diversity_score(config)\n\n        return {\n            'config': config,\n            'quality_score': combined_quality,\n            'transferability_score': real_world_similarity,\n            'diversity_score': diversity_score,\n            'overall_score': (combined_quality * 0.5 + real_world_similarity * 0.3 + diversity_score * 0.2)\n        }\n\n    def calculate_diversity_score(self, config: DomainConfig) -> float:\n        \"\"\"Calculate how diverse this configuration is compared to others\"\"\"\n        if not self.experiment_configs:\n            return 1.0\n\n        # Calculate distance to other configs\n        distances = []\n        for other_config in self.experiment_configs:\n            if other_config != config:\n                # Calculate euclidean distance in parameter space\n                dist = (\n                    (config.lighting_intensity - other_config.lighting_intensity) ** 2 +\n                    (config.fog_density - other_config.fog_density) ** 2 +\n                    (config.texture_complexity - other_config.texture_complexity) ** 2 +\n                    (config.object_count - other_config.object_count) ** 2 +\n                    (config.camera_noise - other_config.camera_noise) ** 2\n                ) ** 0.5\n                distances.append(dist)\n\n        # Average distance to other configs (higher = more diverse)\n        if distances:\n            avg_distance = sum(distances) / len(distances)\n            # Normalize to 0-1 scale\n            return min(1.0, avg_distance / 2.0)  # 2.0 is arbitrary normalization factor\n        else:\n            return 1.0\n\n    def optimize_config_distribution(self):\n        \"\"\"Optimize the distribution of configurations based on results\"\"\"\n        if not self.results_history:\n            return\n\n        # Sort results by overall score\n        sorted_results = sorted(self.results_history, key=lambda x: x['overall_score'], reverse=True)\n\n        # Keep top performers and generate new variations\n        top_configs = [r['config'] for r in sorted_results[:5]]  # Top 5 configs\n\n        # Generate new configs based on successful patterns\n        new_configs = []\n        for top_config in top_configs:\n            for _ in range(2):  # Generate 2 variations of each top config\n                # Add small random perturbations to successful configs\n                new_config = DomainConfig(\n                    lighting_intensity=max(self.lighting_range[0],\n                                         min(self.lighting_range[1],\n                                             top_config.lighting_intensity + random.uniform(-0.1, 0.1))),\n                    fog_density=max(self.fog_range[0],\n                                  min(self.fog_range[1],\n                                      top_config.fog_density + random.uniform(-0.05, 0.05))),\n                    texture_complexity=max(self.texture_range[0],\n                                         min(self.texture_range[1],\n                                             top_config.texture_complexity + random.uniform(-0.1, 0.1))),\n                    object_count=max(self.object_count_range[0],\n                                   min(self.object_count_range[1],\n                                       top_config.object_count + random.randint(-2, 2))),\n                    camera_noise=max(self.camera_noise_range[0],\n                                   min(self.camera_noise_range[1],\n                                       top_config.camera_noise + random.uniform(-0.01, 0.01))),\n                    weather_type=top_config.weather_type if random.random() > 0.3 else random.choice(self.weather_types)\n                )\n                new_configs.append(new_config)\n\n        # Replace experiment configs with optimized set\n        self.experiment_configs = new_configs\n\n    def run_experiment_cycle(self):\n        \"\"\"Run one cycle of the domain randomization experiment\"\"\"\n        if self.experiment_configs:\n            # Get current configuration\n            current_config = self.experiment_configs[self.experiment_counter % len(self.experiment_configs)]\n\n            # Generate synthetic scene with current config\n            result = self.generate_synthetic_scene(current_config)\n\n            # Store result\n            self.results_history.append(result)\n            if len(self.results_history) > 50:  # Limit history size\n                self.results_history.pop(0)\n\n            # Publish configuration for Isaac Sim to use\n            config_msg = String()\n            config_dict = {\n                'lighting_intensity': current_config.lighting_intensity,\n                'fog_density': current_config.fog_density,\n                'texture_complexity': current_config.texture_complexity,\n                'object_count': current_config.object_count,\n                'camera_noise': current_config.camera_noise,\n                'weather_type': current_config.weather_type,\n                'experiment_id': self.experiment_counter\n            }\n            config_msg.data = json.dumps(config_dict)\n            self.config_pub.publish(config_msg)\n\n            # Publish metrics\n            metrics_msg = Float32MultiArray()\n            metrics_msg.data = [\n                result['quality_score'],\n                result['transferability_score'],\n                result['diversity_score'],\n                result['overall_score']\n            ]\n            self.metrics_pub.publish(metrics_msg)\n\n            # Publish status\n            status_msg = String()\n            status_msg.data = f\"Exp {self.experiment_counter}: L={current_config.lighting_intensity:.2f}, \" \\\n                             f\"F={current_config.fog_density:.2f}, W={current_config.weather_type}, \" \\\n                             f\"Score={result['overall_score']:.3f}\"\n            self.status_pub.publish(status_msg)\n\n            self.get_logger().info(f\"Experiment {self.experiment_counter}: Config={current_config.weather_type}, \" \\\n                                  f\"Score={result['overall_score']:.3f}\")\n\n            # Every 10 experiments, optimize the configuration distribution\n            if self.experiment_counter > 0 and self.experiment_counter % 10 == 0:\n                self.optimize_config_distribution()\n                self.get_logger().info(f\"Optimized configuration distribution. History size: {len(self.results_history)}\")\n\n            self.experiment_counter += 1\n\ndef main(args=None):\n    rclpy.init(args=args)\n    experiment = DomainRandomizationExperiment()\n\n    try:\n        rclpy.spin(experiment)\n    except KeyboardInterrupt:\n        experiment.get_logger().info(\"Domain Randomization Experiment stopped by user\")\n    finally:\n        experiment.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"quick-recap",children:"Quick Recap"}),"\n",(0,a.jsx)(n.p,{children:"In this lesson, we've covered:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Synthetic Data Principles"}),": Understanding the benefits and applications of synthetic data generation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Advanced Generation Pipeline"}),": Creating comprehensive synthetic data with multiple sensor modalities"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Domain Randomization"}),": Techniques for improving model generalization through environmental variation"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Quality Assessment"}),": Methods for validating synthetic data quality and transfer potential"]}),"\n",(0,a.jsxs)(n.li,{children:[(0,a.jsx)(n.strong,{children:"Experimentation Framework"}),": Systematic approaches to optimize synthetic data generation"]}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"Synthetic data generation with NVIDIA Isaac Sim enables the creation of diverse, perfectly labeled training datasets that can significantly accelerate AI model development while reducing real-world data collection costs. The combination of photorealistic rendering, physics simulation, and domain randomization techniques makes synthetic data an essential tool for modern robotics AI development."}),"\n",(0,a.jsx)(n.p,{children:"In the next lesson, we'll explore Isaac ROS for Visual Simultaneous Localization and Mapping (VSLAM), focusing on how Isaac Sim can be used to develop and test advanced perception systems."})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(_,{...e})}):_(e)}},8453(e,n,t){t.d(n,{R:()=>s,x:()=>o});var i=t(6540);const a={},r=i.createContext(a);function s(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);