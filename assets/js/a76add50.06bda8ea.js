"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[616],{6762(n,e,i){i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>r,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"physical-ai/the-ai-robot-brain-nvidia-isaac/introduction-to-nvidia-isaac-sim","title":"Introduction to NVIDIA Isaac Sim","description":"Overview","source":"@site/docs/physical-ai/the-ai-robot-brain-nvidia-isaac/introduction-to-nvidia-isaac-sim.md","sourceDirName":"physical-ai/the-ai-robot-brain-nvidia-isaac","slug":"/physical-ai/the-ai-robot-brain-nvidia-isaac/introduction-to-nvidia-isaac-sim","permalink":"/AI/docs/physical-ai/the-ai-robot-brain-nvidia-isaac/introduction-to-nvidia-isaac-sim","draft":false,"unlisted":false,"editUrl":"https://github.com/arijh/physical-ai-book/tree/main/packages/create-docusaurus/templates/shared/docs/physical-ai/the-ai-robot-brain-nvidia-isaac/introduction-to-nvidia-isaac-sim.md","tags":[],"version":"current","frontMatter":{"sidebar_label":"Introduction to NVIDIA Isaac Sim","title":"Introduction to NVIDIA Isaac Sim"},"sidebar":"docs","previous":{"title":"Sensor Simulation","permalink":"/AI/docs/physical-ai/the-digital-twin-gazebo-unity/sensor-simulation"},"next":{"title":"Synthetic Data Generation","permalink":"/AI/docs/physical-ai/the-ai-robot-brain-nvidia-isaac/synthetic-data-generation"}}');var a=i(4848),s=i(8453);const r={sidebar_label:"Introduction to NVIDIA Isaac Sim",title:"Introduction to NVIDIA Isaac Sim"},o="Introduction to NVIDIA Isaac Sim",l={},c=[{value:"Overview",id:"overview",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Hands-on Steps",id:"hands-on-steps",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Code Examples",id:"code-examples",level:2},{value:"Small Simulation",id:"small-simulation",level:2},{value:"Quick Recap",id:"quick-recap",level:2}];function d(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"introduction-to-nvidia-isaac-sim",children:"Introduction to NVIDIA Isaac Sim"})}),"\n",(0,a.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(e.p,{children:"NVIDIA Isaac Sim is a powerful robotics simulation platform built on NVIDIA Omniverse, designed specifically for developing, testing, and training AI-powered robots. It combines high-fidelity physics simulation with advanced graphics rendering, synthetic data generation, and AI training capabilities. Isaac Sim provides a comprehensive environment for creating intelligent robots that can perceive, navigate, and interact with complex real-world scenarios."}),"\n",(0,a.jsx)(e.p,{children:"Isaac Sim is particularly valuable for Physical AI development as it enables the creation of photorealistic simulation environments where AI algorithms can be trained on synthetic data that closely matches real-world conditions, significantly reducing the need for physical prototyping and real-world training data collection."}),"\n",(0,a.jsx)(e.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,a.jsx)(e.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Understand the architecture and capabilities of NVIDIA Isaac Sim"}),"\n",(0,a.jsx)(e.li,{children:"Set up Isaac Sim for robotics development"}),"\n",(0,a.jsx)(e.li,{children:"Create basic simulation environments with realistic lighting and physics"}),"\n",(0,a.jsx)(e.li,{children:"Integrate Isaac Sim with ROS 2 for robot control"}),"\n",(0,a.jsx)(e.li,{children:"Generate synthetic sensor data for AI training"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"hands-on-steps",children:"Hands-on Steps"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Isaac Sim Installation"}),": Set up Isaac Sim with Omniverse"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Basic Scene Creation"}),": Create a simple simulation environment"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Robot Integration"}),": Add a robot model to the simulation"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Sensor Configuration"}),": Set up synthetic sensors"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"ROS 2 Bridge"}),": Connect to ROS 2 for control and data exchange"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Understanding of ROS 2 concepts and basic node development"}),"\n",(0,a.jsx)(e.li,{children:"Knowledge of 3D modeling concepts"}),"\n",(0,a.jsx)(e.li,{children:"Basic understanding of AI and machine learning principles"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,a.jsx)(e.p,{children:"Let's start by exploring how to create a basic Isaac Sim environment using Python and Omniverse Kit extensions:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# isaac_sim_robot_control.py\nimport carb\nimport omni\nimport omni.ext\nimport omni.kit.ui\nimport omni.usd\nfrom pxr import Usd, UsdGeom, Gf, Sdf\nimport numpy as np\nimport math\n\n# This would be part of an actual Isaac Sim extension\n# For demonstration purposes, we'll show the conceptual structure\nclass IsaacSimController:\n    \"\"\"\n    Controller class for Isaac Sim operations\n    This is a conceptual representation of how Isaac Sim extensions work\n    \"\"\"\n    def __init__(self):\n        self.stage = None\n        self.robot_prim = None\n        self.physics_world = None\n        self.sensor_configs = {}\n\n    def create_basic_scene(self):\n        \"\"\"\n        Create a basic Isaac Sim scene with ground plane and simple objects\n        \"\"\"\n        # Get the current stage\n        self.stage = omni.usd.get_context().get_stage()\n\n        # Create a ground plane\n        ground_path = Sdf.Path(\"/World/groundPlane\")\n        ground_prim = self.stage.DefinePrim(ground_path, \"Xform\")\n        plane_mesh = UsdGeom.Mesh.Define(self.stage, \"/World/groundPlane/plane\")\n\n        # Configure the ground plane\n        plane_mesh.CreatePointsAttr().Set([\n            Gf.Vec3f(-10, -10, 0), Gf.Vec3f(10, -10, 0),\n            Gf.Vec3f(10, 10, 0), Gf.Vec3f(-10, 10, 0)\n        ])\n        plane_mesh.CreateFaceVertexIndicesAttr().Set([0, 1, 2, 0, 2, 3])\n        plane_mesh.CreateFaceVertexCountsAttr().Set([3, 3])\n\n        # Add physics properties\n        from omni.physx.scripts import particle_sample\n        particle_sample.add_rigidbody(ground_prim, self.stage)\n\n        carb.log_info(\"Basic scene created with ground plane\")\n\n    def add_robot_to_scene(self, robot_usd_path=\"/Isaac/Robots/Carter/carter_v1.usd\"):\n        \"\"\"\n        Add a robot to the scene (using Carter robot as example)\n        \"\"\"\n        if self.stage:\n            # Define robot prim\n            robot_path = Sdf.Path(\"/World/Robot\")\n            self.robot_prim = self.stage.DefinePrim(robot_path, \"Xform\")\n\n            # Add robot reference\n            self.robot_prim.GetReferences().AddReference(robot_usd_path)\n\n            # Set initial position\n            xform_api = UsdGeom.XformCommonAPI(self.robot_prim)\n            xform_api.SetTranslate((0.0, 0.0, 0.5))\n\n            carb.log_info(f\"Robot added to scene from {robot_usd_path}\")\n\n    def configure_sensors(self):\n        \"\"\"\n        Configure synthetic sensors for the robot\n        \"\"\"\n        # Configure RGB camera\n        camera_config = {\n            'type': 'rgb',\n            'position': [0.2, 0.0, 0.8],  # x, y, z offset from robot base\n            'rotation': [0.0, 0.0, 0.0],  # pitch, yaw, roll\n            'resolution': [640, 480],\n            'fov': 60.0,\n            'sensor_tick': 0.033  # 30 FPS\n        }\n\n        # Configure depth camera\n        depth_config = {\n            'type': 'depth',\n            'position': [0.2, 0.0, 0.8],\n            'resolution': [640, 480],\n            'min_depth': 0.1,\n            'max_depth': 10.0\n        }\n\n        # Configure LIDAR\n        lidar_config = {\n            'type': 'lidar',\n            'position': [0.25, 0.0, 0.9],\n            'rotation': [0.0, 0.0, 0.0],\n            'yaw_samples': 1080,  # Horizontal resolution\n            'yaw_lower_bond': -2.35619,  # -135 degrees\n            'yaw_upper_bond': 2.35619,   # 135 degrees\n            'pitch_samples': 1,  # Vertical resolution (1 for 2D lidar)\n            'range': 25.0,\n            'rotation_frequency': 10  # Hz\n        }\n\n        self.sensor_configs = {\n            'rgb_camera': camera_config,\n            'depth_camera': depth_config,\n            'lidar': lidar_config\n        }\n\n        carb.log_info(\"Sensors configured for robot\")\n\n    def setup_ros_bridge(self):\n        \"\"\"\n        Setup ROS 2 bridge for communication\n        \"\"\"\n        # This would configure the ROS bridge extension\n        # In practice, this involves setting up ROS publishers/subscribers\n        ros_config = {\n            'enabled': True,\n            'namespace': '/isaac_robot',\n            'topics': {\n                'cmd_vel': '/cmd_vel',\n                'odom': '/odom',\n                'scan': '/scan',\n                'rgb_image': '/camera/rgb/image_raw',\n                'depth_image': '/camera/depth/image_raw',\n                'imu': '/imu'\n            }\n        }\n\n        carb.log_info(f\"ROS bridge configured with namespace: {ros_config['namespace']}\")\n\n        return ros_config\n\ndef main():\n    \"\"\"\n    Main function demonstrating Isaac Sim setup\n    Note: This is a conceptual example - actual Isaac Sim extensions\n    would be integrated into the Omniverse extension system\n    \"\"\"\n    controller = IsaacSimController()\n\n    # Create basic scene\n    controller.create_basic_scene()\n\n    # Add robot\n    controller.add_robot_to_scene()\n\n    # Configure sensors\n    controller.configure_sensors()\n\n    # Setup ROS bridge\n    ros_config = controller.setup_ros_bridge()\n\n    print(\"Isaac Sim environment setup complete!\")\n    print(f\"Configured sensors: {list(controller.sensor_configs.keys())}\")\n    print(f\"ROS namespace: {ros_config['namespace']}\")\n\nif __name__ == \"__main__\":\n    main()\n"})}),"\n",(0,a.jsx)(e.p,{children:"Now let's create a more practical ROS 2 node that would interface with Isaac Sim:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# isaac_sim_ros_bridge.py\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist, Pose, Point\nfrom sensor_msgs.msg import LaserScan, Image, Imu, PointCloud2\nfrom nav_msgs.msg import Odometry\nfrom std_msgs.msg import String, Header\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport math\nfrom tf2_ros import TransformBroadcaster\nfrom geometry_msgs.msg import TransformStamped\n\nclass IsaacSimROSBridge(Node):\n    \"\"\"\n    ROS 2 bridge node for interfacing with Isaac Sim\n    \"\"\"\n    def __init__(self):\n        super().__init__('isaac_sim_ros_bridge')\n\n        # Publishers - These would publish to Isaac Sim\n        self.cmd_vel_sub = self.create_subscription(\n            Twist, '/isaac_robot/cmd_vel', self.cmd_vel_callback, 10)\n\n        # Publishers - These would publish sensor data from Isaac Sim\n        self.odom_pub = self.create_publisher(Odometry, '/isaac_robot/odom', 10)\n        self.scan_pub = self.create_publisher(LaserScan, '/isaac_robot/scan', 10)\n        self.rgb_pub = self.create_publisher(Image, '/isaac_robot/camera/rgb/image_raw', 10)\n        self.depth_pub = self.create_publisher(Image, '/isaac_robot/camera/depth/image_raw', 10)\n        self.imu_pub = self.create_publisher(Imu, '/isaac_robot/imu', 10)\n        self.status_pub = self.create_publisher(String, '/isaac_robot/status', 10)\n\n        # TF broadcaster\n        self.tf_broadcaster = TransformBroadcaster(self)\n\n        # Timers\n        self.sim_timer = self.create_timer(0.033, self.simulation_step)  # ~30 FPS\n        self.sensor_timer = self.create_timer(0.05, self.publish_sensor_data)  # 20 Hz\n\n        # Robot state\n        self.robot_pose = Pose()\n        self.robot_twist = Twist()\n        self.linear_velocity = 0.0\n        self.angular_velocity = 0.0\n        self.last_update_time = self.get_clock().now()\n\n        # Sensor simulation parameters\n        self.lidar_ranges = [float('inf')] * 1080  # 1080 samples\n        self.image_width = 640\n        self.image_height = 480\n\n        # CV bridge for image conversion\n        self.cv_bridge = CvBridge()\n\n        # Simulated environment features\n        self.simulated_objects = [\n            {'type': 'box', 'position': (2.0, 1.0, 0.5), 'size': (1.0, 1.0, 1.0)},\n            {'type': 'cylinder', 'position': (-1.5, -2.0, 0.5), 'radius': 0.5, 'height': 1.0},\n            {'type': 'sphere', 'position': (0.0, 3.0, 0.5), 'radius': 0.3}\n        ]\n\n        self.get_logger().info(\"Isaac Sim ROS Bridge initialized\")\n\n    def cmd_vel_callback(self, msg):\n        \"\"\"Process velocity commands from ROS\"\"\"\n        self.linear_velocity = msg.linear.x\n        self.angular_velocity = msg.angular.z\n\n    def simulation_step(self):\n        \"\"\"Main simulation step - update robot state based on commands\"\"\"\n        current_time = self.get_clock().now()\n        dt = (current_time - self.last_update_time).nanoseconds / 1e9\n        self.last_update_time = current_time\n\n        if dt > 0:\n            # Update robot position based on velocities\n            # Simple differential drive kinematics\n            if abs(self.angular_velocity) > 0.001:  # Turning\n                # Calculate turn radius and new position\n                turn_radius = self.linear_velocity / self.angular_velocity if abs(self.angular_velocity) > 0.001 else 0\n                angular_displacement = self.angular_velocity * dt\n                linear_displacement = self.linear_velocity * dt\n\n                # Update orientation\n                current_yaw = math.atan2(\n                    2 * (self.robot_pose.orientation.w * self.robot_pose.orientation.z +\n                         self.robot_pose.orientation.x * self.robot_pose.orientation.y),\n                    1 - 2 * (self.robot_pose.orientation.y**2 + self.robot_pose.orientation.z**2)\n                )\n                new_yaw = current_yaw + angular_displacement\n\n                # Update position\n                dx = linear_displacement * math.cos(new_yaw)\n                dy = linear_displacement * math.sin(new_yaw)\n\n                self.robot_pose.position.x += dx\n                self.robot_pose.position.y += dy\n                self.robot_pose.orientation.z = math.sin(new_yaw / 2)\n                self.robot_pose.orientation.w = math.cos(new_yaw / 2)\n            else:  # Moving straight\n                # Calculate displacement\n                dx = self.linear_velocity * dt * math.cos(\n                    math.atan2(2 * self.robot_pose.orientation.z * self.robot_pose.orientation.w,\n                              1 - 2 * self.robot_pose.orientation.z**2)\n                )\n                dy = self.linear_velocity * dt * math.sin(\n                    math.atan2(2 * self.robot_pose.orientation.z * self.robot_pose.orientation.w,\n                              1 - 2 * self.robot_pose.orientation.z**2)\n                )\n\n                self.robot_pose.position.x += dx\n                self.robot_pose.position.y += dy\n\n            # Update twist for odometry\n            self.robot_twist.linear.x = self.linear_velocity\n            self.robot_twist.angular.z = self.angular_velocity\n\n    def generate_lidar_scan(self):\n        \"\"\"Generate simulated LIDAR scan based on environment\"\"\"\n        # Create a simulated environment with objects\n        ranges = []\n        angle_min = -math.pi * 0.75  # -135 degrees\n        angle_max = math.pi * 0.75   # 135 degrees\n        angle_increment = (angle_max - angle_min) / len(self.lidar_ranges)\n\n        for i in range(len(self.lidar_ranges)):\n            angle = angle_min + i * angle_increment\n\n            # Transform to world coordinates\n            ray_direction = (\n                math.cos(angle),\n                math.sin(angle)\n            )\n\n            # Check for intersections with simulated objects\n            min_distance = float('inf')\n\n            for obj in self.simulated_objects:\n                if obj['type'] == 'box':\n                    # Simple box intersection (axis-aligned)\n                    pos = obj['position']\n                    size = obj['size']\n                    distance = self.ray_box_intersection(\n                        (self.robot_pose.position.x, self.robot_pose.position.y),\n                        ray_direction,\n                        pos[0], pos[1], size[0]/2, size[1]/2\n                    )\n                    if distance and distance < min_distance:\n                        min_distance = distance\n                elif obj['type'] == 'cylinder':\n                    # Cylinder intersection\n                    pos = obj['position']\n                    distance = self.ray_cylinder_intersection(\n                        (self.robot_pose.position.x, self.robot_pose.position.y),\n                        ray_direction,\n                        pos[0], pos[1], obj['radius']\n                    )\n                    if distance and distance < min_distance:\n                        min_distance = distance\n                elif obj['type'] == 'sphere':\n                    # Sphere intersection\n                    pos = obj['position']\n                    distance = self.ray_sphere_intersection(\n                        (self.robot_pose.position.x, self.robot_pose.position.y),\n                        ray_direction,\n                        pos[0], pos[1], obj['radius']\n                    )\n                    if distance and distance < min_distance:\n                        min_distance = distance\n\n            # Add some noise to make it more realistic\n            noise = np.random.normal(0, 0.01)  # 1cm standard deviation\n            final_range = min(min_distance, 25.0)  # Cap at max range\n            ranges.append(max(0.1, final_range + noise))  # Min range 0.1m\n\n        return ranges, angle_min, angle_increment\n\n    def ray_box_intersection(self, ray_origin, ray_dir, box_x, box_y, half_width, half_height):\n        \"\"\"Calculate intersection of ray with axis-aligned box\"\"\"\n        # Calculate intersection with box boundaries\n        t1 = (box_x - half_width - ray_origin[0]) / ray_dir[0] if ray_dir[0] != 0 else float('inf')\n        t2 = (box_x + half_width - ray_origin[0]) / ray_dir[0] if ray_dir[0] != 0 else float('inf')\n        t3 = (box_y - half_height - ray_origin[1]) / ray_dir[1] if ray_dir[1] != 0 else float('inf')\n        t4 = (box_y + half_height - ray_origin[1]) / ray_dir[1] if ray_dir[1] != 0 else float('inf')\n\n        t_min = max(min(t1, t2), min(t3, t4))\n        t_max = min(max(t1, t2), max(t3, t4))\n\n        if t_max >= 0 and t_min <= t_max:\n            return t_min if t_min >= 0 else t_max\n\n        return None\n\n    def ray_cylinder_intersection(self, ray_origin, ray_dir, cyl_x, cyl_y, radius):\n        \"\"\"Calculate intersection of ray with cylinder\"\"\"\n        # Convert to cylinder-relative coordinates\n        rel_x = ray_origin[0] - cyl_x\n        rel_y = ray_origin[1] - cyl_y\n\n        # Quadratic equation coefficients\n        a = ray_dir[0]**2 + ray_dir[1]**2\n        b = 2 * (rel_x * ray_dir[0] + rel_y * ray_dir[1])\n        c = rel_x**2 + rel_y**2 - radius**2\n\n        discriminant = b**2 - 4*a*c\n\n        if discriminant < 0:\n            return None\n\n        sqrt_disc = math.sqrt(discriminant)\n        t1 = (-b - sqrt_disc) / (2*a)\n        t2 = (-b + sqrt_disc) / (2*a)\n\n        # Return the smallest positive intersection\n        if t1 > 0:\n            return t1\n        elif t2 > 0:\n            return t2\n        else:\n            return None\n\n    def ray_sphere_intersection(self, ray_origin, ray_dir, sph_x, sph_y, radius):\n        \"\"\"Calculate intersection of ray with sphere\"\"\"\n        # Convert to sphere-relative coordinates\n        rel_x = ray_origin[0] - sph_x\n        rel_y = ray_origin[1] - sph_y\n\n        # Quadratic equation coefficients\n        a = ray_dir[0]**2 + ray_dir[1]**2\n        b = 2 * (rel_x * ray_dir[0] + rel_y * ray_dir[1])\n        c = rel_x**2 + rel_y**2 - radius**2\n\n        discriminant = b**2 - 4*a*c\n\n        if discriminant < 0:\n            return None\n\n        sqrt_disc = math.sqrt(discriminant)\n        t1 = (-b - sqrt_disc) / (2*a)\n        t2 = (-b + sqrt_disc) / (2*a)\n\n        # Return the smallest positive intersection\n        if t1 > 0:\n            return t1\n        elif t2 > 0:\n            return t2\n        else:\n            return None\n\n    def generate_camera_image(self):\n        \"\"\"Generate a simulated RGB image based on robot position\"\"\"\n        # Create a simple synthetic image based on robot position and environment\n        image = np.zeros((self.image_height, self.image_width, 3), dtype=np.uint8)\n\n        # Create a simple gradient background\n        for y in range(self.image_height):\n            for x in range(self.image_width):\n                # Gradient from blue (top) to green (bottom)\n                image[y, x, 0] = int(255 * y / self.image_height)  # Blue\n                image[y, x, 1] = int(255 * (1 - y / self.image_height))  # Green\n                image[y, x, 2] = 50  # Red (constant)\n\n        # Add simulated objects based on robot's view\n        robot_x, robot_y = self.robot_pose.position.x, self.robot_pose.position.y\n\n        for obj in self.simulated_objects:\n            # Calculate object position relative to robot\n            obj_x, obj_y = obj['position'][0], obj['position'][1]\n            rel_x = obj_x - robot_x\n            rel_y = obj_y - robot_y\n\n            # Calculate apparent size based on distance\n            distance = math.sqrt(rel_x**2 + rel_y**2)\n            if distance < 10:  # Only draw if close enough\n                # Project to image coordinates (simple perspective)\n                # This is a simplified projection for demonstration\n                img_x = int(self.image_width / 2 + rel_x * 50)  # Scale factor for visibility\n                img_y = int(self.image_height / 2 - rel_y * 50)  # Invert Y\n\n                if 0 <= img_x < self.image_width and 0 <= img_y < self.image_height:\n                    if obj['type'] == 'box':\n                        # Draw a rectangle\n                        size = int(obj['size'][0] * 30 / max(distance, 1))  # Scale with distance\n                        image[max(0, img_y-size):min(self.image_height, img_y+size),\n                              max(0, img_x-size):min(self.image_width, img_x+size)] = [255, 0, 0]  # Red\n                    elif obj['type'] == 'cylinder':\n                        # Draw a circle\n                        radius = int(obj['radius'] * 30 / max(distance, 1))\n                        cv2.circle(image, (img_x, img_y), radius, (0, 255, 0), -1)  # Green\n                    elif obj['type'] == 'sphere':\n                        # Draw a circle\n                        radius = int(obj['radius'] * 30 / max(distance, 1))\n                        cv2.circle(image, (img_x, img_y), radius, (0, 0, 255), -1)  # Blue\n\n        return image\n\n    def publish_sensor_data(self):\n        \"\"\"Publish simulated sensor data\"\"\"\n        current_time = self.get_clock().now()\n\n        # Publish odometry\n        odom_msg = Odometry()\n        odom_msg.header.stamp = current_time.to_msg()\n        odom_msg.header.frame_id = 'odom'\n        odom_msg.child_frame_id = 'base_link'\n        odom_msg.pose.pose = self.robot_pose\n        odom_msg.twist.twist = self.robot_twist\n\n        # Add some covariance to make it realistic\n        odom_msg.pose.covariance = [0.1, 0.0, 0.0, 0.0, 0.0, 0.0,\n                                   0.0, 0.1, 0.0, 0.0, 0.0, 0.0,\n                                   0.0, 0.0, 1000000.0, 0.0, 0.0, 0.0,\n                                   0.0, 0.0, 0.0, 1000000.0, 0.0, 0.0,\n                                   0.0, 0.0, 0.0, 0.0, 1000000.0, 0.0,\n                                   0.0, 0.0, 0.0, 0.0, 0.0, 0.05]\n        odom_msg.twist.covariance = [0.1, 0.0, 0.0, 0.0, 0.0, 0.0,\n                                    0.0, 0.1, 0.0, 0.0, 0.0, 0.0,\n                                    0.0, 0.0, 1000000.0, 0.0, 0.0, 0.0,\n                                    0.0, 0.0, 0.0, 1000000.0, 0.0, 0.0,\n                                    0.0, 0.0, 0.0, 0.0, 1000000.0, 0.0,\n                                    0.0, 0.0, 0.0, 0.0, 0.0, 0.05]\n\n        self.odom_pub.publish(odom_msg)\n\n        # Publish LIDAR scan\n        ranges, angle_min, angle_increment = self.generate_lidar_scan()\n        scan_msg = LaserScan()\n        scan_msg.header.stamp = current_time.to_msg()\n        scan_msg.header.frame_id = 'laser_frame'\n        scan_msg.angle_min = angle_min\n        scan_msg.angle_max = -angle_min  # 135 degrees\n        scan_msg.angle_increment = angle_increment\n        scan_msg.time_increment = 0.0\n        scan_msg.scan_time = 0.1  # 10Hz\n        scan_msg.range_min = 0.1\n        scan_msg.range_max = 25.0\n        scan_msg.ranges = ranges\n        # Add intensities (simulated)\n        scan_msg.intensities = [100.0] * len(ranges)\n\n        self.scan_pub.publish(scan_msg)\n\n        # Publish camera image\n        image = self.generate_camera_image()\n        image_msg = self.cv_bridge.cv2_to_imgmsg(image, encoding=\"bgr8\")\n        image_msg.header.stamp = current_time.to_msg()\n        image_msg.header.frame_id = 'camera_rgb_frame'\n\n        self.rgb_pub.publish(image_msg)\n\n        # Publish depth image (simplified)\n        depth_image = np.ones((self.image_height, self.image_width), dtype=np.float32) * 5.0  # Default depth\n        # Add some variation based on the same objects\n        for obj in self.simulated_objects:\n            obj_x, obj_y = obj['position'][0], obj['position'][1]\n            rel_x = obj_x - self.robot_pose.position.x\n            rel_y = obj_y - self.robot_pose.position.y\n            distance = math.sqrt(rel_x**2 + rel_y**2)\n\n            # Project to image coordinates\n            img_x = int(self.image_width / 2 + rel_x * 50)\n            img_y = int(self.image_height / 2 - rel_y * 50)\n\n            if 0 <= img_x < self.image_width and 0 <= img_y < self.image_height:\n                size = max(1, int(30 / max(distance, 1)))\n                depth_image[max(0, img_y-size):min(self.image_height, img_y+size),\n                           max(0, img_x-size):min(self.image_width, img_x+size)] = distance\n\n        depth_msg = self.cv_bridge.cv2_to_imgmsg(depth_image, encoding=\"32FC1\")\n        depth_msg.header.stamp = current_time.to_msg()\n        depth_msg.header.frame_id = 'camera_depth_frame'\n\n        self.depth_pub.publish(depth_msg)\n\n        # Publish IMU data (simulated)\n        imu_msg = Imu()\n        imu_msg.header.stamp = current_time.to_msg()\n        imu_msg.header.frame_id = 'imu_frame'\n\n        # Set orientation (from robot pose)\n        imu_msg.orientation = self.robot_pose.orientation\n\n        # Add some noise to make realistic\n        imu_msg.orientation_covariance = [0.01, 0.0, 0.0,\n                                         0.0, 0.01, 0.0,\n                                         0.0, 0.0, 0.01]\n\n        # Angular velocity (from robot twist)\n        imu_msg.angular_velocity.z = self.robot_twist.angular.z\n        imu_msg.angular_velocity_covariance = [0.01, 0.0, 0.0,\n                                              0.0, 0.01, 0.0,\n                                              0.0, 0.0, 0.01]\n\n        # Linear acceleration (simulate gravity and movement)\n        imu_msg.linear_acceleration.x = self.robot_twist.linear.x * 2.0  # Simulate acceleration\n        imu_msg.linear_acceleration.y = 0.0\n        imu_msg.linear_acceleration.z = 9.81  # Gravity\n        imu_msg.linear_acceleration_covariance = [0.01, 0.0, 0.0,\n                                                 0.0, 0.01, 0.0,\n                                                 0.0, 0.0, 0.01]\n\n        self.imu_pub.publish(imu_msg)\n\n        # Publish status\n        status_msg = String()\n        status_msg.data = f\"Pos: ({self.robot_pose.position.x:.2f}, {self.robot_pose.position.y:.2f}), \" \\\n                         f\"Vel: ({self.linear_velocity:.2f}, {self.angular_velocity:.2f}), \" \\\n                         f\"Objects: {len(self.simulated_objects)}\"\n        self.status_pub.publish(status_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    bridge = IsaacSimROSBridge()\n\n    try:\n        rclpy.spin(bridge)\n    except KeyboardInterrupt:\n        bridge.get_logger().info(\"Isaac Sim ROS Bridge stopped by user\")\n    finally:\n        bridge.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    import cv2  # Import here to avoid issues if not available\n    main()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"small-simulation",children:"Small Simulation"}),"\n",(0,a.jsx)(e.p,{children:"Let's create a synthetic data generation example that demonstrates Isaac Sim's capability for AI training:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"# synthetic_data_generator.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan, PointCloud2\nfrom geometry_msgs.msg import PoseStamped\nfrom std_msgs.msg import String, Header\nfrom cv_bridge import CvBridge\nimport numpy as np\nimport math\nimport json\nimport os\nfrom datetime import datetime\n\nclass SyntheticDataGenerator(Node):\n    \"\"\"\n    Generate synthetic training data for AI models using Isaac Sim principles\n    \"\"\"\n    def __init__(self):\n        super().__init__('synthetic_data_generator')\n\n        # Publishers\n        self.rgb_pub = self.create_publisher(Image, '/synthetic_data/rgb', 10)\n        self.depth_pub = self.create_publisher(Image, '/synthetic_data/depth', 10)\n        self.seg_pub = self.create_publisher(Image, '/synthetic_data/segmentation', 10)\n        self.scan_pub = self.create_publisher(LaserScan, '/synthetic_data/lidar', 10)\n        self.status_pub = self.create_publisher(String, '/synthetic_data/status', 10)\n\n        # Timers\n        self.data_gen_timer = self.create_timer(0.5, self.generate_synthetic_data)\n\n        # Data collection parameters\n        self.cv_bridge = CvBridge()\n        self.data_counter = 0\n        self.collection_enabled = True\n        self.collection_dir = \"/tmp/isaac_synthetic_data\"\n\n        # Create collection directory\n        os.makedirs(self.collection_dir, exist_ok=True)\n\n        # Scene parameters for variation\n        self.scene_configurations = [\n            {'lighting': 'bright', 'weather': 'clear', 'objects': 5},\n            {'lighting': 'dim', 'weather': 'overcast', 'objects': 8},\n            {'lighting': 'bright', 'weather': 'sunny', 'objects': 3},\n            {'lighting': 'variable', 'weather': 'partly_cloudy', 'objects': 6}\n        ]\n        self.current_scene_idx = 0\n\n        # Object classes for segmentation\n        self.object_classes = {\n            1: 'robot',\n            2: 'obstacle',\n            3: 'furniture',\n            4: 'wall',\n            5: 'floor'\n        }\n\n        self.get_logger().info(\"Synthetic Data Generator initialized\")\n\n    def generate_random_scene(self):\n        \"\"\"Generate a random scene configuration\"\"\"\n        scene = self.scene_configurations[self.current_scene_idx]\n        self.current_scene_idx = (self.current_scene_idx + 1) % len(self.scene_configurations)\n\n        # Generate random objects in the scene\n        objects = []\n        for i in range(scene['objects']):\n            obj_type = np.random.choice(['box', 'cylinder', 'sphere'])\n            obj_class = np.random.choice(list(self.object_classes.keys())[1:])  # Exclude robot (class 1)\n\n            obj = {\n                'type': obj_type,\n                'class': obj_class,\n                'position': (\n                    np.random.uniform(-5, 5),\n                    np.random.uniform(-5, 5),\n                    np.random.uniform(0.1, 2.0)\n                ),\n                'size': (\n                    np.random.uniform(0.2, 1.5),\n                    np.random.uniform(0.2, 1.5),\n                    np.random.uniform(0.2, 2.0)\n                ) if obj_type == 'box' else (\n                    np.random.uniform(0.2, 1.0),  # radius for cylinder/sphere\n                    np.random.uniform(0.5, 2.0)   # height for cylinder\n                ),\n                'color': np.random.uniform(0.2, 1.0, 3).tolist()\n            }\n            objects.append(obj)\n\n        return scene, objects\n\n    def generate_synthetic_rgb(self, objects, lighting_condition):\n        \"\"\"Generate synthetic RGB image\"\"\"\n        height, width = 480, 640\n        image = np.zeros((height, width, 3), dtype=np.uint8)\n\n        # Apply lighting condition\n        if lighting_condition == 'bright':\n            base_brightness = 0.8\n        elif lighting_condition == 'dim':\n            base_brightness = 0.3\n        else:\n            base_brightness = 0.6\n\n        # Create a base environment\n        for y in range(height):\n            for x in range(width):\n                # Create a gradient floor\n                floor_intensity = 100 + int(50 * y / height)\n                image[y, x] = [floor_intensity, floor_intensity, floor_intensity]\n\n        # Add objects to the image\n        for obj in objects:\n            # Project 3D object to 2D image\n            # Simplified projection for demonstration\n            center_x = int(width / 2 + obj['position'][0] * 30)\n            center_y = int(height / 2 - obj['position'][1] * 30)\n\n            if 0 <= center_x < width and 0 <= center_y < height:\n                # Draw the object based on its type\n                if obj['type'] == 'box':\n                    size = max(1, int(obj['size'][0] * 20))\n                    color = [int(c * 255) for c in obj['color']]\n                    image[max(0, center_y-size):min(height, center_y+size),\n                          max(0, center_x-size):min(width, center_x+size)] = color\n                elif obj['type'] in ['cylinder', 'sphere']:\n                    radius = max(1, int(obj['size'][0] * 20))\n                    color = [int(c * 255) for c in obj['color']]\n                    for dy in range(-radius, radius):\n                        for dx in range(-radius, radius):\n                            if dx*dx + dy*dy <= radius*radius:\n                                py, px = center_y + dy, center_x + dx\n                                if 0 <= py < height and 0 <= px < width:\n                                    image[py, px] = color\n\n        # Apply lighting effects\n        image = (image * base_brightness).astype(np.uint8)\n\n        # Add some noise to make more realistic\n        noise = np.random.normal(0, 5, image.shape).astype(np.int16)\n        image = np.clip(image + noise, 0, 255).astype(np.uint8)\n\n        return image\n\n    def generate_synthetic_depth(self, objects):\n        \"\"\"Generate synthetic depth image\"\"\"\n        height, width = 480, 640\n        depth = np.ones((height, width), dtype=np.float32) * 10.0  # Default max distance\n\n        # For each object, calculate its depth projection\n        for obj in objects:\n            center_x = int(width / 2 + obj['position'][0] * 30)\n            center_y = int(height / 2 - obj['position'][1] * 30)\n            distance = math.sqrt(obj['position'][0]**2 + obj['position'][1]**2 + obj['position'][2]**2)\n\n            if 0 <= center_x < width and 0 <= center_y < height:\n                if obj['type'] == 'box':\n                    size = max(1, int(obj['size'][0] * 20))\n                    depth[max(0, center_y-size):min(height, center_y+size),\n                          max(0, center_x-size):min(width, center_x+size)] = distance\n                elif obj['type'] in ['cylinder', 'sphere']:\n                    radius = max(1, int(obj['size'][0] * 20))\n                    for dy in range(-radius, radius):\n                        for dx in range(-radius, radius):\n                            if dx*dx + dy*dy <= radius*radius:\n                                py, px = center_y + dy, center_x + dx\n                                if 0 <= py < height and 0 <= px < width:\n                                    depth[py, px] = distance\n\n        # Add some noise\n        noise = np.random.normal(0, 0.05, depth.shape).astype(np.float32)\n        depth = np.maximum(0.1, depth + noise)  # Ensure minimum distance\n\n        return depth\n\n    def generate_synthetic_segmentation(self, objects):\n        \"\"\"Generate synthetic segmentation mask\"\"\"\n        height, width = 480, 640\n        segmentation = np.zeros((height, width), dtype=np.uint8)\n\n        # Floor class\n        segmentation[:, :] = 5  # Floor class\n\n        # Add objects with their class IDs\n        for obj in objects:\n            center_x = int(width / 2 + obj['position'][0] * 30)\n            center_y = int(height / 2 - obj['position'][1] * 30)\n\n            if 0 <= center_x < width and 0 <= center_y < height:\n                if obj['type'] == 'box':\n                    size = max(1, int(obj['size'][0] * 20))\n                    segmentation[max(0, center_y-size):min(height, center_y+size),\n                                max(0, center_x-size):min(width, center_x+size)] = obj['class']\n                elif obj['type'] in ['cylinder', 'sphere']:\n                    radius = max(1, int(obj['size'][0] * 20))\n                    for dy in range(-radius, radius):\n                        for dx in range(-radius, radius):\n                            if dx*dx + dy*dy <= radius*radius:\n                                py, px = center_y + dy, center_x + dx\n                                if 0 <= py < height and 0 <= px < width:\n                                    segmentation[py, px] = obj['class']\n\n        # Convert to 3-channel image for ROS compatibility\n        seg_image = np.stack([segmentation, segmentation, segmentation], axis=2)\n        return seg_image.astype(np.uint8)\n\n    def generate_synthetic_lidar(self, objects):\n        \"\"\"Generate synthetic LIDAR scan\"\"\"\n        num_points = 1080  # Common for 2D LIDAR\n        angle_min = -math.pi * 0.75  # -135 degrees\n        angle_max = math.pi * 0.75   # 135 degrees\n        angle_increment = (angle_max - angle_min) / num_points\n\n        ranges = []\n\n        for i in range(num_points):\n            angle = angle_min + i * angle_increment\n            ray_direction = (math.cos(angle), math.sin(angle))\n\n            # Find closest object in this direction\n            min_distance = 25.0  # Max range\n\n            for obj in objects:\n                if obj['type'] == 'box':\n                    distance = self.ray_box_intersection(\n                        (0, 0), ray_direction,  # Robot at origin\n                        obj['position'][0], obj['position'][1],\n                        obj['size'][0]/2, obj['size'][1]/2\n                    )\n                elif obj['type'] in ['cylinder', 'sphere']:\n                    distance = self.ray_cylinder_intersection(\n                        (0, 0), ray_direction,\n                        obj['position'][0], obj['position'][1],\n                        obj['size'][0]  # radius\n                    )\n\n                if distance and distance < min_distance:\n                    min_distance = distance\n\n            # Add noise and ensure valid range\n            noise = np.random.normal(0, 0.01)\n            final_range = max(0.1, min(25.0, min_distance + noise))\n            ranges.append(final_range)\n\n        return ranges, angle_min, angle_increment\n\n    def ray_box_intersection(self, ray_origin, ray_dir, box_x, box_y, half_width, half_height):\n        \"\"\"Calculate intersection of ray with axis-aligned box\"\"\"\n        t1 = (box_x - half_width - ray_origin[0]) / ray_dir[0] if ray_dir[0] != 0 else float('inf')\n        t2 = (box_x + half_width - ray_origin[0]) / ray_dir[0] if ray_dir[0] != 0 else float('inf')\n        t3 = (box_y - half_height - ray_origin[1]) / ray_dir[1] if ray_dir[1] != 0 else float('inf')\n        t4 = (box_y + half_height - ray_origin[1]) / ray_dir[1] if ray_dir[1] != 0 else float('inf')\n\n        t_min = max(min(t1, t2), min(t3, t4))\n        t_max = min(max(t1, t2), max(t3, t4))\n\n        if t_max >= 0 and t_min <= t_max:\n            return t_min if t_min >= 0 else t_max\n\n        return None\n\n    def ray_cylinder_intersection(self, ray_origin, ray_dir, cyl_x, cyl_y, radius):\n        \"\"\"Calculate intersection of ray with cylinder\"\"\"\n        rel_x = ray_origin[0] - cyl_x\n        rel_y = ray_origin[1] - cyl_y\n\n        a = ray_dir[0]**2 + ray_dir[1]**2\n        b = 2 * (rel_x * ray_dir[0] + rel_y * ray_dir[1])\n        c = rel_x**2 + rel_y**2 - radius**2\n\n        discriminant = b**2 - 4*a*c\n\n        if discriminant < 0:\n            return None\n\n        sqrt_disc = math.sqrt(discriminant)\n        t1 = (-b - sqrt_disc) / (2*a)\n        t2 = (-b + sqrt_disc) / (2*a)\n\n        if t1 > 0:\n            return t1\n        elif t2 > 0:\n            return t2\n        else:\n            return None\n\n    def save_synthetic_data(self, rgb_image, depth_image, seg_image, lidar_ranges, metadata):\n        \"\"\"Save synthetic data to disk with metadata\"\"\"\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n\n        # Save images\n        import cv2\n        cv2.imwrite(f\"{self.collection_dir}/rgb_{timestamp}.png\", cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR))\n        cv2.imwrite(f\"{self.collection_dir}/depth_{timestamp}.png\", (depth_image * 256).astype(np.uint16))\n        cv2.imwrite(f\"{self.collection_dir}/seg_{timestamp}.png\", seg_image)\n\n        # Save LIDAR data\n        np.save(f\"{self.collection_dir}/lidar_{timestamp}.npy\", np.array(lidar_ranges))\n\n        # Save metadata\n        metadata['timestamp'] = timestamp\n        with open(f\"{self.collection_dir}/metadata_{timestamp}.json\", 'w') as f:\n            json.dump(metadata, f, indent=2)\n\n    def generate_synthetic_data(self):\n        \"\"\"Main function to generate synthetic data\"\"\"\n        if not self.collection_enabled:\n            return\n\n        current_time = self.get_clock().now()\n\n        # Generate random scene\n        scene_config, objects = self.generate_random_scene()\n\n        # Generate synthetic sensor data\n        rgb_image = self.generate_synthetic_rgb(objects, scene_config['lighting'])\n        depth_image = self.generate_synthetic_depth(objects)\n        seg_image = self.generate_synthetic_segmentation(objects)\n        lidar_ranges, angle_min, angle_increment = self.generate_synthetic_lidar(objects)\n\n        # Create ROS messages\n        rgb_msg = self.cv_bridge.cv2_to_imgmsg(rgb_image, encoding=\"rgb8\")\n        rgb_msg.header.stamp = current_time.to_msg()\n        rgb_msg.header.frame_id = 'camera_rgb_frame'\n\n        depth_msg = self.cv_bridge.cv2_to_imgmsg(depth_image, encoding=\"32FC1\")\n        depth_msg.header.stamp = current_time.to_msg()\n        depth_msg.header.frame_id = 'camera_depth_frame'\n\n        seg_msg = self.cv_bridge.cv2_to_imgmsg(seg_image, encoding=\"rgb8\")\n        seg_msg.header.stamp = current_time.to_msg()\n        seg_msg.header.frame_id = 'camera_seg_frame'\n\n        scan_msg = LaserScan()\n        scan_msg.header.stamp = current_time.to_msg()\n        scan_msg.header.frame_id = 'laser_frame'\n        scan_msg.angle_min = angle_min\n        scan_msg.angle_max = -angle_min\n        scan_msg.angle_increment = angle_increment\n        scan_msg.time_increment = 0.0\n        scan_msg.scan_time = 0.1\n        scan_msg.range_min = 0.1\n        scan_msg.range_max = 25.0\n        scan_msg.ranges = lidar_ranges\n\n        # Publish synthetic data\n        self.rgb_pub.publish(rgb_msg)\n        self.depth_pub.publish(depth_msg)\n        self.seg_pub.publish(seg_msg)\n        self.scan_pub.publish(scan_msg)\n\n        # Save to disk for AI training\n        metadata = {\n            'scene_config': scene_config,\n            'object_count': len(objects),\n            'objects': objects,\n            'collection_id': self.data_counter\n        }\n        self.save_synthetic_data(rgb_image, depth_image, seg_image, lidar_ranges, metadata)\n\n        # Publish status\n        status_msg = String()\n        status_msg.data = f\"Synthetic Data #{self.data_counter}: Objects={len(objects)}, \" \\\n                         f\"Lighting={scene_config['lighting']}, Weather={scene_config['weather']}\"\n        self.status_pub.publish(status_msg)\n\n        self.data_counter += 1\n\n        self.get_logger().info(f\"Generated synthetic data batch #{self.data_counter}\")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    generator = SyntheticDataGenerator()\n\n    try:\n        rclpy.spin(generator)\n    except KeyboardInterrupt:\n        generator.get_logger().info(\"Synthetic Data Generator stopped by user\")\n    finally:\n        generator.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(e.h2,{id:"quick-recap",children:"Quick Recap"}),"\n",(0,a.jsx)(e.p,{children:"In this lesson, we've covered:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Isaac Sim Architecture"}),": Understanding the components and capabilities of NVIDIA Isaac Sim"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Environment Setup"}),": Creating simulation environments with realistic physics and graphics"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Robot Integration"}),": Adding robots to Isaac Sim with proper sensor configurations"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"ROS Bridge"}),": Connecting Isaac Sim to ROS 2 for control and data exchange"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Synthetic Data Generation"}),": Creating training data for AI models with realistic variation"]}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:"NVIDIA Isaac Sim provides a comprehensive platform for developing AI-powered robots by combining high-fidelity physics simulation with advanced graphics rendering. This enables the creation of photorealistic training data that can bridge the reality gap between simulation and real-world deployment."}),"\n",(0,a.jsx)(e.p,{children:"In the next lesson, we'll explore synthetic data generation in more detail, focusing on how Isaac Sim can create diverse and realistic datasets for training perception and navigation models."})]})}function m(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(d,{...n})}):d(n)}},8453(n,e,i){i.d(e,{R:()=>r,x:()=>o});var t=i(6540);const a={},s=t.createContext(a);function r(n){const e=t.useContext(s);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function o(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:r(n.components),t.createElement(s.Provider,{value:e},n.children)}}}]);