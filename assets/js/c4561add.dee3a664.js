"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[417],{1167(e,n,t){t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>d,frontMatter:()=>s,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"physical-ai/vision-language-action-vla/voice-to-action-using-whisper","title":"Voice-to-Action using Whisper","description":"Overview","source":"@site/docs/physical-ai/vision-language-action-vla/voice-to-action-using-whisper.md","sourceDirName":"physical-ai/vision-language-action-vla","slug":"/physical-ai/vision-language-action-vla/voice-to-action-using-whisper","permalink":"/physical-ai-book/docs/physical-ai/vision-language-action-vla/voice-to-action-using-whisper","draft":false,"unlisted":false,"editUrl":"https://github.com/arijh/physical-ai-book/tree/main/packages/create-docusaurus/templates/shared/docs/physical-ai/vision-language-action-vla/voice-to-action-using-whisper.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2},"sidebar":"docs","previous":{"title":"Introduction to Vision-Language-Action Integration","permalink":"/physical-ai-book/docs/physical-ai/vision-language-action-vla/introduction-to-vla-integration"},"next":{"title":"Cognitive Planning with LLMs Mapped to ROS 2 Actions","permalink":"/physical-ai-book/docs/physical-ai/vision-language-action-vla/cognitive-planning-with-llms"}}');var a=t(4848),i=t(8453);const s={sidebar_position:2},r="Voice-to-Action using Whisper",c={},l=[{value:"Overview",id:"overview",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Hands-on Steps",id:"hands-on-steps",level:2},{value:"Step 1: Set up the Voice Recognition Environment",id:"step-1-set-up-the-voice-recognition-environment",level:3},{value:"Step 2: Create the Voice Recognition Node",id:"step-2-create-the-voice-recognition-node",level:3},{value:"Step 3: Create the Voice Command Parser",id:"step-3-create-the-voice-command-parser",level:3},{value:"Step 4: Create the Voice Action Executor",id:"step-4-create-the-voice-action-executor",level:3},{value:"Step 5: Create the Main Voice Control Launch File",id:"step-5-create-the-main-voice-control-launch-file",level:3},{value:"Step 6: Test the Voice Control System",id:"step-6-test-the-voice-control-system",level:3},{value:"Code Examples",id:"code-examples",level:2},{value:"Small Simulation",id:"small-simulation",level:2},{value:"Quick Recap",id:"quick-recap",level:2}];function m(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"voice-to-action-using-whisper",children:"Voice-to-Action using Whisper"})}),"\n",(0,a.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(n.p,{children:"This lesson explores how to implement voice-to-action systems using OpenAI's Whisper speech recognition model. We'll create a pipeline that captures voice commands, transcribes them using Whisper, and converts them into ROS 2 actions for humanoid robots. This approach enables natural human-robot interaction through spoken language."}),"\n",(0,a.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,a.jsx)(n.p,{children:"By the end of this lesson, you will:"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:"Understand the architecture of voice-to-action systems for robotics"}),"\n",(0,a.jsx)(n.li,{children:"Implement speech recognition using OpenAI's Whisper model"}),"\n",(0,a.jsx)(n.li,{children:"Map voice commands to ROS 2 actions and services"}),"\n",(0,a.jsx)(n.li,{children:"Create a robust voice command parser for humanoid robots"}),"\n",(0,a.jsx)(n.li,{children:"Integrate voice control with existing ROS 2 navigation and manipulation systems"}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"hands-on-steps",children:"Hands-on Steps"}),"\n",(0,a.jsx)(n.h3,{id:"step-1-set-up-the-voice-recognition-environment",children:"Step 1: Set up the Voice Recognition Environment"}),"\n",(0,a.jsx)(n.p,{children:"First, let's create the necessary packages and dependencies for our voice-to-action system."}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"# Create the voice recognition package\ncd ~/ros2_ws/src\nros2 pkg create --dependencies rclpy std_msgs sensor_msgs geometry_msgs action_msgs --node-name voice_recognition_node voice_to_action\n\ncd voice_to_action\nmkdir -p voice_to_action/{perception,utils,nlp}\n"})}),"\n",(0,a.jsx)(n.p,{children:"Install the required Python dependencies:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-bash",children:"pip install openai-whisper torch torchaudio ros2_numpy\n"})}),"\n",(0,a.jsx)(n.h3,{id:"step-2-create-the-voice-recognition-node",children:"Step 2: Create the Voice Recognition Node"}),"\n",(0,a.jsx)(n.p,{children:"Create the main voice recognition node that will capture audio, process it with Whisper, and publish recognized commands:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# voice_to_action/voice_to_action/voice_recognition_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String, Bool\nfrom geometry_msgs.msg import Twist\nfrom sensor_msgs.msg import AudioData\nimport whisper\nimport torch\nimport numpy as np\nimport threading\nimport queue\nimport pyaudio\nimport wave\nimport io\nfrom vosk import Model, KaldiRecognizer\n\nclass VoiceRecognitionNode(Node):\n    def __init__(self):\n        super().__init__('voice_recognition_node')\n\n        # Initialize Whisper model\n        self.get_logger().info('Loading Whisper model...')\n        self.model = whisper.load_model(\"base\")  # Use \"small\" or \"medium\" for better accuracy\n\n        # Publishers\n        self.command_pub = self.create_publisher(String, 'voice_command', 10)\n        self.velocity_pub = self.create_publisher(Twist, 'cmd_vel', 10)\n\n        # Subscribers\n        self.audio_sub = self.create_subscription(\n            AudioData,\n            'audio_input',\n            self.audio_callback,\n            10\n        )\n\n        # Parameters\n        self.declare_parameter('sample_rate', 16000)\n        self.sample_rate = self.get_parameter('sample_rate').value\n\n        self.declare_parameter('chunk_size', 1024)\n        self.chunk_size = self.get_parameter('chunk_size').value\n\n        # Audio recording setup\n        self.audio_queue = queue.Queue()\n        self.recording = False\n        self.audio_thread = None\n\n        # Start audio recording thread\n        self.start_audio_recording()\n\n        # Timer for processing audio chunks\n        self.timer = self.create_timer(1.0, self.process_audio)\n\n        self.get_logger().info('Voice Recognition Node initialized')\n\n    def start_audio_recording(self):\n        \"\"\"Start audio recording thread\"\"\"\n        self.recording = True\n        self.audio_thread = threading.Thread(target=self.record_audio)\n        self.audio_thread.daemon = True\n        self.audio_thread.start()\n\n    def record_audio(self):\n        \"\"\"Record audio from microphone\"\"\"\n        p = pyaudio.PyAudio()\n\n        stream = p.open(\n            format=pyaudio.paInt16,\n            channels=1,\n            rate=self.sample_rate,\n            input=True,\n            frames_per_buffer=self.chunk_size\n        )\n\n        self.get_logger().info('Recording audio...')\n\n        while self.recording:\n            data = stream.read(self.chunk_size)\n            self.audio_queue.put(data)\n\n        stream.stop_stream()\n        stream.close()\n        p.terminate()\n\n    def audio_callback(self, msg):\n        \"\"\"Callback for audio data from ROS topic\"\"\"\n        # Convert audio message to numpy array\n        audio_data = np.frombuffer(msg.data, dtype=np.int16).astype(np.float32) / 32768.0\n\n        # Add to processing queue\n        self.audio_queue.put(audio_data.tobytes())\n\n    def process_audio(self):\n        \"\"\"Process accumulated audio data with Whisper\"\"\"\n        if self.audio_queue.empty():\n            return\n\n        # Collect audio data from queue\n        audio_chunks = []\n        while not self.audio_queue.empty():\n            chunk = self.audio_queue.get()\n            if isinstance(chunk, bytes):\n                # Convert bytes to float32 numpy array\n                chunk_array = np.frombuffer(chunk, dtype=np.int16).astype(np.float32) / 32768.0\n                audio_chunks.append(chunk_array)\n            else:\n                audio_chunks.append(chunk)\n\n        if len(audio_chunks) == 0:\n            return\n\n        # Concatenate all audio chunks\n        full_audio = np.concatenate(audio_chunks)\n\n        # Process with Whisper\n        try:\n            # Convert to tensor and process\n            audio_tensor = torch.from_numpy(full_audio).float()\n\n            # Transcribe using Whisper\n            result = self.model.transcribe(audio_tensor.numpy(), fp16=False)\n            transcription = result['text'].strip()\n\n            if transcription:\n                self.get_logger().info(f'Recognized: {transcription}')\n\n                # Publish the recognized command\n                cmd_msg = String()\n                cmd_msg.data = transcription\n                self.command_pub.publish(cmd_msg)\n\n                # Parse and execute the command\n                self.parse_voice_command(transcription)\n\n        except Exception as e:\n            self.get_logger().error(f'Error processing audio: {str(e)}')\n\n    def parse_voice_command(self, command_text):\n        \"\"\"Parse voice command and convert to ROS 2 actions\"\"\"\n        command_text = command_text.lower().strip()\n\n        # Define command mappings\n        if 'move forward' in command_text or 'go forward' in command_text:\n            self.execute_navigation_command('forward')\n        elif 'move backward' in command_text or 'go backward' in command_text:\n            self.execute_navigation_command('backward')\n        elif 'turn left' in command_text:\n            self.execute_navigation_command('left')\n        elif 'turn right' in command_text:\n            self.execute_navigation_command('right')\n        elif 'stop' in command_text:\n            self.execute_stop_command()\n        elif 'wave' in command_text:\n            self.execute_arm_command('wave')\n        elif 'raise arm' in command_text:\n            self.execute_arm_command('raise')\n        elif 'lower arm' in command_text:\n            self.execute_arm_command('lower')\n        elif 'dance' in command_text:\n            self.execute_dance_command()\n        else:\n            self.get_logger().info(f'Unknown command: {command_text}')\n\n    def execute_navigation_command(self, direction):\n        \"\"\"Execute navigation commands\"\"\"\n        twist_msg = Twist()\n\n        if direction == 'forward':\n            twist_msg.linear.x = 0.5  # Forward speed\n        elif direction == 'backward':\n            twist_msg.linear.x = -0.5  # Backward speed\n        elif direction == 'left':\n            twist_msg.angular.z = 0.5  # Left turn\n        elif direction == 'right':\n            twist_msg.angular.z = -0.5  # Right turn\n\n        self.velocity_pub.publish(twist_msg)\n        self.get_logger().info(f'Executing navigation command: {direction}')\n\n    def execute_stop_command(self):\n        \"\"\"Stop all movement\"\"\"\n        twist_msg = Twist()\n        self.velocity_pub.publish(twist_msg)\n        self.get_logger().info('Stopping robot')\n\n    def execute_arm_command(self, action):\n        \"\"\"Execute arm manipulation commands\"\"\"\n        # This would typically call a service or publish to joint controllers\n        self.get_logger().info(f'Executing arm command: {action}')\n\n    def execute_dance_command(self):\n        \"\"\"Execute dance sequence\"\"\"\n        self.get_logger().info('Executing dance sequence')\n        # Complex sequence of movements\n        # This would involve multiple joint controllers and timing\n\n    def destroy_node(self):\n        \"\"\"Clean up resources\"\"\"\n        self.recording = False\n        if self.audio_thread and self.audio_thread.is_alive():\n            self.audio_thread.join(timeout=1.0)\n        super().destroy_node()\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceRecognitionNode()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info('Shutting down Voice Recognition Node')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"step-3-create-the-voice-command-parser",children:"Step 3: Create the Voice Command Parser"}),"\n",(0,a.jsx)(n.p,{children:"Now let's create a sophisticated parser that can handle complex voice commands and map them to ROS 2 actions:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# voice_to_action/voice_to_action/nlp/command_parser.py\nimport re\nfrom typing import Dict, List, Tuple, Optional\nfrom dataclasses import dataclass\nfrom enum import Enum\n\nclass CommandType(Enum):\n    NAVIGATION = \"navigation\"\n    MANIPULATION = \"manipulation\"\n    INTERACTION = \"interaction\"\n    SYSTEM = \"system\"\n\n@dataclass\nclass ParsedCommand:\n    command_type: CommandType\n    action: str\n    parameters: Dict[str, any]\n    confidence: float\n    raw_text: str\n\nclass VoiceCommandParser:\n    def __init__(self):\n        # Navigation commands\n        self.nav_patterns = {\n            r'(?:move|go|drive|navigate|walk)\\s+(?:forward|ahead)': ('move', {'direction': 'forward'}),\n            r'(?:move|go|drive|navigate|walk)\\s+(?:backward|back|reverse)': ('move', {'direction': 'backward'}),\n            r'(?:turn|rotate)\\s+(?:left|anti-clockwise)': ('turn', {'direction': 'left'}),\n            r'(?:turn|rotate)\\s+(?:right|clockwise)': ('turn', {'direction': 'right'}),\n            r'(?:move|step)\\s+(left|right)': ('strafe', {'direction': '{0}'}),\n            r'stop': ('stop', {}),\n            r'go to (?:position|location|waypoint)\\s+(.+?)\\s*$': ('navigate_to', {'target': '{0}'}),\n            r'follow (me|person|leader)': ('follow', {'target': '{0}'}),\n        }\n\n        # Manipulation commands\n        self.manip_patterns = {\n            r'(?:pick up|grasp|grab|take)\\s+(.+?)\\s*$': ('pick_up', {'object': '{0}'}),\n            r'(?:put down|place|release|drop)\\s+(.+?)\\s*$': ('place', {'object': '{0}'}),\n            r'(?:open|close)\\s+(door|jar|box)': ('manipulate', {'object': '{0}', 'action': '{0}'}),\n            r'(?:wave|raise|lift)\\s+(?:your)?\\s*(?:left|right|both)?\\s*(?:arm|hand)': ('gesture', {'gesture': 'wave'}),\n            r'(?:point|show|indicate)\\s+(?:to)?\\s*(.+?)\\s*$': ('point_to', {'target': '{0}'}),\n        }\n\n        # Interaction commands\n        self.interaction_patterns = {\n            r'(?:say|speak|tell)\\s+(.+?)\\s*$': ('speak', {'text': '{0}'}),\n            r'(?:hello|hi|greet)\\s+(?:everyone|people|humans?)': ('greet', {'target': 'everyone'}),\n            r'(?:introduce|present)\\s+yourself': ('introduce', {}),\n            r'(?:dance|perform|show)\\s+dance': ('dance', {}),\n        }\n\n        # System commands\n        self.system_patterns = {\n            r'(?:shutdown|power off|turn off)\\s+system': ('shutdown', {}),\n            r'(?:reboot|restart)\\s+system': ('reboot', {}),\n            r'(?:sleep|standby|rest)': ('sleep', {}),\n            r'(?:wake up|activate|start)': ('wakeup', {}),\n        }\n\n        self.all_patterns = {\n            CommandType.NAVIGATION: self.nav_patterns,\n            CommandType.MANIPULATION: self.manipulation_patterns,\n            CommandType.INTERACTION: self.interaction_patterns,\n            CommandType.SYSTEM: self.system_patterns,\n        }\n\n    def parse_command(self, text: str) -> Optional[ParsedCommand]:\n        \"\"\"Parse voice command and return structured command\"\"\"\n        text = text.strip().lower()\n\n        # Try each command type\n        for cmd_type, patterns in self.all_patterns.items():\n            for pattern, (action, params_template) in patterns.items():\n                match = re.search(pattern, text)\n                if match:\n                    # Extract groups and fill template\n                    groups = match.groups()\n                    params = params_template.copy()\n\n                    # Replace placeholders in parameters\n                    for key, value in params.items():\n                        if isinstance(value, str) and '{0}' in value:\n                            if groups:\n                                params[key] = value.format(*groups)\n                        elif isinstance(value, str) and '{' in value and '}' in value:\n                            # Handle numbered placeholders like {1}, {2}, etc.\n                            for i, group in enumerate(groups):\n                                placeholder = f'{{{i}}}'\n                                if placeholder in value:\n                                    params[key] = value.replace(placeholder, group)\n\n                    # Calculate confidence based on match length\n                    confidence = min(len(match.group()) / len(text), 1.0)\n\n                    return ParsedCommand(\n                        command_type=cmd_type,\n                        action=action,\n                        parameters=params,\n                        confidence=confidence,\n                        raw_text=text\n                    )\n\n        return None\n\n    def get_suggested_commands(self) -> List[str]:\n        \"\"\"Return list of supported voice commands for user guidance\"\"\"\n        suggestions = []\n\n        for cmd_type, patterns in self.all_patterns.items():\n            for pattern in patterns.keys():\n                # Convert regex patterns to user-friendly examples\n                example = self._regex_to_example(pattern)\n                suggestions.append(f\"{cmd_type.value}: {example}\")\n\n        return suggestions\n\n    def _regex_to_example(self, pattern: str) -> str:\n        \"\"\"Convert regex pattern to example command\"\"\"\n        # Simplify regex for user presentation\n        example = pattern.replace(r'(?:', '').replace(r')', '')\n        example = example.replace(r'\\s+', ' ').replace(r'\\s*', ' ')\n        example = example.replace('|', ' or ')\n        example = example.replace(r'.+?', '[object]')\n        example = example.replace(r'.+?$', '[object]')\n        return example.strip()\n"})}),"\n",(0,a.jsx)(n.h3,{id:"step-4-create-the-voice-action-executor",children:"Step 4: Create the Voice Action Executor"}),"\n",(0,a.jsx)(n.p,{children:"Now let's create a node that executes the parsed commands by calling appropriate ROS 2 services and publishing to topics:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# voice_to_action/voice_to_action/action_execution_node.py\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom action_msgs.msg import GoalStatus\nfrom voice_to_action.nlp.command_parser import VoiceCommandParser, ParsedCommand, CommandType\nimport math\n\nclass VoiceActionExecutor(Node):\n    def __init__(self):\n        super().__init__(\'voice_action_executor\')\n\n        # Subscribe to parsed voice commands\n        self.command_sub = self.create_subscription(\n            String,\n            \'parsed_voice_command\',\n            self.command_callback,\n            10\n        )\n\n        # Publishers for different action types\n        self.cmd_vel_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\n        self.speech_pub = self.create_publisher(String, \'tts_request\', 10)\n\n        # Service clients for complex actions\n        self.nav_client = self.create_client(NavigateToPose, \'/navigate_to_pose\')\n        self.arm_client = self.create_client(MoveArm, \'/move_arm\')\n\n        # Initialize command parser\n        self.parser = VoiceCommandParser()\n\n        # Store robot state\n        self.current_pose = None\n        self.is_moving = False\n\n        self.get_logger().info(\'Voice Action Executor initialized\')\n\n    def command_callback(self, msg):\n        """Process incoming voice command"""\n        command_text = msg.data\n\n        # Parse the command\n        parsed_cmd = self.parser.parse_command(command_text)\n\n        if parsed_cmd and parsed_cmd.confidence > 0.5:  # Confidence threshold\n            self.get_logger().info(f\'Executing command: {parsed_cmd.action} with confidence {parsed_cmd.confidence:.2f}\')\n            self.execute_command(parsed_cmd)\n        else:\n            # Unknown command - respond appropriately\n            self.respond_unknown_command(command_text)\n\n    def execute_command(self, parsed_cmd: ParsedCommand):\n        """Execute the parsed command based on type"""\n        if parsed_cmd.command_type == CommandType.NAVIGATION:\n            self.execute_navigation_command(parsed_cmd)\n        elif parsed_cmd.command_type == CommandType.MANIPULATION:\n            self.execute_manipulation_command(parsed_cmd)\n        elif parsed_cmd.command_type == CommandType.INTERACTION:\n            self.execute_interaction_command(parsed_cmd)\n        elif parsed_cmd.command_type == CommandType.SYSTEM:\n            self.execute_system_command(parsed_cmd)\n\n    def execute_navigation_command(self, cmd: ParsedCommand):\n        """Execute navigation commands"""\n        if cmd.action == \'move\':\n            self.move_robot(cmd.parameters[\'direction\'])\n        elif cmd.action == \'turn\':\n            self.turn_robot(cmd.parameters[\'direction\'])\n        elif cmd.action == \'strafe\':\n            self.strafe_robot(cmd.parameters[\'direction\'])\n        elif cmd.action == \'stop\':\n            self.stop_robot()\n        elif cmd.action == \'navigate_to\':\n            self.navigate_to_location(cmd.parameters[\'target\'])\n        elif cmd.action == \'follow\':\n            self.follow_target(cmd.parameters[\'target\'])\n\n    def execute_manipulation_command(self, cmd: ParsedCommand):\n        """Execute manipulation commands"""\n        if cmd.action == \'pick_up\':\n            self.pick_up_object(cmd.parameters[\'object\'])\n        elif cmd.action == \'place\':\n            self.place_object(cmd.parameters[\'object\'])\n        elif cmd.action == \'gesture\':\n            self.perform_gesture(cmd.parameters[\'gesture\'])\n\n    def execute_interaction_command(self, cmd: ParsedCommand):\n        """Execute interaction commands"""\n        if cmd.action == \'speak\':\n            self.speak_text(cmd.parameters[\'text\'])\n        elif cmd.action == \'greet\':\n            self.greet_people(cmd.parameters.get(\'target\', \'everyone\'))\n\n    def execute_system_command(self, cmd: ParsedCommand):\n        """Execute system commands"""\n        if cmd.action == \'shutdown\':\n            self.shutdown_system()\n        elif cmd.action == \'sleep\':\n            self.enter_sleep_mode()\n\n    def move_robot(self, direction: str):\n        """Move robot in specified direction"""\n        twist = Twist()\n\n        if direction == \'forward\':\n            twist.linear.x = 0.5\n        elif direction == \'backward\':\n            twist.linear.x = -0.5\n        elif direction == \'left\':\n            twist.linear.y = 0.5\n        elif direction == \'right\':\n            twist.linear.y = -0.5\n\n        self.cmd_vel_pub.publish(twist)\n        self.is_moving = True\n\n    def turn_robot(self, direction: str):\n        """Turn robot in specified direction"""\n        twist = Twist()\n\n        if direction == \'left\':\n            twist.angular.z = 0.5\n        elif direction == \'right\':\n            twist.angular.z = -0.5\n\n        self.cmd_vel_pub.publish(twist)\n        self.is_moving = True\n\n    def strafe_robot(self, direction: str):\n        """Strafe robot sideways"""\n        twist = Twist()\n\n        if direction == \'left\':\n            twist.linear.y = 0.3\n        elif direction == \'right\':\n            twist.linear.y = -0.3\n\n        self.cmd_vel_pub.publish(twist)\n\n    def stop_robot(self):\n        """Stop all robot movement"""\n        twist = Twist()\n        self.cmd_vel_pub.publish(twist)\n        self.is_moving = False\n\n    def navigate_to_location(self, location: str):\n        """Navigate to a specific location using navigation stack"""\n        # This would typically use Nav2 or similar navigation system\n        self.get_logger().info(f\'Navigating to: {location}\')\n\n        # Example: Send navigation goal\n        goal_msg = NavigateToPose.Goal()\n        # Set goal pose based on location name\n        # This would require a location database or semantic map\n        self.nav_client.send_goal(goal_msg)\n\n    def speak_text(self, text: str):\n        """Publish text-to-speech request"""\n        tts_msg = String()\n        tts_msg.data = text\n        self.speech_pub.publish(tts_msg)\n\n    def respond_unknown_command(self, command_text: str):\n        """Respond when command is not understood"""\n        response = f"I\'m sorry, I didn\'t understand the command: {command_text}. Could you please repeat that?"\n        self.speak_text(response)\n\n        # Provide command suggestions\n        suggestions = self.parser.get_suggested_commands()\n        if suggestions:\n            help_text = "You can say commands like: " + ", ".join(suggestions[:3])  # Limit to 3 suggestions\n            self.speak_text(help_text)\n\n    def pick_up_object(self, object_name: str):\n        """Pick up specified object"""\n        self.get_logger().info(f\'Attempting to pick up: {object_name}\')\n        # Call manipulation service here\n\n    def place_object(self, object_name: str):\n        """Place object"""\n        self.get_logger.info(f\'Attempting to place: {object_name}\')\n        # Call manipulation service here\n\n    def perform_gesture(self, gesture_name: str):\n        """Perform specified gesture"""\n        self.get_logger().info(f\'Performing gesture: {gesture_name}\')\n        # Call gesture execution service here\n\n    def greet_people(self, target: str):\n        """Greet people"""\n        greeting = f"Hello {target}! Nice to meet you."\n        self.speak_text(greeting)\n\n    def shutdown_system(self):\n        """Shutdown the system"""\n        self.get_logger().info(\'Shutting down system...\')\n        # Perform graceful shutdown procedures\n\n    def enter_sleep_mode(self):\n        """Enter sleep/standby mode"""\n        self.stop_robot()\n        self.get_logger().info(\'Entering sleep mode\')\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VoiceActionExecutor()\n\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        node.get_logger().info(\'Shutting down Voice Action Executor\')\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(n.h3,{id:"step-5-create-the-main-voice-control-launch-file",children:"Step 5: Create the Main Voice Control Launch File"}),"\n",(0,a.jsx)(n.p,{children:"Finally, let's create a launch file that brings together all the voice control components:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# voice_to_action/launch/voice_control.launch.py\nfrom launch import LaunchDescription\nfrom launch_ros.actions import Node\nfrom launch.actions import DeclareLaunchArgument\nfrom launch.substitutions import LaunchConfiguration\n\ndef generate_launch_description():\n    return LaunchDescription([\n        # Declare launch arguments\n        DeclareLaunchArgument(\n            'sample_rate',\n            default_value='16000',\n            description='Audio sample rate for voice recognition'\n        ),\n\n        # Voice recognition node\n        Node(\n            package='voice_to_action',\n            executable='voice_recognition_node',\n            name='voice_recognition',\n            parameters=[\n                {'sample_rate': LaunchConfiguration('sample_rate')}\n            ],\n            output='screen'\n        ),\n\n        # Voice action executor node\n        Node(\n            package='voice_to_action',\n            executable='voice_action_executor',\n            name='voice_action_executor',\n            output='screen'\n        ),\n\n        # Text-to-speech node (if available)\n        Node(\n            package='tts_package',  # Replace with actual TTS package\n            executable='tts_node',\n            name='text_to_speech',\n            output='screen'\n        )\n    ])\n"})}),"\n",(0,a.jsx)(n.h3,{id:"step-6-test-the-voice-control-system",children:"Step 6: Test the Voice Control System"}),"\n",(0,a.jsx)(n.p,{children:"Let's create a simple test script to verify our voice control system works:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:"# voice_to_action/test/test_voice_control.py\n#!/usr/bin/env python3\n\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nimport time\n\nclass VoiceControlTester(Node):\n    def __init__(self):\n        super().__init__('voice_control_tester')\n\n        # Publisher for simulated voice commands\n        self.voice_pub = self.create_publisher(String, 'voice_command', 10)\n\n        # Subscriber to verify actions\n        self.cmd_vel_sub = self.create_subscription(\n            Twist,\n            'cmd_vel',\n            self.cmd_vel_callback,\n            10\n        )\n\n        self.last_velocity = Twist()\n        self.test_passed = True\n\n        self.get_logger().info('Voice Control Tester initialized')\n\n    def cmd_vel_callback(self, msg):\n        \"\"\"Store received velocity commands\"\"\"\n        self.last_velocity = msg\n\n    def run_tests(self):\n        \"\"\"Run comprehensive tests of voice control system\"\"\"\n        self.get_logger().info('Starting voice control tests...')\n\n        # Test 1: Move forward command\n        self.get_logger().info('Test 1: Sending \"move forward\" command')\n        cmd_msg = String()\n        cmd_msg.data = 'move forward'\n        self.voice_pub.publish(cmd_msg)\n        time.sleep(2.0)\n\n        if self.last_velocity.linear.x > 0:\n            self.get_logger().info('\u2713 Move forward test passed')\n        else:\n            self.get_logger().warn('\u2717 Move forward test failed')\n            self.test_passed = False\n\n        # Test 2: Turn left command\n        self.get_logger.info('Test 2: Sending \"turn left\" command')\n        cmd_msg.data = 'turn left'\n        self.voice_pub.publish(cmd_msg)\n        time.sleep(2.0)\n\n        if self.last_velocity.angular.z > 0:\n            self.get_logger.info('\u2713 Turn left test passed')\n        else:\n            self.get_logger().warn('\u2717 Turn left test failed')\n            self.test_passed = False\n\n        # Test 3: Stop command\n        self.get_logger.info('Test 3: Sending \"stop\" command')\n        cmd_msg.data = 'stop'\n        self.voice_pub.publish(cmd_msg)\n        time.sleep(1.0)\n\n        if abs(self.last_velocity.linear.x) < 0.01 and abs(self.last_velocity.angular.z) < 0.01:\n            self.get_logger.info('\u2713 Stop test passed')\n        else:\n            self.get_logger().warn('\u2717 Stop test failed')\n            self.test_passed = False\n\n        # Final result\n        if self.test_passed:\n            self.get_logger().info('\ud83c\udf89 All voice control tests passed!')\n        else:\n            self.get_logger().error('\u274c Some tests failed')\n\n    def destroy_node(self):\n        \"\"\"Clean up resources\"\"\"\n        super().destroy_node()\n\n\ndef main(args=None):\n    rclpy.init(args=args)\n    tester = VoiceControlTester()\n\n    # Schedule tests after a brief delay to allow system to initialize\n    timer = tester.create_timer(3.0, lambda: tester.run_tests())\n\n    try:\n        rclpy.spin(tester)\n    except KeyboardInterrupt:\n        tester.get_logger().info('Test interrupted')\n    finally:\n        tester.destroy_node()\n        rclpy.shutdown()\n\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,a.jsx)(n.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,a.jsx)(n.p,{children:"Here's a complete example of how to use the voice-to-action system in practice:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# Example: Integrating voice control with humanoid robot navigation\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nfrom voice_to_action.nlp.command_parser import VoiceCommandParser\n\nclass HumanoidVoiceController(Node):\n    def __init__(self):\n        super().__init__(\'humanoid_voice_controller\')\n\n        # Initialize voice command parser\n        self.parser = VoiceCommandParser()\n\n        # Publishers for humanoid-specific actions\n        self.body_cmd_pub = self.create_publisher(String, \'humanoid_body_commands\', 10)\n        self.arm_cmd_pub = self.create_publisher(String, \'humanoid_arm_commands\', 10)\n\n        # Example voice command processing\n        self.demo_voice_commands()\n\n    def demo_voice_commands(self):\n        """Demonstrate voice command processing for humanoid robot"""\n        commands = [\n            "move forward slowly",\n            "turn left 90 degrees",\n            "raise both arms",\n            "wave hello",\n            "balance on one foot",\n            "sit down",\n            "stand up"\n        ]\n\n        for cmd_text in commands:\n            parsed_cmd = self.parser.parse_command(cmd_text)\n            if parsed_cmd:\n                self.get_logger().info(f\'Parsed: {parsed_cmd.action} - {parsed_cmd.parameters}\')\n                self.execute_humanoid_command(parsed_cmd)\n\n    def execute_humanoid_command(self, cmd):\n        """Execute command for humanoid robot"""\n        if cmd.command_type == \'navigation\':\n            # Handle navigation for bipedal locomotion\n            body_cmd = String()\n            body_cmd.data = f"locomotion_{cmd.action}"\n            self.body_cmd_pub.publish(body_cmd)\n        elif cmd.action == \'gesture\':\n            # Handle arm gestures\n            arm_cmd = String()\n            arm_cmd.data = f"gesture_{cmd.parameters[\'gesture\']}"\n            self.arm_cmd_pub.publish(arm_cmd)\n\ndef main():\n    rclpy.init()\n    controller = HumanoidVoiceController()\n\n    try:\n        rclpy.spin(controller)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        controller.destroy_node()\n        rclpy.shutdown()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"small-simulation",children:"Small Simulation"}),"\n",(0,a.jsx)(n.p,{children:"Let's create a simple simulation to test our voice-to-action system:"}),"\n",(0,a.jsx)(n.pre,{children:(0,a.jsx)(n.code,{className:"language-python",children:'# voice_to_action/simulations/voice_simulation.py\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\nimport threading\nimport time\n\nclass VoiceToActionSimulation:\n    def __init__(self):\n        self.robot_x = 0.0\n        self.robot_y = 0.0\n        self.robot_theta = 0.0  # Heading angle in radians\n        self.is_moving = False\n        self.command_history = []\n\n        # Setup plot\n        self.fig, self.ax = plt.subplots(figsize=(10, 8))\n        self.robot_plot = self.ax.plot([], [], \'ro\', markersize=15, label=\'Robot\')[0]\n        self.path_plot = self.ax.plot([], [], \'b-\', alpha=0.7, label=\'Path\')[0]\n\n        # Set up the environment\n        self.ax.set_xlim(-10, 10)\n        self.ax.set_ylim(-10, 10)\n        self.ax.set_xlabel(\'X Position (m)\')\n        self.ax.set_ylabel(\'Y Position (m)\')\n        self.ax.set_title(\'Voice-to-Action Robot Simulation\')\n        self.ax.grid(True)\n        self.ax.legend()\n\n        # Robot path storage\n        self.path_x = [0]\n        self.path_y = [0]\n\n    def process_voice_command(self, command_text):\n        """Process voice command and update robot state"""\n        command_text = command_text.lower()\n\n        if \'forward\' in command_text:\n            self.move_forward()\n            self.command_history.append((\'forward\', time.time()))\n        elif \'backward\' in command_text:\n            self.move_backward()\n            self.command_history.append((\'backward\', time.time()))\n        elif \'left\' in command_text and \'turn\' in command_text:\n            self.turn_left()\n            self.command_history.append((\'turn_left\', time.time()))\n        elif \'right\' in command_text and \'turn\' in command_text:\n            self.turn_right()\n            self.command_history.append((\'turn_right\', time.time()))\n        elif \'stop\' in command_text:\n            self.stop_robot()\n            self.command_history.append((\'stop\', time.time()))\n\n    def move_forward(self):\n        """Move robot forward"""\n        self.robot_x += 0.1 * np.cos(self.robot_theta)\n        self.robot_y += 0.1 * np.sin(self.robot_theta)\n        self.update_path()\n\n    def move_backward(self):\n        """Move robot backward"""\n        self.robot_x -= 0.1 * np.cos(self.robot_theta)\n        self.robot_y -= 0.1 * np.sin(self.robot_theta)\n        self.update_path()\n\n    def turn_left(self):\n        """Turn robot left"""\n        self.robot_theta += np.pi / 8  # 22.5 degrees\n\n    def turn_right(self):\n        """Turn robot right"""\n        self.robot_theta -= np.pi / 8  # 22.5 degrees\n\n    def stop_robot(self):\n        """Stop robot movement"""\n        pass  # Just stop the movement\n\n    def update_path(self):\n        """Update the path history"""\n        self.path_x.append(self.robot_x)\n        self.path_y.append(self.robot_y)\n\n    def animate(self, frame):\n        """Animation function for the plot"""\n        # Simulate some voice commands periodically\n        if frame % 50 == 0:  # Every 50 frames, send a command\n            commands = [\'move forward\', \'turn left\', \'move forward\', \'turn right\']\n            cmd = commands[(frame // 50) % len(commands)]\n            self.process_voice_command(cmd)\n            print(f"Voice command: {cmd}")\n\n        # Update robot position on plot\n        self.robot_plot.set_data([self.robot_x], [self.robot_y])\n        self.path_plot.set_data(self.path_x, self.path_y)\n\n        # Add robot orientation indicator\n        arrow_length = 0.5\n        arrow_dx = arrow_length * np.cos(self.robot_theta)\n        arrow_dy = arrow_length * np.sin(self.robot_theta)\n        self.ax.arrow(self.robot_x, self.robot_y, arrow_dx, arrow_dy,\n                     head_width=0.1, head_length=0.1, fc=\'red\', ec=\'red\')\n\n        return self.robot_plot, self.path_plot\n\n    def run_simulation(self):\n        """Run the voice-to-action simulation"""\n        ani = FuncAnimation(self.fig, self.animate, frames=500, interval=100, blit=True)\n        plt.show()\n\nif __name__ == "__main__":\n    sim = VoiceToActionSimulation()\n    sim.run_simulation()\n'})}),"\n",(0,a.jsx)(n.h2,{id:"quick-recap",children:"Quick Recap"}),"\n",(0,a.jsx)(n.p,{children:"In this lesson, we've implemented a comprehensive voice-to-action system using OpenAI's Whisper model for speech recognition. We created:"}),"\n",(0,a.jsxs)(n.ol,{children:["\n",(0,a.jsx)(n.li,{children:"A voice recognition node that captures audio and processes it with Whisper"}),"\n",(0,a.jsx)(n.li,{children:"A sophisticated command parser that maps natural language to ROS 2 actions"}),"\n",(0,a.jsx)(n.li,{children:"An action executor that translates voice commands into robot behaviors"}),"\n",(0,a.jsx)(n.li,{children:"Integration with humanoid robot control systems"}),"\n",(0,a.jsx)(n.li,{children:"A simulation environment to test the voice control system"}),"\n"]}),"\n",(0,a.jsx)(n.p,{children:"The system handles various command types including navigation, manipulation, interaction, and system commands. It's designed to be robust and extensible, allowing for easy addition of new voice commands and robotic capabilities."}),"\n",(0,a.jsx)(n.p,{children:"The next lesson will cover cognitive planning with LLMs mapped to ROS 2 actions, building on this voice-to-action foundation."})]})}function d(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(m,{...e})}):m(e)}},8453(e,n,t){t.d(n,{R:()=>s,x:()=>r});var o=t(6540);const a={},i=o.createContext(a);function s(e){const n=o.useContext(i);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);