"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[983],{6926(e,n,s){s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>c,frontMatter:()=>o,metadata:()=>a,toc:()=>_});const a=JSON.parse('{"id":"physical-ai/the-digital-twin-gazebo-unity/sensor-simulation","title":"Sensor Simulation","description":"Overview","source":"@site/docs/physical-ai/the-digital-twin-gazebo-unity/sensor-simulation.md","sourceDirName":"physical-ai/the-digital-twin-gazebo-unity","slug":"/physical-ai/the-digital-twin-gazebo-unity/sensor-simulation","permalink":"/AI/docs/physical-ai/the-digital-twin-gazebo-unity/sensor-simulation","draft":false,"unlisted":false,"editUrl":"https://github.com/arijh/physical-ai-book/tree/main/packages/create-docusaurus/templates/shared/docs/physical-ai/the-digital-twin-gazebo-unity/sensor-simulation.md","tags":[],"version":"current","frontMatter":{"sidebar_label":"Sensor Simulation","title":"Sensor Simulation"},"sidebar":"docs","previous":{"title":"Unity-Based Visualization","permalink":"/AI/docs/physical-ai/the-digital-twin-gazebo-unity/unity-based-visualization"},"next":{"title":"Introduction to NVIDIA Isaac Sim","permalink":"/AI/docs/physical-ai/the-ai-robot-brain-nvidia-isaac/introduction-to-nvidia-isaac-sim"}}');var i=s(4848),t=s(8453);const o={sidebar_label:"Sensor Simulation",title:"Sensor Simulation"},r="Sensor Simulation",l={},_=[{value:"Overview",id:"overview",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Hands-on Steps",id:"hands-on-steps",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Code Examples",id:"code-examples",level:2},{value:"Small Simulation",id:"small-simulation",level:2},{value:"Quick Recap",id:"quick-recap",level:2},{value:"Summary of Chapter 3",id:"summary-of-chapter-3",level:2}];function d(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"sensor-simulation",children:"Sensor Simulation"})}),"\n",(0,i.jsx)(n.h2,{id:"overview",children:"Overview"}),"\n",(0,i.jsx)(n.p,{children:"Sensor simulation is a critical component of digital twin technology, enabling robots to perceive their environment in virtual worlds just as they would with real sensors. This lesson covers the simulation of various sensor types including LIDAR, depth cameras, IMUs, and other perception systems. Accurate sensor simulation allows for realistic testing of perception algorithms, SLAM systems, and robot autonomy before deployment on physical hardware."}),"\n",(0,i.jsx)(n.p,{children:"Understanding how to properly configure and validate sensor models is essential for creating believable digital twins that can effectively prepare robots for real-world operation."}),"\n",(0,i.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Configure and simulate LIDAR sensors with realistic noise and limitations"}),"\n",(0,i.jsx)(n.li,{children:"Set up depth camera simulation with proper distortion and resolution"}),"\n",(0,i.jsx)(n.li,{children:"Model IMU sensors with realistic drift and noise characteristics"}),"\n",(0,i.jsx)(n.li,{children:"Simulate multiple sensor types for sensor fusion applications"}),"\n",(0,i.jsx)(n.li,{children:"Validate sensor data against expected real-world behavior"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"hands-on-steps",children:"Hands-on Steps"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LIDAR Simulation"}),": Configure realistic LIDAR models with noise parameters"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Depth Camera Setup"}),": Create depth camera sensors with proper distortion"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"IMU Modeling"}),": Implement realistic IMU sensors with drift characteristics"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multi-Sensor Integration"}),": Combine multiple sensors for fusion applications"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensor Validation"}),": Compare simulated vs. expected sensor behavior"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Understanding of ROS 2 sensor message types"}),"\n",(0,i.jsx)(n.li,{children:"Knowledge of sensor physics and characteristics"}),"\n",(0,i.jsx)(n.li,{children:"Experience with Gazebo and Unity integration"}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,i.jsx)(n.p,{children:"Let's start by creating a comprehensive robot model with various sensors:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'\x3c!-- sensor_robot.urdf --\x3e\n<?xml version="1.0"?>\n<robot name="sensor_robot">\n  \x3c!-- Materials --\x3e\n  <material name="blue">\n    <color rgba="0.0 0.0 0.8 1.0"/>\n  </material>\n  <material name="green">\n    <color rgba="0.0 0.8 0.0 1.0"/>\n  </material>\n  <material name="red">\n    <color rgba="0.8 0.0 0.0 1.0"/>\n  </material>\n  <material name="white">\n    <color rgba="1.0 1.0 1.0 1.0"/>\n  </material>\n  <material name="black">\n    <color rgba="0.1 0.1 0.1 1.0"/>\n  </material>\n\n  \x3c!-- Base Link --\x3e\n  <link name="base_link">\n    <visual>\n      <origin xyz="0 0 0.15" rpy="0 0 0"/>\n      <geometry>\n        <box size="0.5 0.4 0.3"/>\n      </geometry>\n      <material name="white"/>\n    </visual>\n    <collision>\n      <origin xyz="0 0 0.15" rpy="0 0 0"/>\n      <geometry>\n        <box size="0.5 0.4 0.3"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="15.0"/>\n      <origin xyz="0 0 0.15" rpy="0 0 0"/>\n      <inertia ixx="0.21875" ixy="0.0" ixz="0.0"\n               iyy="0.31458" iyz="0.0"\n               izz="0.40625"/>\n    </inertial>\n  </link>\n\n  \x3c!-- LIDAR Mount --\x3e\n  <link name="lidar_mount">\n    <visual>\n      <geometry>\n        <cylinder length="0.05" radius="0.05"/>\n      </geometry>\n      <material name="black"/>\n    </visual>\n    <collision>\n      <geometry>\n        <cylinder length="0.05" radius="0.05"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="0.2"/>\n      <inertia ixx="0.00015" ixy="0.0" ixz="0.0"\n               iyy="0.00015" iyz="0.0"\n               izz="0.0001"/>\n    </inertial>\n  </link>\n\n  <joint name="lidar_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="lidar_mount"/>\n    <origin xyz="0.2 0 0.3" rpy="0 0 0"/>\n  </joint>\n\n  \x3c!-- Camera Mount --\x3e\n  <link name="camera_mount">\n    <visual>\n      <geometry>\n        <box size="0.05 0.05 0.05"/>\n      </geometry>\n      <material name="black"/>\n    </visual>\n    <collision>\n      <geometry>\n        <box size="0.05 0.05 0.05"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="0.1"/>\n      <inertia ixx="0.000025" ixy="0.0" ixz="0.0"\n               iyy="0.000025" iyz="0.0"\n               izz="0.000025"/>\n    </inertial>\n  </link>\n\n  <joint name="camera_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="camera_mount"/>\n    <origin xyz="0.22 0 0.25" rpy="0 0 0"/>\n  </joint>\n\n  \x3c!-- IMU Mount --\x3e\n  <link name="imu_mount">\n    <visual>\n      <geometry>\n        <box size="0.02 0.02 0.02"/>\n      </geometry>\n      <material name="red"/>\n    </visual>\n    <collision>\n      <geometry>\n        <box size="0.02 0.02 0.02"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="0.05"/>\n      <inertia ixx="0.000001" ixy="0.0" ixz="0.0"\n               iyy="0.000001" iyz="0.0"\n               izz="0.000001"/>\n    </inertial>\n  </link>\n\n  <joint name="imu_joint" type="fixed">\n    <parent link="base_link"/>\n    <child link="imu_mount"/>\n    <origin xyz="0 0 0.1" rpy="0 0 0"/>\n  </joint>\n\n  \x3c!-- Wheel links --\x3e\n  <link name="left_wheel">\n    <visual>\n      <origin xyz="0 0 0" rpy="1.5708 0 0"/>\n      <geometry>\n        <cylinder length="0.08" radius="0.15"/>\n      </geometry>\n      <material name="blue"/>\n    </visual>\n    <collision>\n      <origin xyz="0 0 0" rpy="1.5708 0 0"/>\n      <geometry>\n        <cylinder length="0.08" radius="0.15"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="1.0"/>\n      <inertia ixx="0.008125" ixy="0.0" ixz="0.0"\n               iyy="0.008125" iyz="0.0"\n               izz="0.01125"/>\n    </inertial>\n  </link>\n\n  <joint name="left_wheel_joint" type="continuous">\n    <parent link="base_link"/>\n    <child link="left_wheel"/>\n    <origin xyz="0 0.25 0.15" rpy="0 0 0"/>\n    <axis xyz="0 0 1"/>\n  </joint>\n\n  <link name="right_wheel">\n    <visual>\n      <origin xyz="0 0 0" rpy="1.5708 0 0"/>\n      <geometry>\n        <cylinder length="0.08" radius="0.15"/>\n      </geometry>\n      <material name="blue"/>\n    </visual>\n    <collision>\n      <origin xyz="0 0 0" rpy="1.5708 0 0"/>\n      <geometry>\n        <cylinder length="0.08" radius="0.15"/>\n      </geometry>\n    </collision>\n    <inertial>\n      <mass value="1.0"/>\n      <inertia ixx="0.008125" ixy="0.0" ixz="0.0"\n               iyy="0.008125" iyz="0.0"\n               izz="0.01125"/>\n    </inertial>\n  </link>\n\n  <joint name="right_wheel_joint" type="continuous">\n    <parent link="base_link"/>\n    <child link="right_wheel"/>\n    <origin xyz="0 -0.25 0.15" rpy="0 0 0"/>\n    <axis xyz="0 0 1"/>\n  </joint>\n\n  \x3c!-- Gazebo-specific sensor plugins --\x3e\n\n  \x3c!-- LIDAR Sensor (Hokuyo-like) --\x3e\n  <gazebo reference="lidar_mount">\n    <sensor name="laser" type="ray">\n      <pose>0 0 0 0 0 0</pose>\n      <visualize>true</visualize>\n      <update_rate>10</update_rate>\n      <ray>\n        <scan>\n          <horizontal>\n            <samples>720</samples>\n            <resolution>1</resolution>\n            <min_angle>-2.35619</min_angle>  \x3c!-- -135 degrees --\x3e\n            <max_angle>2.35619</max_angle>   \x3c!-- 135 degrees --\x3e\n          </horizontal>\n        </scan>\n        <range>\n          <min>0.1</min>\n          <max>30.0</max>\n          <resolution>0.01</resolution>\n        </range>\n      </ray>\n      <plugin name="laser_controller" filename="libgazebo_ros_ray_sensor.so">\n        <ros>\n          <namespace>/sensor_robot</namespace>\n          <remapping>~/out:=scan</remapping>\n        </ros>\n        <output_type>sensor_msgs/LaserScan</output_type>\n        <frame_name>lidar_mount</frame_name>\n      </plugin>\n      \x3c!-- Add noise to make it more realistic --\x3e\n      <noise type="gaussian">\n        <mean>0.0</mean>\n        <stddev>0.01</stddev>\n      </noise>\n    </sensor>\n  </gazebo>\n\n  \x3c!-- Depth Camera --\x3e\n  <gazebo reference="camera_mount">\n    <sensor name="depth_camera" type="depth">\n      <pose>0 0 0 0 0 0</pose>\n      <visualize>true</visualize>\n      <update_rate>30</update_rate>\n      <camera name="head">\n        <horizontal_fov>1.047</horizontal_fov>  \x3c!-- 60 degrees --\x3e\n        <image>\n          <width>640</width>\n          <height>480</height>\n          <format>R8G8B8</format>\n        </image>\n        <clip>\n          <near>0.1</near>\n          <far>10</far>\n        </clip>\n        <noise>\n          <type>gaussian</type>\n          <mean>0.0</mean>\n          <stddev>0.007</stddev>\n        </noise>\n      </camera>\n      <plugin name="depth_camera_controller" filename="libgazebo_ros_openni_kinect.so">\n        <ros>\n          <namespace>/sensor_robot</namespace>\n          <remapping>rgb/image_raw:=camera/color/image_raw</remapping>\n          <remapping>depth/image_raw:=camera/depth/image_raw</remapping>\n          <remapping>depth/camera_info:=camera/depth/camera_info</remapping>\n        </ros>\n        <camera_name>camera</camera_name>\n        <frame_name>camera_mount</frame_name>\n        <baseline>0.1</baseline>\n        <distortion_k1>0.0</distortion_k1>\n        <distortion_k2>0.0</distortion_k2>\n        <distortion_k3>0.0</distortion_k3>\n        <distortion_t1>0.0</distortion_t1>\n        <distortion_t2>0.0</distortion_t2>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  \x3c!-- IMU Sensor --\x3e\n  <gazebo reference="imu_mount">\n    <sensor name="imu_sensor" type="imu">\n      <always_on>true</always_on>\n      <update_rate>100</update_rate>\n      <visualize>false</visualize>\n      <imu>\n        <angular_velocity>\n          <x>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>0.0017</stddev>  \x3c!-- ~0.1 deg/s (1 sigma) --\x3e\n              <bias_mean>0.0001</bias_mean>\n              <bias_stddev>0.00001</bias_stddev>\n            </noise>\n          </x>\n          <y>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>0.0017</stddev>\n              <bias_mean>0.0001</bias_mean>\n              <bias_stddev>0.00001</bias_stddev>\n            </noise>\n          </y>\n          <z>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>0.0017</stddev>\n              <bias_mean>0.0001</bias_mean>\n              <bias_stddev>0.00001</bias_stddev>\n            </noise>\n          </z>\n        </angular_velocity>\n        <linear_acceleration>\n          <x>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>1.7e-2</stddev>  \x3c!-- 17 mg (1 sigma) --\x3e\n              <bias_mean>0.01</bias_mean>\n              <bias_stddev>0.001</bias_stddev>\n            </noise>\n          </x>\n          <y>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>1.7e-2</stddev>\n              <bias_mean>0.01</bias_mean>\n              <bias_stddev>0.001</bias_stddev>\n            </noise>\n          </y>\n          <z>\n            <noise type="gaussian">\n              <mean>0.0</mean>\n              <stddev>1.7e-2</stddev>\n              <bias_mean>0.01</bias_mean>\n              <bias_stddev>0.001</bias_stddev>\n            </noise>\n          </z>\n        </linear_acceleration>\n      </imu>\n      <plugin name="imu_controller" filename="libgazebo_ros_imu.so">\n        <ros>\n          <namespace>/sensor_robot</namespace>\n          <remapping>imu:=imu</remapping>\n        </ros>\n        <frame_name>imu_mount</frame_name>\n        <body_name>imu_mount</body_name>\n        <update_rate>100</update_rate>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  \x3c!-- GPS Sensor --\x3e\n  <gazebo reference="base_link">\n    <sensor name="gps_sensor" type="gps">\n      <always_on>true</always_on>\n      <update_rate>10</update_rate>\n      <plugin name="gps_controller" filename="libgazebo_ros_gps.so">\n        <ros>\n          <namespace>/sensor_robot</namespace>\n          <remapping>fix:=gps/fix</remapping>\n        </ros>\n        <frame_name>base_link</frame_name>\n        <update_rate>10</update_rate>\n        <gaussian_noise>0.1</gaussian_noise>\n        <velocity_gaussian_noise>0.1</velocity_gaussian_noise>\n      </plugin>\n    </sensor>\n  </gazebo>\n\n  \x3c!-- Differential drive plugin --\x3e\n  <gazebo>\n    <plugin name="differential_drive" filename="libgazebo_ros_diff_drive.so">\n      <ros>\n        <namespace>/sensor_robot</namespace>\n        <remapping>cmd_vel:=cmd_vel</remapping>\n        <remapping>odom:=odom</remapping>\n      </ros>\n      <left_joint>left_wheel_joint</left_joint>\n      <right_joint>right_wheel_joint</right_joint>\n      <wheel_separation>0.5</wheel_separation>\n      <wheel_diameter>0.3</wheel_diameter>\n      <max_wheel_torque>20</max_wheel_torque>\n      <max_wheel_acceleration>1.0</max_wheel_acceleration>\n      <publish_odom>true</publish_odom>\n      <publish_odom_tf>true</publish_odom_tf>\n      <publish_wheel_tf>true</publish_wheel_tf>\n      <odometry_frame>odom</odometry_frame>\n      <robot_base_frame>base_link</robot_base_frame>\n    </plugin>\n  </gazebo>\n\n  \x3c!-- Joint state publisher --\x3e\n  <gazebo>\n    <plugin name="joint_state_publisher" filename="libgazebo_ros_joint_state_publisher.so">\n      <ros>\n        <namespace>/sensor_robot</namespace>\n        <remapping>joint_states:=joint_states</remapping>\n      </ros>\n      <update_rate>30</update_rate>\n      <joint_name>left_wheel_joint</joint_name>\n      <joint_name>right_wheel_joint</joint_name>\n    </plugin>\n  </gazebo>\n</robot>\n'})}),"\n",(0,i.jsx)(n.p,{children:"Now let's create a ROS 2 node for sensor data processing and validation:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# sensor_processor.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Image, Imu, NavSatFix\nfrom geometry_msgs.msg import Twist, Pose\nfrom nav_msgs.msg import Odometry\nfrom std_msgs.msg import String, Float32\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\nimport math\nfrom scipy.spatial.transform import Rotation as R\nimport statistics\n\nclass SensorProcessor(Node):\n    """\n    Process and validate data from multiple simulated sensors\n    """\n    def __init__(self):\n        super().__init__(\'sensor_processor\')\n\n        # Publishers\n        self.status_pub = self.create_publisher(String, \'/sensor_status\', 10)\n        self.laser_stats_pub = self.create_publisher(String, \'/laser_stats\', 10)\n        self.imu_stats_pub = self.create_publisher(String, \'/imu_stats\', 10)\n\n        # Subscribers\n        self.scan_sub = self.create_subscription(LaserScan, \'/sensor_robot/scan\', self.scan_callback, 10)\n        self.imu_sub = self.create_subscription(Imu, \'/sensor_robot/imu\', self.imu_callback, 10)\n        self.gps_sub = self.create_subscription(NavSatFix, \'/sensor_robot/gps/fix\', self.gps_callback, 10)\n        self.odom_sub = self.create_subscription(Odometry, \'/sensor_robot/odom\', self.odom_callback, 10)\n        self.image_sub = self.create_subscription(Image, \'/sensor_robot/camera/color/image_raw\', self.image_callback, 10)\n\n        # Timers\n        self.processing_timer = self.create_timer(1.0, self.process_sensor_data)\n\n        # Sensor data storage\n        self.scan_data = None\n        self.imu_data = None\n        self.gps_data = None\n        self.odom_data = None\n        self.image_data = None\n        self.cv_bridge = CvBridge()\n\n        # Statistics tracking\n        self.laser_ranges_history = []\n        self.imu_angular_velocity_history = []\n        self.imu_linear_acceleration_history = []\n        self.position_history = []\n\n        # Sensor validation parameters\n        self.laser_min_range = 0.1\n        self.laser_max_range = 30.0\n        self.imu_angular_velocity_limits = 10.0  # rad/s\n        self.imu_linear_acceleration_limits = 50.0  # m/s\xb2\n\n        self.get_logger().info("Sensor Processor initialized")\n\n    def scan_callback(self, msg):\n        """Process LIDAR scan data"""\n        self.scan_data = msg\n\n        # Store for statistics\n        valid_ranges = [r for r in msg.ranges if not math.isnan(r) and self.laser_min_range < r < self.laser_max_range]\n        if valid_ranges:\n            self.laser_ranges_history.extend(valid_ranges[-10:])  # Keep last 10 valid readings\n            if len(self.laser_ranges_history) > 100:\n                self.laser_ranges_history = self.laser_ranges_history[-100:]  # Limit history\n\n    def imu_callback(self, msg):\n        """Process IMU data"""\n        self.imu_data = msg\n\n        # Store for statistics\n        self.imu_angular_velocity_history.append([\n            msg.angular_velocity.x,\n            msg.angular_velocity.y,\n            msg.angular_velocity.z\n        ])\n        self.imu_linear_acceleration_history.append([\n            msg.linear_acceleration.x,\n            msg.linear_acceleration.y,\n            msg.linear_acceleration.z\n        ])\n\n        # Limit history size\n        if len(self.imu_angular_velocity_history) > 1000:\n            self.imu_angular_velocity_history = self.imu_angular_velocity_history[-1000:]\n        if len(self.imu_linear_acceleration_history) > 1000:\n            self.imu_linear_acceleration_history = self.imu_linear_acceleration_history[-1000:]\n\n    def gps_callback(self, msg):\n        """Process GPS data"""\n        self.gps_data = msg\n\n    def odom_callback(self, msg):\n        """Process odometry data"""\n        self.odom_data = msg\n\n        # Store position for drift analysis\n        pos = msg.pose.pose.position\n        self.position_history.append((pos.x, pos.y, pos.z))\n        if len(self.position_history) > 1000:\n            self.position_history = self.position_history[-1000:]\n\n    def image_callback(self, msg):\n        """Process camera image data"""\n        try:\n            self.image_data = self.cv_bridge.imgmsg_to_cv2(msg, "bgr8")\n        except Exception as e:\n            self.get_logger().error(f"Could not convert image: {e}")\n\n    def analyze_laser_data(self):\n        """Analyze LIDAR sensor data"""\n        if not self.scan_data or not self.laser_ranges_history:\n            return "No LIDAR data available"\n\n        # Basic statistics\n        avg_range = statistics.mean(self.laser_ranges_history) if self.laser_ranges_history else 0\n        min_range = min(self.laser_ranges_history) if self.laser_ranges_history else 0\n        max_range = max(self.laser_ranges_history) if self.laser_ranges_history else 0\n\n        # Check for valid range values\n        invalid_count = sum(1 for r in self.scan_data.ranges if math.isnan(r) or r <= 0)\n        total_readings = len(self.scan_data.ranges)\n        invalid_percentage = (invalid_count / total_readings) * 100 if total_readings > 0 else 0\n\n        # Analyze for obstacles\n        obstacle_distances = [r for r in self.scan_data.ranges if self.laser_min_range < r < 1.0]\n        obstacle_count = len(obstacle_distances)\n\n        analysis = f"LIDAR: Avg={avg_range:.2f}m, Min={min_range:.2f}m, Max={max_range:.2f}m, " \\\n                  f"Invalid={invalid_percentage:.1f}%, Obstacles={obstacle_count}"\n\n        # Publish detailed stats\n        stats_msg = String()\n        stats_msg.data = analysis\n        self.laser_stats_pub.publish(stats_msg)\n\n        return analysis\n\n    def analyze_imu_data(self):\n        """Analyze IMU sensor data"""\n        if not self.imu_data or not self.imu_angular_velocity_history:\n            return "No IMU data available"\n\n        # Calculate statistics for angular velocity\n        if self.imu_angular_velocity_history:\n            avg_ang_vel = np.mean(self.imu_angular_velocity_history, axis=0)\n            std_ang_vel = np.std(self.imu_angular_velocity_history, axis=0)\n            max_ang_vel = np.max(np.abs(self.imu_angular_velocity_history), axis=0)\n\n        # Calculate statistics for linear acceleration\n        if self.imu_linear_acceleration_history:\n            avg_lin_acc = np.mean(self.imu_linear_acceleration_history, axis=0)\n            std_lin_acc = np.std(self.imu_linear_acceleration_history, axis=0)\n            max_lin_acc = np.max(np.abs(self.imu_linear_acceleration_history), axis=0)\n\n        # Check for drift in linear acceleration (should average around gravity)\n        gravity_drift = abs(avg_lin_acc[2] + 9.81) if len(avg_lin_acc) > 2 else 0  # Z should be ~-9.81 for gravity\n\n        analysis = f"IMU: AngVel(Avg)[{avg_ang_vel[0]:.3f}, {avg_ang_vel[1]:.3f}, {avg_ang_vel[2]:.3f}], " \\\n                  f"LinAcc(Avg)[{avg_lin_acc[0]:.2f}, {avg_lin_acc[1]:.2f}, {avg_lin_acc[2]:.2f}], " \\\n                  f"GravityDrift: {gravity_drift:.2f}m/s\xb2"\n\n        # Publish detailed stats\n        stats_msg = String()\n        stats_msg.data = analysis\n        self.imu_stats_pub.publish(stats_msg)\n\n        return analysis\n\n    def analyze_sensor_fusion(self):\n        """Analyze data from multiple sensors together"""\n        fusion_analysis = []\n\n        # Check if robot is moving and if sensors agree\n        if self.odom_data and self.imu_data:\n            linear_speed = math.sqrt(\n                self.odom_data.twist.twist.linear.x**2 +\n                self.odom_data.twist.twist.linear.y**2\n            )\n\n            # IMU-based linear acceleration magnitude\n            imu_lin_acc_mag = math.sqrt(\n                self.imu_data.linear_acceleration.x**2 +\n                self.imu_data.linear_acceleration.y**2 +\n                self.imu_data.linear_acceleration.z**2\n            )\n\n            fusion_analysis.append(f"Motion: OdomSpeed={linear_speed:.3f}m/s, IMULinAcc={imu_lin_acc_mag:.3f}m/s\xb2")\n\n        # Check GPS consistency with odometry\n        if self.gps_data and self.odom_data and self.position_history:\n            if len(self.position_history) >= 2:\n                # Calculate recent movement from odometry\n                recent_pos = self.position_history[-1]\n                prev_pos = self.position_history[-2]\n                odom_displacement = math.sqrt(\n                    (recent_pos[0] - prev_pos[0])**2 +\n                    (recent_pos[1] - prev_pos[1])**2\n                )\n\n                fusion_analysis.append(f"Position: OdomDisplacement={odom_displacement:.3f}m")\n\n        return "; ".join(fusion_analysis) if fusion_analysis else "No fusion data available"\n\n    def process_sensor_data(self):\n        """Main processing function that runs periodically"""\n        # Analyze each sensor type\n        laser_analysis = self.analyze_laser_data()\n        imu_analysis = self.analyze_imu_data()\n        fusion_analysis = self.analyze_sensor_fusion()\n\n        # Overall status\n        status_msg = String()\n        status_msg.data = f"LIDAR: {laser_analysis} | IMU: {imu_analysis} | Fusion: {fusion_analysis}"\n        self.status_pub.publish(status_msg)\n\n        # Log for debugging\n        self.get_logger().info(f"Sensor Status: {status_msg.data}")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    processor = SensorProcessor()\n\n    try:\n        rclpy.spin(processor)\n    except KeyboardInterrupt:\n        processor.get_logger().info("Sensor Processor stopped by user")\n    finally:\n        processor.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.p,{children:"Now let's create a sensor validation node that compares simulated data with expected real-world characteristics:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# sensor_validator.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Imu, NavSatFix\nfrom geometry_msgs.msg import Vector3\nfrom std_msgs.msg import String, Bool\nimport numpy as np\nimport math\nfrom collections import deque\nimport statistics\n\nclass SensorValidator(Node):\n    \"\"\"\n    Validate simulated sensor data against expected real-world characteristics\n    \"\"\"\n    def __init__(self):\n        super().__init__('sensor_validator')\n\n        # Publishers\n        self.validation_pub = self.create_publisher(String, '/sensor_validation', 10)\n        self.health_pub = self.create_publisher(String, '/sensor_health', 10)\n\n        # Subscribers\n        self.scan_sub = self.create_subscription(LaserScan, '/sensor_robot/scan', self.scan_callback, 10)\n        self.imu_sub = self.create_subscription(Imu, '/sensor_robot/imu', self.imu_callback, 10)\n        self.gps_sub = self.create_subscription(NavSatFix, '/sensor_robot/gps/fix', self.gps_callback, 10)\n\n        # Timers\n        self.validation_timer = self.create_timer(2.0, self.run_validation)\n\n        # Sensor data with history\n        self.scan_history = deque(maxlen=10)  # Keep last 10 scans\n        self.imu_history = deque(maxlen=100)  # Keep last 100 IMU readings\n        self.gps_history = deque(maxlen=20)   # Keep last 20 GPS readings\n\n        # Validation parameters\n        self.validation_results = {\n            'lidar': {'status': 'unknown', 'details': ''},\n            'imu': {'status': 'unknown', 'details': ''},\n            'gps': {'status': 'unknown', 'details': ''}\n        }\n\n        # Expected characteristics for validation\n        self.expected_lidar_specs = {\n            'min_range': 0.1,\n            'max_range': 30.0,\n            'fov_horizontal': 270,  # degrees\n            'resolution': 0.5  # degrees per sample\n        }\n\n        self.expected_imu_specs = {\n            'angular_velocity_noise_std': 0.0017,  # rad/s\n            'linear_acceleration_noise_std': 0.017,  # m/s\xb2\n            'gravity': 9.81  # m/s\xb2\n        }\n\n        self.get_logger().info(\"Sensor Validator initialized\")\n\n    def scan_callback(self, msg):\n        \"\"\"Process and store LIDAR scan data\"\"\"\n        self.scan_history.append({\n            'header': msg.header,\n            'ranges': list(msg.ranges),\n            'intensities': list(msg.intensities) if msg.intensities else [],\n            'angle_min': msg.angle_min,\n            'angle_max': msg.angle_max,\n            'angle_increment': msg.angle_increment,\n            'time_increment': msg.time_increment,\n            'scan_time': msg.scan_time,\n            'range_min': msg.range_min,\n            'range_max': msg.range_max\n        })\n\n    def imu_callback(self, msg):\n        \"\"\"Process and store IMU data\"\"\"\n        self.imu_history.append({\n            'header': msg.header,\n            'orientation': {\n                'x': msg.orientation.x,\n                'y': msg.orientation.y,\n                'z': msg.orientation.z,\n                'w': msg.orientation.w\n            },\n            'angular_velocity': {\n                'x': msg.angular_velocity.x,\n                'y': msg.angular_velocity.y,\n                'z': msg.angular_velocity.z\n            },\n            'linear_acceleration': {\n                'x': msg.linear_acceleration.x,\n                'y': msg.linear_acceleration.y,\n                'z': msg.linear_acceleration.z\n            }\n        })\n\n    def gps_callback(self, msg):\n        \"\"\"Process and store GPS data\"\"\"\n        self.gps_history.append({\n            'header': msg.header,\n            'latitude': msg.latitude,\n            'longitude': msg.longitude,\n            'altitude': msg.altitude,\n            'position_covariance': list(msg.position_covariance),\n            'position_covariance_type': msg.position_covariance_type\n        })\n\n    def validate_lidar(self):\n        \"\"\"Validate LIDAR sensor characteristics\"\"\"\n        if not self.scan_history:\n            return {'status': 'no_data', 'details': 'No LIDAR data received'}\n\n        latest_scan = self.scan_history[-1]\n        issues = []\n\n        # Check range limits\n        valid_ranges = [r for r in latest_scan['ranges'] if not math.isnan(r)]\n        if valid_ranges:\n            min_range = min(valid_ranges)\n            max_range = max(valid_ranges)\n\n            if min_range < self.expected_lidar_specs['min_range']:\n                issues.append(f\"Range too low: {min_range:.2f}m < {self.expected_lidar_specs['min_range']}m\")\n            if max_range > self.expected_lidar_specs['max_range']:\n                issues.append(f\"Range too high: {max_range:.2f}m > {self.expected_lidar_specs['max_range']}m\")\n\n        # Check field of view\n        fov_degrees = math.degrees(latest_scan['angle_max'] - latest_scan['angle_min'])\n        expected_fov = self.expected_lidar_specs['fov_horizontal']\n        if abs(fov_degrees - expected_fov) > 10:  # Allow 10 degree tolerance\n            issues.append(f\"FOV mismatch: {fov_degrees:.1f}\xb0 vs expected {expected_fov}\xb0\")\n\n        # Check number of samples vs expected resolution\n        expected_samples = (fov_degrees / self.expected_lidar_specs['resolution'])\n        actual_samples = len(latest_scan['ranges'])\n        if abs(actual_samples - expected_samples) > expected_samples * 0.1:  # 10% tolerance\n            issues.append(f\"Sample count mismatch: {actual_samples} vs expected ~{expected_samples}\")\n\n        # Check for realistic data patterns (not all same values)\n        if len(set(valid_ranges)) < len(valid_ranges) * 0.1:  # Less than 10% unique values\n            issues.append(\"Unrealistic: too many identical range values\")\n\n        status = 'valid' if not issues else 'issues'\n        details = '; '.join(issues) if issues else 'All checks passed'\n\n        return {'status': status, 'details': details}\n\n    def validate_imu(self):\n        \"\"\"Validate IMU sensor characteristics\"\"\"\n        if len(self.imu_history) < 10:  # Need some history for validation\n            return {'status': 'insufficient_data', 'details': 'Need more IMU data for validation'}\n\n        # Extract angular velocity and linear acceleration data\n        ang_vel_data = []\n        lin_acc_data = []\n\n        for imu_reading in self.imu_history:\n            ang_vel_data.append([\n                imu_reading['angular_velocity']['x'],\n                imu_reading['angular_velocity']['y'],\n                imu_reading['angular_velocity']['z']\n            ])\n            lin_acc_data.append([\n                imu_reading['linear_acceleration']['x'],\n                imu_reading['linear_acceleration']['y'],\n                imu_reading['linear_acceleration']['z']\n            ])\n\n        ang_vel_array = np.array(ang_vel_data)\n        lin_acc_array = np.array(lin_acc_data)\n\n        issues = []\n\n        # Check angular velocity noise levels\n        ang_vel_std = np.std(ang_vel_array, axis=0)\n        expected_ang_vel_noise = self.expected_imu_specs['angular_velocity_noise_std']\n\n        for i, std_val in enumerate(ang_vel_std):\n            if std_val > expected_ang_vel_noise * 3:  # 3x tolerance\n                axis = ['X', 'Y', 'Z'][i]\n                issues.append(f\"Angular velocity {axis} noise too high: {std_val:.6f} > {expected_ang_vel_noise:.6f}\")\n\n        # Check linear acceleration noise levels\n        lin_acc_std = np.std(lin_acc_array, axis=0)\n        expected_lin_acc_noise = self.expected_imu_specs['linear_acceleration_noise_std']\n\n        for i, std_val in enumerate(lin_acc_std):\n            if std_val > expected_lin_acc_noise * 3:  # 3x tolerance\n                axis = ['X', 'Y', 'Z'][i]\n                issues.append(f\"Linear acceleration {axis} noise too high: {std_val:.6f} > {expected_lin_acc_noise:.6f}\")\n\n        # Check gravity in Z-axis (should be around 9.81 m/s\xb2 when robot is stationary)\n        lin_acc_z_mean = np.mean(lin_acc_array[:, 2])\n        gravity_diff = abs(abs(lin_acc_z_mean) - self.expected_imu_specs['gravity'])\n\n        if gravity_diff > 1.0:  # Allow 1 m/s\xb2 tolerance\n            issues.append(f\"Gravity detection issue: Z-axis avg {lin_acc_z_mean:.2f} vs expected ~{self.expected_imu_specs['gravity']}\")\n\n        # Check for realistic IMU data patterns\n        # (e.g., shouldn't have impossible acceleration values)\n        max_acc_magnitude = np.max(np.linalg.norm(lin_acc_array, axis=1))\n        if max_acc_magnitude > 100:  # Unlikely to have >100 m/s\xb2 acceleration\n            issues.append(f\"Unrealistic acceleration: {max_acc_magnitude:.2f} m/s\xb2\")\n\n        status = 'valid' if not issues else 'issues'\n        details = '; '.join(issues) if issues else 'All checks passed'\n\n        return {'status': status, 'details': details}\n\n    def validate_gps(self):\n        \"\"\"Validate GPS sensor characteristics\"\"\"\n        if not self.gps_history:\n            return {'status': 'no_data', 'details': 'No GPS data received'}\n\n        latest_gps = self.gps_history[-1]\n        issues = []\n\n        # Check if coordinates are reasonable (not zero or invalid)\n        if latest_gps['latitude'] == 0.0 and latest_gps['longitude'] == 0.0:\n            issues.append(\"GPS at origin (0,0) - possibly not initialized\")\n\n        # Check position covariance (should be finite and reasonable)\n        pos_cov = latest_gps['position_covariance']\n        if any(c > 1000 for c in pos_cov if not math.isnan(c)):  # If covariance is too high\n            issues.append(\"High position uncertainty\")\n\n        # Check for realistic coordinate changes between readings\n        if len(self.gps_history) > 1:\n            prev_gps = self.gps_history[-2]\n            lat_diff = abs(latest_gps['latitude'] - prev_gps['latitude'])\n            lon_diff = abs(latest_gps['longitude'] - prev_gps['longitude'])\n\n            # Convert to approximate meters (roughly 111km per degree latitude)\n            lat_meters = lat_diff * 111000\n            lon_meters = lon_diff * 111000 * math.cos(math.radians(latest_gps['latitude']))\n            distance = math.sqrt(lat_meters**2 + lon_meters**2)\n\n            # If the robot is moving too fast (e.g., >10 m/s) for a ground robot, flag it\n            # This assumes ~2Hz GPS update rate\n            if distance > 20:  # More than 20m in ~0.5s (40 m/s average)\n                issues.append(f\"Unrealistic GPS movement: {distance:.2f}m in recent update\")\n\n        status = 'valid' if not issues else 'issues'\n        details = '; '.join(issues) if issues else 'All checks passed'\n\n        return {'status': status, 'details': details}\n\n    def run_validation(self):\n        \"\"\"Run comprehensive sensor validation\"\"\"\n        # Validate each sensor type\n        self.validation_results['lidar'] = self.validate_lidar()\n        self.validation_results['imu'] = self.validate_imu()\n        self.validation_results['gps'] = self.validate_gps()\n\n        # Overall validation status\n        all_valid = all(result['status'] in ['valid', 'issues'] for result in self.validation_results.values())\n        any_issues = any(result['status'] == 'issues' for result in self.validation_results.values())\n\n        overall_status = 'valid' if all_valid and not any_issues else 'issues' if any_issues else 'invalid'\n\n        # Publish validation results\n        validation_msg = String()\n        validation_msg.data = f\"Overall: {overall_status} | \" \\\n                             f\"LIDAR: {self.validation_results['lidar']['status']} | \" \\\n                             f\"IMU: {self.validation_results['imu']['status']} | \" \\\n                             f\"GPS: {self.validation_results['gps']['status']}\"\n        self.validation_pub.publish(validation_msg)\n\n        # Publish detailed health report\n        health_msg = String()\n        health_details = []\n        for sensor, result in self.validation_results.items():\n            health_details.append(f\"{sensor.upper()}: {result['status']} - {result['details']}\")\n\n        health_msg.data = \" | \".join(health_details)\n        self.health_pub.publish(health_msg)\n\n        # Log results\n        self.get_logger().info(f\"Sensor Validation: {validation_msg.data}\")\n\ndef main(args=None):\n    rclpy.init(args=args)\n    validator = SensorValidator()\n\n    try:\n        rclpy.spin(validator)\n    except KeyboardInterrupt:\n        validator.get_logger().info(\"Sensor Validator stopped by user\")\n    finally:\n        validator.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.h2,{id:"small-simulation",children:"Small Simulation"}),"\n",(0,i.jsx)(n.p,{children:"Let's create a sensor fusion simulation that demonstrates how multiple sensors work together:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'# sensor_fusion_demo.py\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import LaserScan, Imu, NavSatFix\nfrom geometry_msgs.msg import PoseWithCovarianceStamped, TwistWithCovarianceStamped\nfrom nav_msgs.msg import Odometry\nfrom std_msgs.msg import String, Float64MultiArray\nfrom tf2_ros import TransformBroadcaster\nfrom geometry_msgs.msg import TransformStamped\nimport numpy as np\nimport math\nfrom scipy.spatial.transform import Rotation as R\nfrom collections import deque\n\nclass SensorFusionDemo(Node):\n    """\n    Demonstrate sensor fusion using multiple simulated sensors\n    """\n    def __init__(self):\n        super().__init__(\'sensor_fusion_demo\')\n\n        # Publishers\n        self.fused_pose_pub = self.create_publisher(PoseWithCovarianceStamped, \'/sensor_fusion/pose\', 10)\n        self.fused_twist_pub = self.create_publisher(TwistWithCovarianceStamped, \'/sensor_fusion/twist\', 10)\n        self.status_pub = self.create_publisher(String, \'/sensor_fusion/status\', 10)\n\n        # Subscribers\n        self.scan_sub = self.create_subscription(LaserScan, \'/sensor_robot/scan\', self.scan_callback, 10)\n        self.imu_sub = self.create_subscription(Imu, \'/sensor_robot/imu\', self.imu_callback, 10)\n        self.gps_sub = self.create_subscription(NavSatFix, \'/sensor_robot/gps/fix\', self.gps_callback, 10)\n        self.odom_sub = self.create_subscription(Odometry, \'/sensor_robot/odom\', self.odom_callback, 10)\n\n        # Timers\n        self.fusion_timer = self.create_timer(0.05, self.run_fusion)  # 20 Hz\n\n        # Sensor data storage\n        self.scan_data = None\n        self.imu_data = None\n        self.gps_data = None\n        self.odom_data = None\n\n        # Fusion state\n        self.fused_position = np.array([0.0, 0.0, 0.0])  # x, y, z\n        self.fused_orientation = np.array([0.0, 0.0, 0.0, 1.0])  # x, y, z, w quaternion\n        self.fused_velocity = np.array([0.0, 0.0, 0.0])  # vx, vy, vz\n        self.fused_angular_velocity = np.array([0.0, 0.0, 0.0])  # wx, wy, wz\n\n        # Covariance matrices (diagonal for simplicity)\n        self.position_covariance = np.array([0.1, 0.1, 0.1])  # Initial uncertainty\n        self.orientation_covariance = np.array([0.05, 0.05, 0.05])  # Initial uncertainty\n        self.velocity_covariance = np.array([0.2, 0.2, 0.2])  # Initial uncertainty\n\n        # Sensor weights for fusion\n        self.sensor_weights = {\n            \'odom\': 0.6,   # Wheel odometry - good for short-term precision\n            \'imu\': 0.3,    # IMU - good for orientation and acceleration\n            \'gps\': 0.1,    # GPS - good for absolute position (when available)\n            \'lidar\': 0.2   # LIDAR - good for relative positioning to landmarks\n        }\n\n        # History for smoothing\n        self.position_history = deque(maxlen=10)\n        self.velocity_history = deque(maxlen=10)\n\n        # Flags to track sensor availability\n        self.imu_available = False\n        self.gps_available = False\n        self.lidar_available = False\n\n        self.get_logger().info("Sensor Fusion Demo initialized")\n\n    def scan_callback(self, msg):\n        """Process LIDAR scan data"""\n        self.scan_data = msg\n        self.lidar_available = True\n\n    def imu_callback(self, msg):\n        """Process IMU data"""\n        self.imu_data = msg\n        self.imu_available = True\n\n    def gps_callback(self, msg):\n        """Process GPS data"""\n        self.gps_data = msg\n        self.gps_available = True\n\n    def odom_callback(self, msg):\n        """Process odometry data"""\n        self.odom_data = msg\n\n    def predict_state(self, dt):\n        """Predict state based on IMU and previous state"""\n        if self.imu_data and dt > 0:\n            # Update orientation from angular velocity\n            ang_vel = np.array([\n                self.imu_data.angular_velocity.x,\n                self.imu_data.angular_velocity.y,\n                self.imu_data.angular_velocity.z\n            ])\n\n            # Convert angular velocity to quaternion derivative\n            q = self.fused_orientation\n            omega_matrix = np.array([\n                [0, -ang_vel[0], -ang_vel[1], -ang_vel[2]],\n                [ang_vel[0], 0, ang_vel[2], -ang_vel[1]],\n                [ang_vel[1], -ang_vel[2], 0, ang_vel[0]],\n                [ang_vel[2], ang_vel[1], -ang_vel[0], 0]\n            ])\n\n            # Integrate quaternion\n            q_dot = 0.5 * omega_matrix @ q\n            self.fused_orientation += q_dot * dt\n            # Normalize quaternion\n            self.fused_orientation = self.fused_orientation / np.linalg.norm(self.fused_orientation)\n\n            # Update velocity from linear acceleration (in world frame)\n            acc = np.array([\n                self.imu_data.linear_acceleration.x,\n                self.imu_data.linear_acceleration.y,\n                self.imu_data.linear_acceleration.z\n            ])\n\n            # Transform acceleration to world frame using current orientation\n            r = R.from_quat(self.fused_orientation)\n            world_acc = r.apply(acc)\n\n            self.fused_velocity += world_acc * dt\n\n            # Update position from velocity\n            self.fused_position += self.fused_velocity * dt\n\n    def update_from_odom(self):\n        """Update state estimate from odometry"""\n        if self.odom_data:\n            odom_pos = self.odom_data.pose.pose.position\n            odom_pos_array = np.array([odom_pos.x, odom_pos.y, odom_pos.z])\n\n            # Simple weighted update\n            self.fused_position = (self.sensor_weights[\'odom\'] * odom_pos_array +\n                                  (1 - self.sensor_weights[\'odom\']) * self.fused_position)\n\n            # Update velocity from odometry twist\n            odom_vel = self.odom_data.twist.twist.linear\n            odom_vel_array = np.array([odom_vel.x, odom_vel.y, odom_vel.z])\n            self.fused_velocity = (self.sensor_weights[\'odom\'] * odom_vel_array +\n                                  (1 - self.sensor_weights[\'odom\']) * self.fused_velocity)\n\n    def update_from_gps(self):\n        """Update state estimate from GPS (simplified)"""\n        if self.gps_data:\n            # Convert lat/lon to local coordinates (very simplified)\n            # In a real system, you\'d use a proper coordinate transformation\n            gps_pos = np.array([self.gps_data.longitude * 1000,\n                               self.gps_data.latitude * 1000,\n                               self.gps_data.altitude])\n\n            # Update position with GPS (low weight due to drift)\n            self.fused_position = (self.sensor_weights[\'gps\'] * gps_pos +\n                                  (1 - self.sensor_weights[\'gps\']) * self.fused_position)\n\n    def update_from_lidar(self):\n        """Update state from LIDAR landmark detection (simplified)"""\n        if self.scan_data:\n            # Simplified: detect "landmarks" as consistent LIDAR returns\n            # In reality, you\'d run SLAM or landmark detection algorithms\n            valid_ranges = [i for i, r in enumerate(self.scan_data.ranges)\n                           if not math.isnan(r) and self.scan_data.range_min < r < self.scan_data.range_max]\n\n            if len(valid_ranges) > 10:  # If we have significant features\n                # This is a placeholder - in reality you\'d match to a map\n                # For now, just reduce uncertainty if we have good LIDAR data\n                self.position_covariance *= 0.9  # Reduce uncertainty\n\n    def run_fusion(self):\n        """Main sensor fusion loop"""\n        current_time = self.get_clock().now()\n\n        # Get time delta since last update\n        dt = 0.05  # Fixed 20Hz rate\n\n        # Prediction step: use IMU to predict state\n        self.predict_state(dt)\n\n        # Update steps: incorporate other sensor data\n        self.update_from_odom()\n        if self.gps_available:\n            self.update_from_gps()\n        if self.lidar_available:\n            self.update_from_lidar()\n\n        # Store in history for smoothing\n        self.position_history.append(self.fused_position.copy())\n        self.velocity_history.append(self.fused_velocity.copy())\n\n        # Apply simple smoothing\n        if len(self.position_history) > 1:\n            smoothed_pos = np.mean(self.position_history, axis=0)\n            self.fused_position = 0.7 * self.fused_position + 0.3 * smoothed_pos\n\n        if len(self.velocity_history) > 1:\n            smoothed_vel = np.mean(self.velocity_history, axis=0)\n            self.fused_velocity = 0.7 * self.fused_velocity + 0.3 * smoothed_vel\n\n        # Publish fused state\n        self.publish_fused_state(current_time)\n\n        # Publish status\n        status_msg = String()\n        status_msg.data = f"Fused Pos: ({self.fused_position[0]:.2f}, {self.fused_position[1]:.2f}, {self.fused_position[2]:.2f}), " \\\n                         f"Vel: ({self.fused_velocity[0]:.2f}, {self.fused_velocity[1]:.2f}, {self.fused_velocity[2]:.2f}), " \\\n                         f"PosCov: ({self.position_covariance[0]:.3f}, {self.position_covariance[1]:.3f}, {self.position_covariance[2]:.3f})"\n        self.status_pub.publish(status_msg)\n\n    def publish_fused_state(self, timestamp):\n        """Publish the fused state estimate"""\n        # Publish fused pose\n        pose_msg = PoseWithCovarianceStamped()\n        pose_msg.header.stamp = timestamp.to_msg()\n        pose_msg.header.frame_id = "map"\n\n        pose_msg.pose.pose.position.x = float(self.fused_position[0])\n        pose_msg.pose.pose.position.y = float(self.fused_position[1])\n        pose_msg.pose.pose.position.z = float(self.fused_position[2])\n\n        pose_msg.pose.pose.orientation.x = float(self.fused_orientation[0])\n        pose_msg.pose.pose.orientation.y = float(self.fused_orientation[1])\n        pose_msg.pose.pose.orientation.z = float(self.fused_orientation[2])\n        pose_msg.pose.pose.orientation.w = float(self.fused_orientation[3])\n\n        # Set covariance (diagonal elements only, for simplicity)\n        for i in range(3):\n            pose_msg.pose.covariance[i*6 + i] = float(self.position_covariance[i])\n        for i in range(3):\n            pose_msg.pose.covariance[(i+3)*6 + (i+3)] = float(self.orientation_covariance[i])\n\n        self.fused_pose_pub.publish(pose_msg)\n\n        # Publish fused twist\n        twist_msg = TwistWithCovarianceStamped()\n        twist_msg.header.stamp = timestamp.to_msg()\n        twist_msg.header.frame_id = "base_link"\n\n        twist_msg.twist.twist.linear.x = float(self.fused_velocity[0])\n        twist_msg.twist.twist.linear.y = float(self.fused_velocity[1])\n        twist_msg.twist.twist.linear.z = float(self.fused_velocity[2])\n\n        if self.imu_data:\n            twist_msg.twist.twist.angular.x = self.imu_data.angular_velocity.x\n            twist_msg.twist.twist.angular.y = self.imu_data.angular_velocity.y\n            twist_msg.twist.twist.angular.z = self.imu_data.angular_velocity.z\n\n        # Set velocity covariance\n        for i in range(3):\n            twist_msg.twist.covariance[i*6 + i] = float(self.velocity_covariance[i])\n\n        self.fused_twist_pub.publish(twist_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    fusion_demo = SensorFusionDemo()\n\n    try:\n        rclpy.spin(fusion_demo)\n    except KeyboardInterrupt:\n        fusion_demo.get_logger().info("Sensor Fusion Demo stopped by user")\n    finally:\n        fusion_demo.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsx)(n.h2,{id:"quick-recap",children:"Quick Recap"}),"\n",(0,i.jsx)(n.p,{children:"In this lesson, we've covered:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LIDAR Simulation"}),": Configuring realistic LIDAR sensors with proper noise models and characteristics"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Camera Simulation"}),": Setting up depth cameras with realistic distortion and resolution parameters"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"IMU Modeling"}),": Creating IMU sensors with realistic drift, noise, and bias characteristics"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multi-Sensor Integration"}),": Combining multiple sensors for comprehensive perception"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensor Validation"}),": Techniques for validating simulated sensor data against real-world expectations"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensor Fusion"}),": Demonstrating how multiple sensors work together to improve robot perception"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Sensor simulation is crucial for creating believable digital twins. The accuracy of your simulated sensors directly impacts how well your robot algorithms will perform when transferred to real hardware. Properly configured sensors with realistic noise models, drift characteristics, and limitations ensure that your robot's perception and navigation systems are robust enough to handle real-world conditions."}),"\n",(0,i.jsx)(n.h2,{id:"summary-of-chapter-3",children:"Summary of Chapter 3"}),"\n",(0,i.jsx)(n.p,{children:'In Chapter 3: "The Digital Twin (Gazebo & Unity)", we\'ve covered:'}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Introduction to Gazebo Simulation"}),": Core concepts and basic robot simulation setup"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Physics and Collision Modeling"}),": Realistic physics properties and collision detection"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Unity-Based Visualization"}),": High-quality visualization and user interface creation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Sensor Simulation"}),": Comprehensive sensor modeling and fusion techniques"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This chapter provides the foundation for creating realistic digital twins that combine accurate physics simulation with high-quality visualization and realistic sensor models."})]})}function c(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}},8453(e,n,s){s.d(n,{R:()=>o,x:()=>r});var a=s(6540);const i={},t=a.createContext(i);function o(e){const n=a.useContext(t);return a.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),a.createElement(t.Provider,{value:n},e.children)}}}]);