"use strict";(globalThis.webpackChunkphysical_ai_book=globalThis.webpackChunkphysical_ai_book||[]).push([[892],{3685(n,e,r){r.r(e),r.d(e,{assets:()=>l,contentTitle:()=>i,default:()=>p,frontMatter:()=>s,metadata:()=>t,toc:()=>m});const t=JSON.parse('{"id":"physical-ai/the-digital-twin-gazebo-unity/unity-based-visualization","title":"Unity-Based Visualization","description":"Overview","source":"@site/docs/physical-ai/the-digital-twin-gazebo-unity/unity-based-visualization.md","sourceDirName":"physical-ai/the-digital-twin-gazebo-unity","slug":"/physical-ai/the-digital-twin-gazebo-unity/unity-based-visualization","permalink":"/physical-ai-book/docs/physical-ai/the-digital-twin-gazebo-unity/unity-based-visualization","draft":false,"unlisted":false,"editUrl":"https://github.com/arijh/physical-ai-book/tree/main/packages/create-docusaurus/templates/shared/docs/physical-ai/the-digital-twin-gazebo-unity/unity-based-visualization.md","tags":[],"version":"current","frontMatter":{"sidebar_label":"Unity-Based Visualization","title":"Unity-Based Visualization"},"sidebar":"docs","previous":{"title":"Physics and Collision Modeling","permalink":"/physical-ai-book/docs/physical-ai/the-digital-twin-gazebo-unity/physics-and-collision-modeling"},"next":{"title":"Sensor Simulation","permalink":"/physical-ai-book/docs/physical-ai/the-digital-twin-gazebo-unity/sensor-simulation"}}');var a=r(4848),o=r(8453);const s={sidebar_label:"Unity-Based Visualization",title:"Unity-Based Visualization"},i="Unity-Based Visualization",l={},m=[{value:"Overview",id:"overview",level:2},{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Hands-on Steps",id:"hands-on-steps",level:2},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Code Examples",id:"code-examples",level:2},{value:"Small Simulation",id:"small-simulation",level:2},{value:"Quick Recap",id:"quick-recap",level:2}];function c(n){const e={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,o.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"unity-based-visualization",children:"Unity-Based Visualization"})}),"\n",(0,a.jsx)(e.h2,{id:"overview",children:"Overview"}),"\n",(0,a.jsx)(e.p,{children:"Unity provides a powerful real-time visualization platform that can serve as a complementary tool to Gazebo's physics simulation. While Gazebo excels at physics-based simulation, Unity offers high-quality graphics rendering, advanced visual effects, and immersive user interfaces. This lesson explores how to integrate Unity with ROS 2 for enhanced robot visualization, debugging, and human-robot interaction design."}),"\n",(0,a.jsx)(e.p,{children:"Unity's capabilities include photorealistic rendering, virtual reality integration, and custom user interfaces that can provide additional insights into robot behavior beyond what traditional simulation environments offer."}),"\n",(0,a.jsx)(e.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,a.jsx)(e.p,{children:"By the end of this lesson, you will be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Understand the role of Unity in robotics visualization"}),"\n",(0,a.jsx)(e.li,{children:"Set up Unity-ROS 2 communication bridges"}),"\n",(0,a.jsx)(e.li,{children:"Create custom visualizations for robot states and sensor data"}),"\n",(0,a.jsx)(e.li,{children:"Implement VR/AR interfaces for robot teleoperation"}),"\n",(0,a.jsx)(e.li,{children:"Design intuitive interfaces for robot monitoring and control"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"hands-on-steps",children:"Hands-on Steps"}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Unity-ROS Bridge Setup"}),": Configure communication between Unity and ROS 2"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Robot Visualization"}),": Create 3D models and animations for robot states"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Sensor Data Visualization"}),": Display sensor data in Unity environment"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"User Interface Creation"}),": Build custom UI for robot monitoring"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"VR Integration"}),": Implement basic VR teleoperation interface"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"Understanding of ROS 2 concepts and message types"}),"\n",(0,a.jsx)(e.li,{children:"Basic familiarity with Unity development (or willingness to learn)"}),"\n",(0,a.jsx)(e.li,{children:"Knowledge of 3D modeling concepts"}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"code-examples",children:"Code Examples"}),"\n",(0,a.jsx)(e.p,{children:"Let's start by creating ROS 2 nodes that can interface with Unity for visualization purposes:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# unity_bridge.py\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Pose, Twist, Point\nfrom nav_msgs.msg import Odometry\nfrom sensor_msgs.msg import LaserScan, Image, JointState\nfrom std_msgs.msg import String, Float32MultiArray\nfrom visualization_msgs.msg import Marker, MarkerArray\nimport math\nimport json\n\nclass UnityBridge(Node):\n    """\n    Bridge node for sending ROS 2 data to Unity for visualization\n    """\n    def __init__(self):\n        super().__init__(\'unity_bridge\')\n\n        # Publishers for Unity visualization\n        self.robot_pose_pub = self.create_publisher(Float32MultiArray, \'/unity/robot_pose\', 10)\n        self.laser_data_pub = self.create_publisher(Float32MultiArray, \'/unity/laser_data\', 10)\n        self.marker_pub = self.create_publisher(MarkerArray, \'/unity/markers\', 10)\n        self.status_pub = self.create_publisher(String, \'/unity/status\', 10)\n\n        # Subscribers for robot data\n        self.odom_sub = self.create_subscription(Odometry, \'/physics_robot/odom\', self.odom_callback, 10)\n        self.scan_sub = self.create_subscription(LaserScan, \'/physics_robot/scan\', self.scan_callback, 10)\n        self.joint_sub = self.create_subscription(JointState, \'/physics_robot/joint_states\', self.joint_callback, 10)\n\n        # Timer for publishing visualization data\n        self.viz_timer = self.create_timer(0.033, self.publish_visualization_data)  # ~30 FPS\n\n        # Robot state storage\n        self.robot_pose = Pose()\n        self.laser_ranges = []\n        self.joint_positions = {}\n        self.last_update_time = self.get_clock().now()\n\n        # Visualization parameters\n        self.robot_scale = [1.0, 1.0, 1.0]\n        self.robot_color = [0.2, 0.6, 1.0, 1.0]  # RGBA\n\n        self.get_logger().info("Unity Bridge initialized")\n\n    def odom_callback(self, msg):\n        """Process odometry data for visualization"""\n        self.robot_pose = msg.pose.pose\n\n    def scan_callback(self, msg):\n        """Process laser scan data for visualization"""\n        self.laser_ranges = msg.ranges\n\n    def joint_callback(self, msg):\n        """Process joint state data"""\n        for i, name in enumerate(msg.name):\n            if i < len(msg.position):\n                self.joint_positions[name] = msg.position[i]\n\n    def publish_visualization_data(self):\n        """Publish robot data in Unity-compatible format"""\n        # Publish robot pose (position and orientation as float array)\n        pose_msg = Float32MultiArray()\n        pose_msg.data = [\n            self.robot_pose.position.x,\n            self.robot_pose.position.y,\n            self.robot_pose.position.z,\n            self.robot_pose.orientation.x,\n            self.robot_pose.orientation.y,\n            self.robot_pose.orientation.z,\n            self.robot_pose.orientation.w\n        ]\n        self.robot_pose_pub.publish(pose_msg)\n\n        # Publish laser scan data\n        if self.laser_ranges:\n            laser_msg = Float32MultiArray()\n            # Limit to 360 points for performance\n            step = max(1, len(self.laser_ranges) // 360)\n            laser_msg.data = self.laser_ranges[::step][:360]  # Take up to 360 points\n            self.laser_data_pub.publish(laser_msg)\n\n        # Publish markers for Unity visualization\n        marker_array = MarkerArray()\n\n        # Robot marker\n        robot_marker = Marker()\n        robot_marker.header.frame_id = "map"\n        robot_marker.header.stamp = self.get_clock().now().to_msg()\n        robot_marker.ns = "robot"\n        robot_marker.id = 0\n        robot_marker.type = Marker.MESH_RESOURCE\n        robot_marker.mesh_resource = "package://unity_visualization/meshes/robot.dae"\n        robot_marker.action = Marker.ADD\n        robot_marker.pose = self.robot_pose\n        robot_marker.scale.x = self.robot_scale[0]\n        robot_marker.scale.y = self.robot_scale[1]\n        robot_marker.scale.z = self.robot_scale[2]\n        robot_marker.color.r = self.robot_color[0]\n        robot_marker.color.g = self.robot_color[1]\n        robot_marker.color.b = self.robot_color[2]\n        robot_marker.color.a = self.robot_color[3]\n        marker_array.markers.append(robot_marker)\n\n        # Laser scan visualization as points\n        if self.laser_ranges:\n            scan_marker = Marker()\n            scan_marker.header.frame_id = "base_link"  # Robot\'s frame\n            scan_marker.header.stamp = self.get_clock().now().to_msg()\n            scan_marker.ns = "laser_scan"\n            scan_marker.id = 1\n            scan_marker.type = Marker.POINTS\n            scan_marker.action = Marker.ADD\n            scan_marker.scale.x = 0.02  # Point size\n            scan_marker.scale.y = 0.02\n            scan_marker.color.r = 1.0\n            scan_marker.color.g = 0.0\n            scan_marker.color.b = 0.0\n            scan_marker.color.a = 0.8\n\n            # Convert laser ranges to points\n            angle_min = -math.pi/2  # Assuming 90 degree FOV\n            angle_increment = math.pi / len(self.laser_ranges) if len(self.laser_ranges) > 0 else 0.01\n\n            for i, range_val in enumerate(self.laser_ranges[::10]):  # Sample every 10th point for performance\n                if not math.isnan(range_val) and range_val > 0.1 and range_val < 10.0:\n                    angle = angle_min + i * 10 * angle_increment\n                    x = range_val * math.cos(angle)\n                    y = range_val * math.sin(angle)\n\n                    point = Point()\n                    point.x = x\n                    point.y = y\n                    point.z = 0.1  # Slightly above ground\n                    scan_marker.points.append(point)\n\n                    # Add color based on distance\n                    scan_marker.colors.append(self.get_color_for_distance(range_val))\n\n            marker_array.markers.append(scan_marker)\n\n        self.marker_pub.publish(marker_array)\n\n        # Publish status\n        status_msg = String()\n        status_msg.data = f"Unity Bridge: Pos=({self.robot_pose.position.x:.2f}, {self.robot_pose.position.y:.2f}), " \\\n                         f"Scan Points={len(self.laser_ranges)}, Joints={len(self.joint_positions)}"\n        self.status_pub.publish(status_msg)\n\n    def get_color_for_distance(self, distance):\n        """Get color based on distance value for visualization"""\n        from std_msgs.msg import ColorRGBA\n        color = ColorRGBA()\n\n        # Color coding: red for close, green for far\n        if distance < 1.0:\n            color.r = 1.0\n            color.g = 0.0\n            color.b = 0.0\n        elif distance < 3.0:\n            color.r = 1.0\n            color.g = 1.0\n            color.b = 0.0\n        else:\n            color.r = 0.0\n            color.g = 1.0\n            color.b = 0.0\n\n        color.a = 1.0\n        return color\n\ndef main(args=None):\n    rclpy.init(args=args)\n    bridge = UnityBridge()\n\n    try:\n        rclpy.spin(bridge)\n    except KeyboardInterrupt:\n        bridge.get_logger().info("Unity Bridge stopped by user")\n    finally:\n        bridge.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(e.p,{children:"Now let's create a Unity C# script that can receive and visualize the ROS 2 data:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-csharp",children:'// UnityRobotVisualizer.cs\nusing System.Collections;\nusing System.Collections.Generic;\nusing UnityEngine;\nusing Ros2ForUnity.Messages.Std_msgs;\nusing Ros2ForUnity.Messages.Geometry_msgs;\nusing Ros2ForUnity.Messages.Sensor_msgs;\nusing Ros2ForUnity.Messages.Nav_msgs;\n\npublic class UnityRobotVisualizer : MonoBehaviour\n{\n    [Header("ROS Connection")]\n    public string rosAgentIp = "127.0.0.1";\n    public int rosAgentPort = 8888;\n\n    [Header("Robot Visualization")]\n    public GameObject robotModel;\n    public Material robotMaterial;\n    public float robotScale = 1.0f;\n\n    [Header("Laser Visualization")]\n    public GameObject laserPointPrefab;\n    public Material laserMaterial;\n    public float maxLaserDistance = 10.0f;\n    public int laserPointCount = 360;\n\n    [Header("UI Elements")]\n    public UnityEngine.UI.Text statusText;\n    public UnityEngine.UI.Text positionText;\n\n    private Ros2Node rosNode;\n    private Ros2Publisher<Float32MultiArray> robotPosePublisher;\n    private Ros2Subscriber<Float32MultiArray> robotPoseSubscriber;\n    private Ros2Subscriber<Float32MultiArray> laserDataSubscriber;\n    private Ros2Subscriber<Visualization_msgs.MarkerArray> markerSubscriber;\n\n    private GameObject[] laserPoints;\n    private Vector3 robotPosition;\n    private Quaternion robotRotation;\n\n    void Start()\n    {\n        // Initialize ROS 2 connection\n        InitializeROSConnection();\n\n        // Initialize laser visualization\n        InitializeLaserVisualization();\n\n        // Set initial values\n        robotPosition = Vector3.zero;\n        robotRotation = Quaternion.identity;\n    }\n\n    void InitializeROSConnection()\n    {\n        // Create ROS node\n        rosNode = new Ros2Node("unity_visualizer");\n\n        // Create subscribers\n        robotPoseSubscriber = rosNode.CreateSubscription<Float32MultiArray>(\n            "/unity/robot_pose",\n            ReceiveRobotPose);\n\n        laserDataSubscriber = rosNode.CreateSubscription<Float32MultiArray>(\n            "/unity/laser_data",\n            ReceiveLaserData);\n\n        markerSubscriber = rosNode.CreateSubscription<Visualization_msgs.MarkerArray>(\n            "/unity/markers",\n            ReceiveMarkers);\n\n        // Start ROS communication\n        Ros2ForUnity.Ros2cs.Init();\n        Ros2ForUnity.Ros2cs.CreateNode(rosNode);\n\n        rosNode.Spinner.Start();\n    }\n\n    void InitializeLaserVisualization()\n    {\n        // Create laser point objects\n        laserPoints = new GameObject[laserPointCount];\n        for (int i = 0; i < laserPointCount; i++)\n        {\n            laserPoints[i] = Instantiate(laserPointPrefab);\n            laserPoints[i].SetActive(false);\n            laserPoints[i].GetComponent<Renderer>().material = laserMaterial;\n        }\n    }\n\n    void ReceiveRobotPose(Float32MultiArray msg)\n    {\n        if (msg.Data.Count >= 7) // Position (3) + Orientation (4)\n        {\n            robotPosition = new Vector3(msg.Data[0], msg.Data[1], msg.Data[2]);\n            robotRotation = new Quaternion(msg.Data[3], msg.Data[4], msg.Data[5], msg.Data[6]);\n\n            // Update robot position in Unity\n            if (robotModel != null)\n            {\n                robotModel.transform.position = robotPosition;\n                robotModel.transform.rotation = robotRotation;\n            }\n\n            // Update UI\n            if (positionText != null)\n            {\n                positionText.text = $"Position: ({robotPosition.x:F2}, {robotPosition.y:F2}, {robotPosition.z:F2})";\n            }\n        }\n    }\n\n    void ReceiveLaserData(Float32MultiArray msg)\n    {\n        if (msg.Data.Count > 0)\n        {\n            // Update laser visualization\n            UpdateLaserVisualization(msg.Data);\n        }\n    }\n\n    void ReceiveMarkers(Visualization_msgs.MarkerArray msg)\n    {\n        foreach (var marker in msg.Markers)\n        {\n            if (marker.Type == Visualization_msgs.Marker.POINTS)\n            {\n                // Handle point cloud visualization\n                UpdatePointCloudVisualization(marker);\n            }\n            else if (marker.Type == Visualization_msgs.Marker.MESH_RESOURCE)\n            {\n                // Handle robot model update\n                UpdateRobotModel(marker);\n            }\n        }\n    }\n\n    void UpdateLaserVisualization(List<float> ranges)\n    {\n        float angleStep = 2.0f * Mathf.PI / ranges.Count;\n\n        for (int i = 0; i < Mathf.Min(ranges.Count, laserPointCount); i++)\n        {\n            float range = ranges[i];\n            if (range > 0.1f && range < maxLaserDistance)\n            {\n                float angle = i * angleStep - Mathf.PI; // Center at -\u03c0 to \u03c0\n\n                Vector3 pointPos = new Vector3(\n                    range * Mathf.Cos(angle),\n                    0.1f, // Slightly above ground\n                    range * Mathf.Sin(angle)\n                );\n\n                laserPoints[i].transform.position = robotModel.transform.TransformPoint(pointPos);\n                laserPoints[i].SetActive(true);\n\n                // Color based on distance\n                float colorValue = Mathf.InverseLerp(0.1f, maxLaserDistance, range);\n                laserPoints[i].GetComponent<Renderer>().material.color =\n                    Color.Lerp(Color.red, Color.green, colorValue);\n            }\n            else\n            {\n                laserPoints[i].SetActive(false);\n            }\n        }\n\n        // Hide remaining points\n        for (int i = ranges.Count; i < laserPointCount; i++)\n        {\n            laserPoints[i].SetActive(false);\n        }\n    }\n\n    void UpdatePointCloudVisualization(Visualization_msgs.Marker marker)\n    {\n        // Process point cloud data from ROS marker\n        for (int i = 0; i < Mathf.Min(marker.Points.Count, laserPointCount); i++)\n        {\n            var rosPoint = marker.Points[i];\n            Vector3 unityPoint = new Vector3(rosPoint.X, rosPoint.Z, rosPoint.Y); // Convert ROS to Unity coordinates\n\n            if (i < laserPoints.Length)\n            {\n                laserPoints[i].transform.position = unityPoint;\n                laserPoints[i].SetActive(true);\n\n                if (i < marker.Colors.Count)\n                {\n                    var rosColor = marker.Colors[i];\n                    laserPoints[i].GetComponent<Renderer>().material.color =\n                        new Color(rosColor.R, rosColor.G, rosColor.B, rosColor.A);\n                }\n            }\n        }\n    }\n\n    void UpdateRobotModel(Visualization_msgs.Marker marker)\n    {\n        // Update robot model based on marker data\n        if (robotModel != null)\n        {\n            robotModel.transform.position = new Vector3(\n                (float)marker.Pose.Position.X,\n                (float)marker.Pose.Position.Z, // ROS Y -> Unity Z\n                (float)marker.Pose.Position.Y  // ROS Z -> Unity Y\n            );\n\n            robotModel.transform.rotation = new Quaternion(\n                (float)marker.Pose.Orientation.X,\n                (float)marker.Pose.Orientation.Z,\n                (float)marker.Pose.Orientation.Y,\n                (float)marker.Pose.Orientation.W\n            );\n        }\n    }\n\n    void Update()\n    {\n        // Update status text\n        if (statusText != null)\n        {\n            statusText.text = $"ROS Connection: {(rosNode != null ? "Connected" : "Disconnected")}\\n" +\n                             $"Laser Points: {(laserPoints != null ? laserPoints.Length : 0)}";\n        }\n    }\n\n    void OnDestroy()\n    {\n        if (rosNode != null)\n        {\n            rosNode.Spinner.Stop();\n            Ros2ForUnity.Ros2cs.DestroyNode(rosNode);\n            Ros2ForUnity.Ros2cs.Shutdown();\n        }\n    }\n}\n'})}),"\n",(0,a.jsx)(e.p,{children:"Now let's create a ROS 2 node for Unity-based teleoperation interface:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# unity_teleop.py\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist, PoseStamped\nfrom sensor_msgs.msg import Joy\nfrom std_msgs.msg import String, Bool\nfrom visualization_msgs.msg import InteractiveMarker, InteractiveMarkerControl\nimport math\n\nclass UnityTeleop(Node):\n    """\n    Unity-based teleoperation interface for robot control\n    """\n    def __init__(self):\n        super().__init__(\'unity_teleop\')\n\n        # Publishers\n        self.cmd_vel_pub = self.create_publisher(Twist, \'/physics_robot/cmd_vel\', 10)\n        self.pose_pub = self.create_publisher(PoseStamped, \'/unity/teleop_pose\', 10)\n        self.status_pub = self.create_publisher(String, \'/unity/teleop_status\', 10)\n\n        # Subscribers\n        self.joy_sub = self.create_subscription(Joy, \'/unity/joy_input\', self.joy_callback, 10)\n\n        # Timers\n        self.teleop_timer = self.create_timer(0.05, self.teleop_loop)\n\n        # Teleoperation state\n        self.linear_speed = 0.0\n        self.angular_speed = 0.0\n        self.last_joy_input = None\n        self.teleop_active = False\n        self.control_mode = "velocity"  # velocity, position, or trajectory\n\n        # Velocity limits\n        self.max_linear = 0.5\n        self.max_angular = 1.0\n\n        self.get_logger().info("Unity Teleoperation Interface initialized")\n\n    def joy_callback(self, msg):\n        """Process joystick input from Unity"""\n        self.last_joy_input = msg\n        self.teleop_active = True\n\n        # Map joystick axes to robot commands\n        # Axis 1: Left stick vertical (forward/backward)\n        # Axis 0: Left stick horizontal (turn left/right)\n        linear_input = msg.axes[1] if len(msg.axes) > 1 else 0.0\n        angular_input = msg.axes[0] if len(msg.axes) > 0 else 0.0\n\n        # Apply deadzone\n        if abs(linear_input) < 0.2:\n            linear_input = 0.0\n        if abs(angular_input) < 0.2:\n            angular_input = 0.0\n\n        # Scale to max velocities\n        self.linear_speed = linear_input * self.max_linear\n        self.angular_speed = angular_input * self.max_angular\n\n    def teleop_loop(self):\n        """Main teleoperation loop"""\n        cmd = Twist()\n\n        if self.teleop_active and self.last_joy_input:\n            # Apply current speeds\n            cmd.linear.x = self.linear_speed\n            cmd.angular.z = self.angular_speed\n\n            # Check for special buttons\n            if len(self.last_joy_input.buttons) > 0 and self.last_joy_input.buttons[0] == 1:  # A button\n                # Emergency stop\n                cmd.linear.x = 0.0\n                cmd.angular.z = 0.0\n                self.get_logger().info("Emergency stop activated")\n\n            # Check for mode change (e.g., right bumper button)\n            if len(self.last_joy_input.buttons) > 5 and self.last_joy_input.buttons[5] == 1:  # Right bumper\n                self.control_mode = "position" if self.control_mode == "velocity" else "velocity"\n                self.get_logger().info(f"Control mode changed to: {self.control_mode}")\n\n        # Publish command\n        self.cmd_vel_pub.publish(cmd)\n\n        # Publish status\n        status_msg = String()\n        status_msg.data = f"Mode: {self.control_mode}, Lin: {self.linear_speed:.2f}, Ang: {self.angular_speed:.2f}, " \\\n                         f"Active: {self.teleop_active}"\n        self.status_pub.publish(status_msg)\n\n        # Reset activity if no input for a while\n        if self.last_joy_input is None:\n            self.teleop_active = False\n        else:\n            # Update last input time\n            pass\n\ndef main(args=None):\n    rclpy.init(args=args)\n    teleop = UnityTeleop()\n\n    try:\n        rclpy.spin(teleop)\n    except KeyboardInterrupt:\n        teleop.get_logger().info("Unity Teleoperation stopped by user")\n    finally:\n        # Send stop command on exit\n        cmd = Twist()\n        cmd.linear.x = 0.0\n        cmd.angular.z = 0.0\n        teleop.cmd_vel_pub.publish(cmd)\n        teleop.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(e.h2,{id:"small-simulation",children:"Small Simulation"}),"\n",(0,a.jsx)(e.p,{children:"Let's create a Unity-ROS integration test that demonstrates advanced visualization capabilities:"}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# unity_integration_test.py\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Pose, Point, Vector3\nfrom sensor_msgs.msg import LaserScan, Image\nfrom std_msgs.msg import ColorRGBA, String, Float32MultiArray\nfrom visualization_msgs.msg import Marker, MarkerArray\nfrom builtin_interfaces.msg import Duration\nimport math\nimport numpy as np\nfrom datetime import datetime\n\nclass UnityIntegrationTest(Node):\n    """\n    Comprehensive test of Unity-ROS integration capabilities\n    """\n    def __init__(self):\n        super().__init__(\'unity_integration_test\')\n\n        # Publishers for Unity visualization\n        self.marker_pub = self.create_publisher(MarkerArray, \'/unity_integration/markers\', 10)\n        self.path_pub = self.create_publisher(Marker, \'/unity_integration/path\', 10)\n        self.heatmap_pub = self.create_publisher(Float32MultiArray, \'/unity_integration/heatmap\', 10)\n        self.status_pub = self.create_publisher(String, \'/unity_integration/status\', 10)\n\n        # Timer for dynamic visualization\n        self.test_timer = self.create_timer(0.1, self.run_integration_test)\n\n        # Test parameters\n        self.test_phase = 0\n        self.test_start_time = self.get_clock().now()\n        self.robot_path = []\n        self.heatmap_data = np.zeros((50, 50))  # 50x50 grid for heatmap\n        self.animation_time = 0.0\n\n        self.get_logger().info("Unity Integration Test initialized")\n\n    def create_robot_marker(self, x, y, z, angle=0.0):\n        """Create a marker for the robot"""\n        marker = Marker()\n        marker.header.frame_id = "map"\n        marker.header.stamp = self.get_clock().now().to_msg()\n        marker.ns = "integration_test"\n        marker.id = 0\n        marker.type = Marker.MESH_RESOURCE\n        marker.action = Marker.ADD\n        marker.pose.position.x = x\n        marker.pose.position.y = y\n        marker.pose.position.z = z\n        marker.pose.orientation.z = math.sin(angle / 2.0)\n        marker.pose.orientation.w = math.cos(angle / 2.0)\n        marker.scale.x = 1.0\n        marker.scale.y = 1.0\n        marker.scale.z = 1.0\n        marker.color.r = 0.2\n        marker.color.g = 0.6\n        marker.color.b = 1.0\n        marker.color.a = 1.0\n        marker.mesh_resource = "package://unity_integration_test/meshes/robot.dae"\n        return marker\n\n    def create_path_marker(self, path_points):\n        """Create a marker for the robot\'s path"""\n        marker = Marker()\n        marker.header.frame_id = "map"\n        marker.header.stamp = self.get_clock().now().to_msg()\n        marker.ns = "integration_test"\n        marker.id = 1\n        marker.type = Marker.LINE_STRIP\n        marker.action = Marker.ADD\n        marker.scale.x = 0.05  # Line width\n        marker.color.r = 1.0\n        marker.color.a = 0.8\n\n        for point in path_points:\n            p = Point()\n            p.x = point[0]\n            p.y = point[1]\n            p.z = 0.1  # Slightly above ground\n            marker.points.append(p)\n\n            # Color gradient based on position in path\n            color = ColorRGBA()\n            progress = len(marker.points) / len(path_points) if path_points else 0\n            color.r = progress\n            color.g = 1.0 - progress\n            color.b = 0.5\n            color.a = 0.8\n            marker.colors.append(color)\n\n        return marker\n\n    def create_heatmap_marker(self):\n        """Create heatmap data for Unity"""\n        heatmap_msg = Float32MultiArray()\n\n        # Generate dynamic heatmap data\n        current_time = (self.get_clock().now() - self.test_start_time).nanoseconds / 1e9\n        for i in range(50):\n            for j in range(50):\n                # Create wave pattern\n                x_norm = (i - 25) / 25.0\n                y_norm = (j - 25) / 25.0\n                distance = math.sqrt(x_norm*x_norm + y_norm*y_norm)\n\n                # Add time-varying wave\n                wave = math.sin(distance * 5 - current_time * 2)\n                self.heatmap_data[i, j] = (wave + 1) / 2  # Normalize to 0-1\n\n        # Flatten the 2D array for transmission\n        heatmap_msg.data = self.heatmap_data.flatten().tolist()\n        return heatmap_msg\n\n    def create_sensor_fusion_marker(self):\n        """Create visualization for sensor fusion results"""\n        marker_array = MarkerArray()\n\n        # Create multiple sensor data representations\n        current_time = (self.get_clock().now() - self.test_start_time).nanoseconds / 1e9\n\n        # Simulated LIDAR points (spiral pattern)\n        lidar_marker = Marker()\n        lidar_marker.header.frame_id = "map"\n        lidar_marker.header.stamp = self.get_clock().now().to_msg()\n        lidar_marker.ns = "sensor_fusion"\n        lidar_marker.id = 10\n        lidar_marker.type = Marker.POINTS\n        lidar_marker.action = Marker.ADD\n        lidar_marker.scale.x = 0.03\n        lidar_marker.scale.y = 0.03\n        lidar_marker.color.r = 1.0\n        lidar_marker.color.a = 0.7\n\n        for i in range(100):\n            angle = i * 0.1 + current_time\n            distance = 2.0 + math.sin(i * 0.05) * 0.5\n            x = distance * math.cos(angle)\n            y = distance * math.sin(angle)\n\n            point = Point()\n            point.x = x\n            point.y = y\n            point.z = 0.1\n            lidar_marker.points.append(point)\n\n            # Color based on distance\n            color = ColorRGBA()\n            color.r = min(1.0, distance / 3.0)\n            color.g = 1.0 - min(1.0, distance / 3.0)\n            color.b = 0.5\n            color.a = 0.7\n            lidar_marker.colors.append(color)\n\n        marker_array.markers.append(lidar_marker)\n\n        # Simulated camera FOV\n        fov_marker = Marker()\n        fov_marker.header.frame_id = "map"\n        fov_marker.header.stamp = self.get_clock().now().to_msg()\n        fov_marker.ns = "sensor_fusion"\n        fov_marker.id = 11\n        fov_marker.type = Marker.TRIANGLE_LIST\n        fov_marker.action = Marker.ADD\n        fov_marker.scale.x = 1.0\n        fov_marker.scale.y = 1.0\n        fov_marker.scale.z = 1.0\n        fov_marker.color.b = 0.3\n        fov_marker.color.a = 0.3\n\n        # Create a simple camera FOV triangle\n        robot_pos = Point()\n        robot_pos.x = 0.0\n        robot_pos.y = 0.0\n        robot_pos.z = 0.1\n\n        # FOV vertices\n        left_corner = Point()\n        left_corner.x = 2.0 * math.cos(-0.5)  # 60 degree FOV\n        left_corner.y = 2.0 * math.sin(-0.5)\n        left_corner.z = 0.1\n\n        right_corner = Point()\n        right_corner.x = 2.0 * math.cos(0.5)\n        right_corner.y = 2.0 * math.sin(0.5)\n        right_corner.z = 0.1\n\n        # Add triangle (robot position to left corner to right corner)\n        fov_marker.points.extend([robot_pos, left_corner, right_corner])\n\n        marker_array.markers.append(fov_marker)\n\n        return marker_array\n\n    def run_integration_test(self):\n        """Run comprehensive integration test"""\n        marker_array = MarkerArray()\n        current_time = (self.get_clock().now() - self.test_start_time).nanoseconds / 1e9\n\n        # Phase 0: Basic robot visualization\n        if self.test_phase == 0:\n            # Moving robot in a circle\n            radius = 2.0\n            angle = current_time * 0.5  # Rotate at 0.5 rad/s\n            x = radius * math.cos(angle)\n            y = radius * math.sin(angle)\n            z = 0.1\n\n            robot_marker = self.create_robot_marker(x, y, z, angle)\n            marker_array.markers.append(robot_marker)\n\n            # Add to path\n            self.robot_path.append((x, y))\n            if len(self.robot_path) > 100:  # Limit path length\n                self.robot_path.pop(0)\n\n            path_marker = self.create_path_marker(self.robot_path)\n            marker_array.markers.append(path_marker)\n\n            if current_time > 10.0:  # Move to next phase after 10 seconds\n                self.test_phase = 1\n                self.test_start_time = self.get_clock().now()\n\n        # Phase 1: Sensor fusion visualization\n        elif self.test_phase == 1:\n            sensor_markers = self.create_sensor_fusion_marker()\n            marker_array.markers.extend(sensor_markers.markers)\n\n            if current_time > 10.0:  # Move to next phase\n                self.test_phase = 2\n                self.test_start_time = self.get_clock().now()\n\n        # Phase 2: Heatmap visualization\n        elif self.test_phase == 2:\n            heatmap_msg = self.create_heatmap_marker()\n            self.heatmap_pub.publish(heatmap_msg)\n\n            # Also publish a simple status marker\n            status_marker = Marker()\n            status_marker.header.frame_id = "map"\n            status_marker.header.stamp = self.get_clock().now().to_msg()\n            status_marker.ns = "integration_test"\n            status_marker.id = 20\n            status_marker.type = Marker.TEXT_VIEW_FACING\n            status_marker.action = Marker.ADD\n            status_marker.pose.position.x = 0.0\n            status_marker.pose.position.y = 0.0\n            status_marker.pose.position.z = 1.0\n            status_marker.scale.z = 0.3\n            status_marker.color.r = 1.0\n            status_marker.color.g = 1.0\n            status_marker.color.b = 1.0\n            status_marker.color.a = 1.0\n            status_marker.text = f"Integration Test Phase 2\\nTime: {current_time:.1f}s"\n            marker_array.markers.append(status_marker)\n\n            if current_time > 10.0:  # Cycle back to phase 0\n                self.test_phase = 0\n                self.test_start_time = self.get_clock().now()\n\n        # Publish all markers\n        self.marker_pub.publish(marker_array)\n\n        # Publish path separately as well\n        if len(self.robot_path) > 1 and self.test_phase == 0:\n            path_marker = self.create_path_marker(self.robot_path)\n            self.path_pub.publish(path_marker)\n\n        # Publish status\n        status_msg = String()\n        status_msg.data = f"Phase: {self.test_phase}, Time: {current_time:.1f}s, Markers: {len(marker_array.markers)}"\n        self.status_pub.publish(status_msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    test_node = UnityIntegrationTest()\n\n    try:\n        rclpy.spin(test_node)\n    except KeyboardInterrupt:\n        test_node.get_logger().info("Unity Integration Test stopped by user")\n    finally:\n        test_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,a.jsx)(e.h2,{id:"quick-recap",children:"Quick Recap"}),"\n",(0,a.jsx)(e.p,{children:"In this lesson, we've covered:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Unity-ROS Bridge"}),": Setting up communication between Unity and ROS 2 for visualization"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Robot Visualization"}),": Creating 3D models and animations for robot states in Unity"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Sensor Data Visualization"}),": Displaying LIDAR, camera, and other sensor data in Unity"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Teleoperation Interface"}),": Building Unity-based interfaces for robot control"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Advanced Visualization"}),": Creating heatmaps, path visualizations, and sensor fusion displays"]}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:"Unity provides a powerful platform for creating high-quality visualizations that complement Gazebo's physics simulation. The combination allows for both accurate physics simulation and photorealistic rendering, which is invaluable for robot development, debugging, and human-robot interaction design."}),"\n",(0,a.jsx)(e.p,{children:"In the next lesson, we'll explore sensor simulation including LIDAR, depth cameras, and IMUs, and how to integrate these with Unity for comprehensive robot perception simulation."})]})}function p(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(c,{...n})}):c(n)}},8453(n,e,r){r.d(e,{R:()=>s,x:()=>i});var t=r(6540);const a={},o=t.createContext(a);function s(n){const e=t.useContext(o);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function i(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:s(n.components),t.createElement(o.Provider,{value:e},n.children)}}}]);